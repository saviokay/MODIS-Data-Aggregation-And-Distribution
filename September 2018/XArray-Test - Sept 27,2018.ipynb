{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjGsPzMyWMDr"
   },
   "source": [
    "###Installing dependecies based on the documentation provided over at the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbqaBwtHNDq7",
    "outputId": "d5db5ecf-be55-4b76-8013-f525529a994f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (1.14.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S9v2qshNDrA",
    "outputId": "fc836394-8ea7-448f-ecdd-d7aa1b49e78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (0.23.4)\r\n",
      "Requirement already satisfied: pytz>=2011k in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (2018.4)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (1.14.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (2.7.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2xT4m-KNDrG",
    "outputId": "c6eb8861-958f-4565-97e2-3bed418c2c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netcdf4 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from netcdf4) (1.14.3)\r\n",
      "Requirement already satisfied: cftime in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from netcdf4) (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9Pps1QTNDrL",
    "outputId": "8f5b674e-ee22-4a0b-c991-cc28d22c7133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (0.23.4)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (1.14.3)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (2.7.3)\r\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas) (2018.4)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHxRAFq8NDrN"
   },
   "outputs": [],
   "source": [
    "!python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FC9dtoHsWY7N"
   },
   "source": [
    "###Installing the XArray package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VS8ORaY5NDrP",
    "outputId": "2c5a81d4-3700-4157-c172-f1c13f48afc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xarray in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (0.10.9)\r\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from xarray) (1.14.3)\r\n",
      "Requirement already satisfied: pandas>=0.19.2 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from xarray) (0.23.4)\r\n",
      "Requirement already satisfied: pytz>=2011k in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas>=0.19.2->xarray) (2018.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pandas>=0.19.2->xarray) (2.7.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.19.2->xarray) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqoSCurpNDrS",
    "outputId": "14ff8979-1013-42d4-d146-7308811991af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/e5/1ce7de3e87ec499a056800fa0d7a689d6502d791c44eb1315a6ecadcb333/pytest-3.8.0-py2.py3-none-any.whl (208kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 5.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting atomicwrites>=1.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/9a/9d878f8d885706e2530402de6417141129a943802c084238914fa6798d97/atomicwrites-1.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pytest) (39.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pytest) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pytest) (1.5.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pytest) (18.1.0)\n",
      "Collecting pluggy>=0.7 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/f1/5a93c118663896d83f7bcbfb7f657ce1d0c0d617e6b4a443a53abcc658ca/pluggy-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from pytest) (4.1.0)\n",
      "Installing collected packages: atomicwrites, pluggy, pytest\n",
      "  Found existing installation: pluggy 0.6.0\n",
      "    Uninstalling pluggy-0.6.0:\n",
      "      Successfully uninstalled pluggy-0.6.0\n",
      "  Found existing installation: pytest 3.5.1\n",
      "    Uninstalling pytest-3.5.1:\n",
      "      Successfully uninstalled pytest-3.5.1\n",
      "Successfully installed atomicwrites-1.2.1 pluggy-0.7.1 pytest-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37zv3ffLNDrV",
    "outputId": "3df7ccb3-2f16-475d-83f3-a4ddcac1cefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mock\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pbr>=0.11 (from mock)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1c/98cba002ed975a91a0294863d9c774cc0ebe38e05bbb65e83314550b1677/pbr-4.2.0-py2.py3-none-any.whl (100kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 5.5MB/s a 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /Users/saviosebastian/anaconda3/lib/python3.6/site-packages (from mock) (1.11.0)\n",
      "Installing collected packages: pbr, mock\n",
      "Successfully installed mock-2.0.0 pbr-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCsEMhjSNDrY",
    "outputId": "87bc72d4-54cd-4e10-b620-6e1393e5e83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.6.5, pytest-3.8.0, py-1.5.3, pluggy-0.7.1\n",
      "rootdir: /Users/saviosebastian/Desktop/XArraysTest, inifile:\n",
      "plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2\n",
      "collected 6858 items / 1 skipped                                               \u001b[0m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\n",
      "\n",
      "xarray/tests/test_accessors.py .........................................\u001b[36m [  0%]\n",
      "\u001b[0m..............................................................\u001b[36m           [  1%]\u001b[0m\n",
      "xarray/tests/test_backends.py ..........................................\u001b[36m [  2%]\n",
      "\u001b[0m........................................................................\u001b[36m [  3%]\n",
      "\u001b[0m........................................................................\u001b[36m [  4%]\n",
      "\u001b[0m........................................................................\u001b[36m [  5%]\n",
      "\u001b[0m.................sssssssssssssssssssssssssssssssssssssssssssssssssssssss\u001b[36m [  6%]\n",
      "\u001b[0msssssssssssssssssssssssssssssssssssssssssssssssssssssssssss.............\u001b[36m [  7%]\n",
      "\u001b[0m........................................................................\u001b[36m [  8%]\n",
      "\u001b[0m....................................ss..................................\u001b[36m [  9%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 10%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 11%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 12%]\n",
      "\u001b[0m.....................................................................X..\u001b[36m [ 13%]\n",
      "\u001b[0m...............................................X........................\u001b[36m [ 14%]\n",
      "\u001b[0m....................ssssssssssssssssssssssssssssssssssssssssssssssssssss\u001b[36m [ 15%]\n",
      "\u001b[0msssssssssssssssssss..............................F.sssssssssssssssssssss\u001b[36m [ 16%]\n",
      "\u001b[0msssssssssss.............................................................\u001b[36m [ 17%]\n",
      "\u001b[0m.................................sssssssssssssssssssssssssssssssssssssss\u001b[36m [ 18%]\n",
      "\u001b[0mssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\u001b[36m [ 19%]\n",
      "\u001b[0msssssssss...........\u001b[36m                                                     [ 20%]\u001b[0m\n",
      "xarray/tests/test_cftime_offsets.py ....................................\u001b[36m [ 20%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 21%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 22%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 23%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 24%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 26%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 27%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 28%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 29%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 30%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 31%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 32%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 33%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 34%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 35%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 36%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 37%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 38%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 39%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 40%]\n",
      "\u001b[0m.................................................................\u001b[36m        [ 41%]\u001b[0m\n",
      "xarray/tests/test_cftimeindex.py .......................................\u001b[36m [ 42%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 43%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 44%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 45%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 46%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 47%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 48%]\n",
      "\u001b[0m..........\u001b[36m                                                               [ 48%]\u001b[0m\n",
      "xarray/tests/test_coding.py .........\u001b[36m                                    [ 48%]\u001b[0m\n",
      "xarray/tests/test_coding_strings.py ...................\u001b[36m                  [ 49%]\u001b[0m\n",
      "xarray/tests/test_coding_times.py ......................................\u001b[36m [ 49%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 50%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 51%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 52%]\n",
      "\u001b[0m...............\u001b[36m                                                          [ 53%]\u001b[0m\n",
      "xarray/tests/test_combine.py ..................\u001b[36m                          [ 53%]\u001b[0m\n",
      "xarray/tests/test_computation.py .................................\u001b[36m       [ 53%]\u001b[0m\n",
      "xarray/tests/test_conventions.py .......................................\u001b[36m [ 54%]\n",
      "\u001b[0m.......s.................\u001b[36m                                                [ 54%]\u001b[0m\n",
      "xarray/tests/test_dask.py ..............................................\u001b[36m [ 55%]\n",
      "\u001b[0m..x..................\u001b[36m                                                    [ 55%]\u001b[0m\n",
      "xarray/tests/test_dataarray.py .........................................\u001b[36m [ 56%]\n",
      "\u001b[0m.....................................s..................................\u001b[36m [ 57%]\n",
      "\u001b[0m........................................sss.............................\u001b[36m [ 58%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 59%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 60%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 61%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 62%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 63%]\n",
      "\u001b[0m...............................................................sssssssss\u001b[36m [ 64%]\n",
      "\u001b[0mssss\u001b[36m                                                                     [ 64%]\u001b[0m\n",
      "xarray/tests/test_dataset.py ...........................................\u001b[36m [ 65%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 66%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 67%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 68%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 69%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 70%]\n",
      "\u001b[0m.......................................................................s\u001b[36m [ 71%]\n",
      "\u001b[0msssssssssssssss.........................................................\u001b[36m [ 72%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 73%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 74%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 75%]\n",
      "\u001b[0m......................................\u001b[36m                                   [ 76%]\u001b[0m\n",
      "xarray/tests/test_dtypes.py .......................................\u001b[36m      [ 76%]\u001b[0m\n",
      "xarray/tests/test_duck_array_ops.py .................................ss.\u001b[36m [ 77%]\n",
      "\u001b[0m.............ss..............ss......ss......ss..............ss.........\u001b[36m [ 78%]\n",
      "\u001b[0m.............................................ss......ss.................\u001b[36m [ 79%]\n",
      "\u001b[0m.............ss..............ss..............ss......ss......ss.........\u001b[36m [ 80%]\n",
      "\u001b[0m.....ss......................................................ss......ss.\u001b[36m [ 81%]\n",
      "\u001b[0m...............ssssssssss..........ssssssssss..........ssssssssss.......\u001b[36m [ 82%]\n",
      "\u001b[0m...ssssssssss..........ssssss..............ssssss..............ssssss...\u001b[36m [ 83%]\n",
      "\u001b[0m...........ssssss..............sssssssssss.s.s.s.s.sssssssssss.s.s.s.s.s\u001b[36m [ 84%]\n",
      "\u001b[0mssssssssss.s.s.s.s.sssssssssss.s.s.s.s.sssssss.s.s.s.s.s.s.sssssss.s.s.s\u001b[36m [ 85%]\n",
      "\u001b[0m.s.s.s.sssssss.s.s.s.s.s.s.sssssss.s.s.s.s.s.s..........................\u001b[36m [ 86%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 87%]\n",
      "\u001b[0m......\u001b[36m                                                                   [ 88%]\u001b[0m\n",
      "xarray/tests/test_extensions.py ....\u001b[36m                                     [ 88%]\u001b[0m\n",
      "xarray/tests/test_formatting.py .............\u001b[36m                            [ 88%]\u001b[0m\n",
      "xarray/tests/test_groupby.py ......\u001b[36m                                      [ 88%]\u001b[0m\n",
      "xarray/tests/test_indexing.py ..........................................\u001b[36m [ 89%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 90%]\n",
      "\u001b[0m....\u001b[36m                                                                     [ 90%]\u001b[0m\n",
      "xarray/tests/test_interp.py .......ss.........................x.\u001b[36m         [ 90%]\u001b[0m\n",
      "xarray/tests/test_merge.py .................\u001b[36m                             [ 90%]\u001b[0m\n",
      "xarray/tests/test_missing.py .................................\u001b[36m           [ 91%]\u001b[0m\n",
      "xarray/tests/test_nputils.py ...\u001b[36m                                         [ 91%]\u001b[0m\n",
      "xarray/tests/test_options.py ..\u001b[36m                                          [ 91%]\u001b[0m\n",
      "xarray/tests/test_plot.py ..............................................\u001b[36m [ 92%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 93%]\n",
      "\u001b[0m.....................................................s..................\u001b[36m [ 94%]\n",
      "\u001b[0m........................................................................\u001b[36m [ 95%]\n",
      "\u001b[0m.....................................\u001b[36m                                    [ 95%]\u001b[0m\n",
      "xarray/tests/test_testing.py .\u001b[36m                                           [ 95%]\u001b[0m\n",
      "xarray/tests/test_tutorial.py s\u001b[36m                                          [ 95%]\u001b[0m\n",
      "xarray/tests/test_ufuncs.py ..............\u001b[36m                               [ 96%]\u001b[0m\n",
      "xarray/tests/test_utils.py ............................\u001b[36m                  [ 96%]\u001b[0m\n",
      "xarray/tests/test_variable.py ..........................................\u001b[36m [ 97%]\n",
      "\u001b[0m................................................................X.....x.\u001b[36m [ 98%]\n",
      "\u001b[0m......x......X..........................................................\u001b[36m [ 99%]\n",
      "\u001b[0m.......x.xxx...................x......x.................\u001b[36m                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________ test_open_mfdataset_manyfiles[netcdf4-100-False-False-None] __________\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "nfiles = 100, suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_files(nfiles, suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        with ExitStack() as stack:\u001b[0m\n",
      "\u001b[1m            files = [stack.enter_context(create_tmp_file(suffix,\u001b[0m\n",
      "\u001b[1m                                                         allow_cleanup_failure))\u001b[0m\n",
      "\u001b[1m                     for apath in np.arange(nfiles)]\u001b[0m\n",
      "\u001b[1m>           yield files\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:900: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "readengine = 'netcdf4', nfiles = 100, autoclose = False, parallel = False\n",
      "chunks = None\n",
      "\n",
      "\u001b[1m    def test_open_mfdataset_manyfiles(readengine, nfiles, autoclose, parallel,\u001b[0m\n",
      "\u001b[1m                                      chunks):\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # skip certain combinations\u001b[0m\n",
      "\u001b[1m        skip_if_not_engine(readengine)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if not has_dask and parallel:\u001b[0m\n",
      "\u001b[1m            pytest.skip('parallel requires dask')\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if readengine == 'h5netcdf' and autoclose:\u001b[0m\n",
      "\u001b[1m            pytest.skip('h5netcdf does not support autoclose yet')\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if ON_WINDOWS:\u001b[0m\n",
      "\u001b[1m            pytest.skip('Skipping on Windows')\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        randdata = np.random.randn(nfiles)\u001b[0m\n",
      "\u001b[1m        original = Dataset({'foo': ('x', randdata)})\u001b[0m\n",
      "\u001b[1m        # test standard open_mfdataset approach with too many files\u001b[0m\n",
      "\u001b[1m        with create_tmp_files(nfiles) as tmpfiles:\u001b[0m\n",
      "\u001b[1m            writeengine = (readengine if readengine != 'pynio' else 'netcdf4')\u001b[0m\n",
      "\u001b[1m            # split into multiple sets of temp files\u001b[0m\n",
      "\u001b[1m            for ii in original.x.values:\u001b[0m\n",
      "\u001b[1m                subds = original.isel(x=slice(ii, ii + 1))\u001b[0m\n",
      "\u001b[1m                subds.to_netcdf(tmpfiles[ii], engine=writeengine)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            # check that calculation on opened datasets works properly\u001b[0m\n",
      "\u001b[1m            actual = open_mfdataset(tmpfiles, engine=readengine, parallel=parallel,\u001b[0m\n",
      "\u001b[1m>                                   autoclose=autoclose, chunks=chunks)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:1979: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "paths = ['/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp160z1ge_/temp-2827.nc', '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsq...qc0000gn/T/tmpvswgy2zw/temp-2831.nc', '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp48cq28y9/temp-2832.nc', ...]\n",
      "chunks = None, concat_dim = '__infer_concat_dim__', compat = 'no_conflicts'\n",
      "preprocess = None, engine = 'netcdf4'\n",
      "lock = <SerializableLock: 30e5ea2b-ea5c-4264-b53d-6dfaf62acba0>\n",
      "data_vars = 'all', coords = 'different', autoclose = False, parallel = False\n",
      "kwargs = {}\n",
      "\n",
      "\u001b[1m    def open_mfdataset(paths, chunks=None, concat_dim=_CONCAT_DIM_DEFAULT,\u001b[0m\n",
      "\u001b[1m                       compat='no_conflicts', preprocess=None, engine=None,\u001b[0m\n",
      "\u001b[1m                       lock=None, data_vars='all', coords='different',\u001b[0m\n",
      "\u001b[1m                       autoclose=False, parallel=False, **kwargs):\u001b[0m\n",
      "\u001b[1m        \"\"\"Open multiple files as a single dataset.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        Requires dask to be installed. See documentation for details on dask [1].\u001b[0m\n",
      "\u001b[1m        Attributes from the first dataset file are used for the combined dataset.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        Parameters\u001b[0m\n",
      "\u001b[1m        ----------\u001b[0m\n",
      "\u001b[1m        paths : str or sequence\u001b[0m\n",
      "\u001b[1m            Either a string glob in the form \"path/to/my/files/*.nc\" or an explicit\u001b[0m\n",
      "\u001b[1m            list of files to open.  Paths can be given as strings or as pathlib\u001b[0m\n",
      "\u001b[1m            Paths.\u001b[0m\n",
      "\u001b[1m        chunks : int or dict, optional\u001b[0m\n",
      "\u001b[1m            Dictionary with keys given by dimension names and values given by chunk\u001b[0m\n",
      "\u001b[1m            sizes. In general, these should divide the dimensions of each dataset.\u001b[0m\n",
      "\u001b[1m            If int, chunk each dimension by ``chunks``.\u001b[0m\n",
      "\u001b[1m            By default, chunks will be chosen to load entire input files into\u001b[0m\n",
      "\u001b[1m            memory at once. This has a major impact on performance: please see the\u001b[0m\n",
      "\u001b[1m            full documentation for more details [2].\u001b[0m\n",
      "\u001b[1m        concat_dim : None, str, DataArray or Index, optional\u001b[0m\n",
      "\u001b[1m            Dimension to concatenate files along. This argument is passed on to\u001b[0m\n",
      "\u001b[1m            :py:func:`xarray.auto_combine` along with the dataset objects. You only\u001b[0m\n",
      "\u001b[1m            need to provide this argument if the dimension along which you want to\u001b[0m\n",
      "\u001b[1m            concatenate is not a dimension in the original datasets, e.g., if you\u001b[0m\n",
      "\u001b[1m            want to stack a collection of 2D arrays along a third dimension.\u001b[0m\n",
      "\u001b[1m            By default, xarray attempts to infer this argument by examining\u001b[0m\n",
      "\u001b[1m            component files. Set ``concat_dim=None`` explicitly to disable\u001b[0m\n",
      "\u001b[1m            concatenation.\u001b[0m\n",
      "\u001b[1m        compat : {'identical', 'equals', 'broadcast_equals',\u001b[0m\n",
      "\u001b[1m                  'no_conflicts'}, optional\u001b[0m\n",
      "\u001b[1m            String indicating how to compare variables of the same name for\u001b[0m\n",
      "\u001b[1m            potential conflicts when merging:\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            - 'broadcast_equals': all values must be equal when variables are\u001b[0m\n",
      "\u001b[1m              broadcast against each other to ensure common dimensions.\u001b[0m\n",
      "\u001b[1m            - 'equals': all values and dimensions must be the same.\u001b[0m\n",
      "\u001b[1m            - 'identical': all values, dimensions and attributes must be the\u001b[0m\n",
      "\u001b[1m              same.\u001b[0m\n",
      "\u001b[1m            - 'no_conflicts': only values which are not null in both datasets\u001b[0m\n",
      "\u001b[1m              must be equal. The returned dataset then contains the combination\u001b[0m\n",
      "\u001b[1m              of all non-null values.\u001b[0m\n",
      "\u001b[1m        preprocess : callable, optional\u001b[0m\n",
      "\u001b[1m            If provided, call this function on each dataset prior to concatenation.\u001b[0m\n",
      "\u001b[1m        engine : {'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio'}, optional\u001b[0m\n",
      "\u001b[1m            Engine to use when reading files. If not provided, the default engine\u001b[0m\n",
      "\u001b[1m            is chosen based on available dependencies, with a preference for\u001b[0m\n",
      "\u001b[1m            'netcdf4'.\u001b[0m\n",
      "\u001b[1m        autoclose : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, automatically close files to avoid OS Error of too many files\u001b[0m\n",
      "\u001b[1m            being open.  However, this option doesn't work with streams, e.g.,\u001b[0m\n",
      "\u001b[1m            BytesIO.\u001b[0m\n",
      "\u001b[1m        lock : False, True or threading.Lock, optional\u001b[0m\n",
      "\u001b[1m            This argument is passed on to :py:func:`dask.array.from_array`. By\u001b[0m\n",
      "\u001b[1m            default, a per-variable lock is used when reading data from netCDF\u001b[0m\n",
      "\u001b[1m            files with the netcdf4 and h5netcdf engines to avoid issues with\u001b[0m\n",
      "\u001b[1m            concurrent access when using dask's multithreaded backend.\u001b[0m\n",
      "\u001b[1m        data_vars : {'minimal', 'different', 'all' or list of str}, optional\u001b[0m\n",
      "\u001b[1m            These data variables will be concatenated together:\u001b[0m\n",
      "\u001b[1m              * 'minimal': Only data variables in which the dimension already\u001b[0m\n",
      "\u001b[1m                appears are included.\u001b[0m\n",
      "\u001b[1m              * 'different': Data variables which are not equal (ignoring\u001b[0m\n",
      "\u001b[1m                attributes) across all datasets are also concatenated (as well as\u001b[0m\n",
      "\u001b[1m                all for which dimension already appears). Beware: this option may\u001b[0m\n",
      "\u001b[1m                load the data payload of data variables into memory if they are not\u001b[0m\n",
      "\u001b[1m                already loaded.\u001b[0m\n",
      "\u001b[1m              * 'all': All data variables will be concatenated.\u001b[0m\n",
      "\u001b[1m              * list of str: The listed data variables will be concatenated, in\u001b[0m\n",
      "\u001b[1m                addition to the 'minimal' data variables.\u001b[0m\n",
      "\u001b[1m        coords : {'minimal', 'different', 'all' o list of str}, optional\u001b[0m\n",
      "\u001b[1m            These coordinate variables will be concatenated together:\u001b[0m\n",
      "\u001b[1m              * 'minimal': Only coordinates in which the dimension already appears\u001b[0m\n",
      "\u001b[1m                are included.\u001b[0m\n",
      "\u001b[1m              * 'different': Coordinates which are not equal (ignoring attributes)\u001b[0m\n",
      "\u001b[1m                across all datasets are also concatenated (as well as all for which\u001b[0m\n",
      "\u001b[1m                dimension already appears). Beware: this option may load the data\u001b[0m\n",
      "\u001b[1m                payload of coordinate variables into memory if they are not already\u001b[0m\n",
      "\u001b[1m                loaded.\u001b[0m\n",
      "\u001b[1m              * 'all': All coordinate variables will be concatenated, except\u001b[0m\n",
      "\u001b[1m                those corresponding to other dimensions.\u001b[0m\n",
      "\u001b[1m              * list of str: The listed coordinate variables will be concatenated,\u001b[0m\n",
      "\u001b[1m                in addition the 'minimal' coordinates.\u001b[0m\n",
      "\u001b[1m        parallel : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, the open and preprocess steps of this function will be\u001b[0m\n",
      "\u001b[1m            performed in parallel using ``dask.delayed``. Default is False.\u001b[0m\n",
      "\u001b[1m        **kwargs : optional\u001b[0m\n",
      "\u001b[1m            Additional arguments passed on to :py:func:`xarray.open_dataset`.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        Returns\u001b[0m\n",
      "\u001b[1m        -------\u001b[0m\n",
      "\u001b[1m        xarray.Dataset\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        See Also\u001b[0m\n",
      "\u001b[1m        --------\u001b[0m\n",
      "\u001b[1m        auto_combine\u001b[0m\n",
      "\u001b[1m        open_dataset\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        References\u001b[0m\n",
      "\u001b[1m        ----------\u001b[0m\n",
      "\u001b[1m        .. [1] http://xarray.pydata.org/en/stable/dask.html\u001b[0m\n",
      "\u001b[1m        .. [2] http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if isinstance(paths, basestring):\u001b[0m\n",
      "\u001b[1m            if is_remote_uri(paths):\u001b[0m\n",
      "\u001b[1m                raise ValueError(\u001b[0m\n",
      "\u001b[1m                    'cannot do wild-card matching for paths that are remote URLs: '\u001b[0m\n",
      "\u001b[1m                    '{!r}. Instead, supply paths as an explicit list of strings.'\u001b[0m\n",
      "\u001b[1m                    .format(paths))\u001b[0m\n",
      "\u001b[1m            paths = sorted(glob(paths))\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            paths = [str(p) if isinstance(p, path_type) else p for p in paths]\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if not paths:\u001b[0m\n",
      "\u001b[1m            raise IOError('no files to open')\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if lock is None:\u001b[0m\n",
      "\u001b[1m            lock = _default_lock(paths[0], engine)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        open_kwargs = dict(engine=engine, chunks=chunks or {}, lock=lock,\u001b[0m\n",
      "\u001b[1m                           autoclose=autoclose, **kwargs)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if parallel:\u001b[0m\n",
      "\u001b[1m            import dask\u001b[0m\n",
      "\u001b[1m            # wrap the open_dataset, getattr, and preprocess with delayed\u001b[0m\n",
      "\u001b[1m            open_ = dask.delayed(open_dataset)\u001b[0m\n",
      "\u001b[1m            getattr_ = dask.delayed(getattr)\u001b[0m\n",
      "\u001b[1m            if preprocess is not None:\u001b[0m\n",
      "\u001b[1m                preprocess = dask.delayed(preprocess)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            open_ = open_dataset\u001b[0m\n",
      "\u001b[1m            getattr_ = getattr\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       datasets = [open_(p, **open_kwargs) for p in paths]\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m:624: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      ".0 = <list_iterator object at 0x1c13f77c18>\n",
      "\n",
      "\u001b[1m>   datasets = [open_(p, **open_kwargs) for p in paths]\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m:624: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "filename_or_obj = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5/temp-2871.nc'\n",
      "group = None, decode_cf = True, mask_and_scale = True, decode_times = True\n",
      "autoclose = False, concat_characters = True, decode_coords = True\n",
      "engine = 'netcdf4', chunks = {}\n",
      "lock = <SerializableLock: 30e5ea2b-ea5c-4264-b53d-6dfaf62acba0>, cache = False\n",
      "drop_variables = None, backend_kwargs = {}\n",
      "\n",
      "\u001b[1m    def open_dataset(filename_or_obj, group=None, decode_cf=True,\u001b[0m\n",
      "\u001b[1m                     mask_and_scale=None, decode_times=True, autoclose=False,\u001b[0m\n",
      "\u001b[1m                     concat_characters=True, decode_coords=True, engine=None,\u001b[0m\n",
      "\u001b[1m                     chunks=None, lock=None, cache=None, drop_variables=None,\u001b[0m\n",
      "\u001b[1m                     backend_kwargs=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Load and decode a dataset from a file or file-like object.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        Parameters\u001b[0m\n",
      "\u001b[1m        ----------\u001b[0m\n",
      "\u001b[1m        filename_or_obj : str, Path, file or xarray.backends.*DataStore\u001b[0m\n",
      "\u001b[1m            Strings and Path objects are interpreted as a path to a netCDF file\u001b[0m\n",
      "\u001b[1m            or an OpenDAP URL and opened with python-netCDF4, unless the filename\u001b[0m\n",
      "\u001b[1m            ends with .gz, in which case the file is gunzipped and opened with\u001b[0m\n",
      "\u001b[1m            scipy.io.netcdf (only netCDF3 supported). File-like objects are opened\u001b[0m\n",
      "\u001b[1m            with scipy.io.netcdf (only netCDF3 supported).\u001b[0m\n",
      "\u001b[1m        group : str, optional\u001b[0m\n",
      "\u001b[1m            Path to the netCDF4 group in the given file to open (only works for\u001b[0m\n",
      "\u001b[1m            netCDF4 files).\u001b[0m\n",
      "\u001b[1m        decode_cf : bool, optional\u001b[0m\n",
      "\u001b[1m            Whether to decode these variables, assuming they were saved according\u001b[0m\n",
      "\u001b[1m            to CF conventions.\u001b[0m\n",
      "\u001b[1m        mask_and_scale : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, replace array values equal to `_FillValue` with NA and scale\u001b[0m\n",
      "\u001b[1m            values according to the formula `original_values * scale_factor +\u001b[0m\n",
      "\u001b[1m            add_offset`, where `_FillValue`, `scale_factor` and `add_offset` are\u001b[0m\n",
      "\u001b[1m            taken from variable attributes (if they exist).  If the `_FillValue` or\u001b[0m\n",
      "\u001b[1m            `missing_value` attribute contains multiple values a warning will be\u001b[0m\n",
      "\u001b[1m            issued and all array values matching one of the multiple values will\u001b[0m\n",
      "\u001b[1m            be replaced by NA. mask_and_scale defaults to True except for the\u001b[0m\n",
      "\u001b[1m            pseudonetcdf backend.\u001b[0m\n",
      "\u001b[1m        decode_times : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, decode times encoded in the standard NetCDF datetime format\u001b[0m\n",
      "\u001b[1m            into datetime objects. Otherwise, leave them encoded as numbers.\u001b[0m\n",
      "\u001b[1m        autoclose : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, automatically close files to avoid OS Error of too many files\u001b[0m\n",
      "\u001b[1m            being open.  However, this option doesn't work with streams, e.g.,\u001b[0m\n",
      "\u001b[1m            BytesIO.\u001b[0m\n",
      "\u001b[1m        concat_characters : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, concatenate along the last dimension of character arrays to\u001b[0m\n",
      "\u001b[1m            form string arrays. Dimensions will only be concatenated over (and\u001b[0m\n",
      "\u001b[1m            removed) if they have no corresponding variable and if they are only\u001b[0m\n",
      "\u001b[1m            used as the last dimension of character arrays.\u001b[0m\n",
      "\u001b[1m        decode_coords : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, decode the 'coordinates' attribute to identify coordinates in\u001b[0m\n",
      "\u001b[1m            the resulting dataset.\u001b[0m\n",
      "\u001b[1m        engine : {'netcdf4', 'scipy', 'pydap', 'h5netcdf', 'pynio', 'pseudonetcdf'}, optional\u001b[0m\n",
      "\u001b[1m            Engine to use when reading files. If not provided, the default engine\u001b[0m\n",
      "\u001b[1m            is chosen based on available dependencies, with a preference for\u001b[0m\n",
      "\u001b[1m            'netcdf4'.\u001b[0m\n",
      "\u001b[1m        chunks : int or dict, optional\u001b[0m\n",
      "\u001b[1m            If chunks is provided, it used to load the new dataset into dask\u001b[0m\n",
      "\u001b[1m            arrays. ``chunks={}`` loads the dataset with dask using a single\u001b[0m\n",
      "\u001b[1m            chunk for all arrays.\u001b[0m\n",
      "\u001b[1m        lock : False, True or threading.Lock, optional\u001b[0m\n",
      "\u001b[1m            If chunks is provided, this argument is passed on to\u001b[0m\n",
      "\u001b[1m            :py:func:`dask.array.from_array`. By default, a global lock is\u001b[0m\n",
      "\u001b[1m            used when reading data from netCDF files with the netcdf4 and h5netcdf\u001b[0m\n",
      "\u001b[1m            engines to avoid issues with concurrent access when using dask's\u001b[0m\n",
      "\u001b[1m            multithreaded backend.\u001b[0m\n",
      "\u001b[1m        cache : bool, optional\u001b[0m\n",
      "\u001b[1m            If True, cache data loaded from the underlying datastore in memory as\u001b[0m\n",
      "\u001b[1m            NumPy arrays when accessed to avoid reading from the underlying data-\u001b[0m\n",
      "\u001b[1m            store multiple times. Defaults to True unless you specify the `chunks`\u001b[0m\n",
      "\u001b[1m            argument to use dask, in which case it defaults to False. Does not\u001b[0m\n",
      "\u001b[1m            change the behavior of coordinates corresponding to dimensions, which\u001b[0m\n",
      "\u001b[1m            always load their data from disk into a ``pandas.Index``.\u001b[0m\n",
      "\u001b[1m        drop_variables: string or iterable, optional\u001b[0m\n",
      "\u001b[1m            A variable or list of variables to exclude from being parsed from the\u001b[0m\n",
      "\u001b[1m            dataset. This may be useful to drop variables with problems or\u001b[0m\n",
      "\u001b[1m            inconsistent values.\u001b[0m\n",
      "\u001b[1m        backend_kwargs: dictionary, optional\u001b[0m\n",
      "\u001b[1m            A dictionary of keyword arguments to pass on to the backend. This\u001b[0m\n",
      "\u001b[1m            may be useful when backend options would improve performance or\u001b[0m\n",
      "\u001b[1m            allow user control of dataset processing.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        Returns\u001b[0m\n",
      "\u001b[1m        -------\u001b[0m\n",
      "\u001b[1m        dataset : Dataset\u001b[0m\n",
      "\u001b[1m            The newly created dataset.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        See Also\u001b[0m\n",
      "\u001b[1m        --------\u001b[0m\n",
      "\u001b[1m        open_mfdataset\u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if mask_and_scale is None:\u001b[0m\n",
      "\u001b[1m            mask_and_scale = not engine == 'pseudonetcdf'\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if not decode_cf:\u001b[0m\n",
      "\u001b[1m            mask_and_scale = False\u001b[0m\n",
      "\u001b[1m            decode_times = False\u001b[0m\n",
      "\u001b[1m            concat_characters = False\u001b[0m\n",
      "\u001b[1m            decode_coords = False\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if cache is None:\u001b[0m\n",
      "\u001b[1m            cache = chunks is None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if backend_kwargs is None:\u001b[0m\n",
      "\u001b[1m            backend_kwargs = {}\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        def maybe_decode_store(store, lock=False):\u001b[0m\n",
      "\u001b[1m            ds = conventions.decode_cf(\u001b[0m\n",
      "\u001b[1m                store, mask_and_scale=mask_and_scale, decode_times=decode_times,\u001b[0m\n",
      "\u001b[1m                concat_characters=concat_characters, decode_coords=decode_coords,\u001b[0m\n",
      "\u001b[1m                drop_variables=drop_variables)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            _protect_dataset_variables_inplace(ds, cache)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            if chunks is not None:\u001b[0m\n",
      "\u001b[1m                from dask.base import tokenize\u001b[0m\n",
      "\u001b[1m                # if passed an actual file path, augment the token with\u001b[0m\n",
      "\u001b[1m                # the file modification time\u001b[0m\n",
      "\u001b[1m                if (isinstance(filename_or_obj, basestring) and\u001b[0m\n",
      "\u001b[1m                        not is_remote_uri(filename_or_obj)):\u001b[0m\n",
      "\u001b[1m                    mtime = os.path.getmtime(filename_or_obj)\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    mtime = None\u001b[0m\n",
      "\u001b[1m                token = tokenize(filename_or_obj, mtime, group, decode_cf,\u001b[0m\n",
      "\u001b[1m                                 mask_and_scale, decode_times, concat_characters,\u001b[0m\n",
      "\u001b[1m                                 decode_coords, engine, chunks, drop_variables)\u001b[0m\n",
      "\u001b[1m                name_prefix = 'open_dataset-%s' % token\u001b[0m\n",
      "\u001b[1m                ds2 = ds.chunk(chunks, name_prefix=name_prefix, token=token,\u001b[0m\n",
      "\u001b[1m                               lock=lock)\u001b[0m\n",
      "\u001b[1m                ds2._file_obj = ds._file_obj\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                ds2 = ds\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            # protect so that dataset store isn't necessarily closed, e.g.,\u001b[0m\n",
      "\u001b[1m            # streams like BytesIO  can't be reopened\u001b[0m\n",
      "\u001b[1m            # datastore backend is responsible for determining this capability\u001b[0m\n",
      "\u001b[1m            if store._autoclose:\u001b[0m\n",
      "\u001b[1m                store.close()\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            return ds2\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if isinstance(filename_or_obj, path_type):\u001b[0m\n",
      "\u001b[1m            filename_or_obj = str(filename_or_obj)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        if isinstance(filename_or_obj, backends.AbstractDataStore):\u001b[0m\n",
      "\u001b[1m            store = filename_or_obj\u001b[0m\n",
      "\u001b[1m        elif isinstance(filename_or_obj, basestring):\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            if (isinstance(filename_or_obj, bytes) and\u001b[0m\n",
      "\u001b[1m                    filename_or_obj.startswith(b'\\x89HDF')):\u001b[0m\n",
      "\u001b[1m                raise ValueError('cannot read netCDF4/HDF5 file images')\u001b[0m\n",
      "\u001b[1m            elif (isinstance(filename_or_obj, bytes) and\u001b[0m\n",
      "\u001b[1m                    filename_or_obj.startswith(b'CDF')):\u001b[0m\n",
      "\u001b[1m                # netCDF3 file images are handled by scipy\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m            elif isinstance(filename_or_obj, basestring):\u001b[0m\n",
      "\u001b[1m                filename_or_obj = _normalize_path(filename_or_obj)\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            if filename_or_obj.endswith('.gz'):\u001b[0m\n",
      "\u001b[1m                if engine is not None and engine != 'scipy':\u001b[0m\n",
      "\u001b[1m                    raise ValueError('can only read gzipped netCDF files with '\u001b[0m\n",
      "\u001b[1m                                     \"default engine or engine='scipy'\")\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    engine = 'scipy'\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m            if engine is None:\u001b[0m\n",
      "\u001b[1m                engine = _get_default_engine(filename_or_obj,\u001b[0m\n",
      "\u001b[1m                                             allow_remote=True)\u001b[0m\n",
      "\u001b[1m            if engine == 'netcdf4':\u001b[0m\n",
      "\u001b[1m                store = backends.NetCDF4DataStore.open(filename_or_obj,\u001b[0m\n",
      "\u001b[1m                                                       group=group,\u001b[0m\n",
      "\u001b[1m                                                       autoclose=autoclose,\u001b[0m\n",
      "\u001b[1m>                                                      **backend_kwargs)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m:320: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "cls = <class 'xarray.backends.netCDF4_.NetCDF4DataStore'>\n",
      "filename = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5/temp-2871.nc'\n",
      "mode = 'r', format = 'NETCDF4', group = None, writer = None, clobber = True\n",
      "diskless = False, persist = False, autoclose = False\n",
      "lock = <SerializableLock: 30e5ea2b-ea5c-4264-b53d-6dfaf62acba0>\n",
      "\n",
      "\u001b[1m    @classmethod\u001b[0m\n",
      "\u001b[1m    def open(cls, filename, mode='r', format='NETCDF4', group=None,\u001b[0m\n",
      "\u001b[1m             writer=None, clobber=True, diskless=False, persist=False,\u001b[0m\n",
      "\u001b[1m             autoclose=False, lock=HDF5_LOCK):\u001b[0m\n",
      "\u001b[1m        import netCDF4 as nc4\u001b[0m\n",
      "\u001b[1m        if (len(filename) == 88 and\u001b[0m\n",
      "\u001b[1m                LooseVersion(nc4.__version__) < \"1.3.1\"):\u001b[0m\n",
      "\u001b[1m            warnings.warn(\u001b[0m\n",
      "\u001b[1m                'A segmentation fault may occur when the '\u001b[0m\n",
      "\u001b[1m                'file path has exactly 88 characters as it does '\u001b[0m\n",
      "\u001b[1m                'in this case. The issue is known to occur with '\u001b[0m\n",
      "\u001b[1m                'version 1.2.4 of netCDF4 and can be addressed by '\u001b[0m\n",
      "\u001b[1m                'upgrading netCDF4 to at least version 1.3.1. '\u001b[0m\n",
      "\u001b[1m                'More details can be found here: '\u001b[0m\n",
      "\u001b[1m                'https://github.com/pydata/xarray/issues/1745')\u001b[0m\n",
      "\u001b[1m        if format is None:\u001b[0m\n",
      "\u001b[1m            format = 'NETCDF4'\u001b[0m\n",
      "\u001b[1m        opener = functools.partial(_open_netcdf4_group, filename, mode=mode,\u001b[0m\n",
      "\u001b[1m                                   group=group, clobber=clobber,\u001b[0m\n",
      "\u001b[1m                                   diskless=diskless, persist=persist,\u001b[0m\n",
      "\u001b[1m                                   format=format)\u001b[0m\n",
      "\u001b[1m>       ds = opener()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m:331: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "filename = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5/temp-2871.nc'\n",
      "mode = 'r', group = None\n",
      "kwargs = {'clobber': True, 'diskless': False, 'format': 'NETCDF4', 'persist': False}\n",
      "nc4 = <module 'netCDF4' from '/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/netCDF4/__init__.py'>\n",
      "\n",
      "\u001b[1m    def _open_netcdf4_group(filename, mode, group=None, **kwargs):\u001b[0m\n",
      "\u001b[1m        import netCDF4 as nc4\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m>       ds = nc4.Dataset(filename, mode=mode, **kwargs)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m:230: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\u001b[1m>   ???\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mnetCDF4/_netCDF4.pyx\u001b[0m:2123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "\u001b[1m>   ???\u001b[0m\n",
      "\u001b[1m\u001b[31mE   OSError: [Errno 24] Too many open files: b'/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5/temp-2871.nc'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mnetCDF4/_netCDF4.pyx\u001b[0m:1743: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a0dbc8>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c08abc5c0>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c14a0dbc8>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi2b94rxo'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c149ee488>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi2b94rxo'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c149ee488>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi2b94rxo'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c149ee488>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi2b94rxo'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a0da48>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218898>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c14a0da48>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp8f3r_ybz'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984378>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp8f3r_ybz'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984378>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp8f3r_ybz'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984378>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp8f3r_ybz'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a5afc8>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c142182e8>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c14a5afc8>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdkx93qpx'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984f28>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdkx93qpx'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984f28>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdkx93qpx'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984f28>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdkx93qpx'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c1491a488>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c142181d0>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c1491a488>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3ldejwcw'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984048>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3ldejwcw'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984048>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3ldejwcw'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14984048>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3ldejwcw'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c1496d448>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218da0>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c1496d448>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2j3shrxr'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a9d8>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2j3shrxr'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a9d8>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2j3shrxr'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a9d8>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2j3shrxr'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c1496d288>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c142184e0>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c1496d288>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp4kgidg35'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4ac80>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp4kgidg35'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4ac80>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp4kgidg35'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4ac80>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp4kgidg35'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a47f08>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218668>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c14a47f08>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvr2o2lnw'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a598>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvr2o2lnw'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a598>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvr2o2lnw'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a598>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvr2o2lnw'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1ddf88>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c142180b8>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c2e1ddf88>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_gdvcp0i'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a7b8>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_gdvcp0i'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a7b8>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_gdvcp0i'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14a4a7b8>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_gdvcp0i'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1dd408>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c142184a8>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c2e1dd408>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw8yd2kxh'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db268>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw8yd2kxh'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db268>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw8yd2kxh'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db268>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw8yd2kxh'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c145ddb48>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218e80>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c145ddb48>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp7y2_0d43'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbb70>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp7y2_0d43'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbb70>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp7y2_0d43'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbb70>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp7y2_0d43'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13df2ec8>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218518>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c13df2ec8>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\n",
      "\u001b[1m                value = type()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            yield path\u001b[0m\n",
      "\u001b[1m        finally:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpnbup7u11'\n",
      "ignore_errors = False\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db048>\n",
      "\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        \"\"\"\u001b[0m\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                pass\u001b[0m\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\n",
      "\u001b[1m                raise\u001b[0m\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\n",
      "\u001b[1m            except Exception:\u001b[0m\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                return\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m                else:\u001b[0m\n",
      "\u001b[1m                    try:\u001b[0m\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m                    except OSError:\u001b[0m\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            finally:\u001b[0m\n",
      "\u001b[1m                os.close(fd)\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpnbup7u11'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db048>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpnbup7u11'\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db048>\n",
      "\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\n",
      "\u001b[1m        except OSError:\u001b[0m\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\n",
      "\u001b[1m            return\u001b[0m\n",
      "\u001b[1m        names = []\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpnbup7u11'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\n",
      "\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "suffix = '.nc', allow_cleanup_failure = False\n",
      "\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\n",
      "\u001b[1m        try:\u001b[0m\n",
      "\u001b[1m>           yield path\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\n",
      "received_exc = True\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\n",
      "suppressed_exc = False\n",
      "\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\n",
      "\u001b[1m            while 1:\u001b[0m\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\n",
      "\u001b[1m                    return\u001b[0m\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\n",
      "\u001b[1m                    break\u001b[0m\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\n",
      "\u001b[1m        # nested context managers\u001b[0m\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\n",
      "\u001b[1m        pending_raise = False\u001b[0m\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a00808>)\n",
      "\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218208>\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\n",
      "traceback = <traceback object at 0x1c14a00808>\n",
      "\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\n",
      "\u001b[1m        if type is None:\u001b[0m\n",
      "\u001b[1m            try:\u001b[0m\n",
      "\u001b[1m                next(self.gen)\u001b[0m\n",
      "\u001b[1m            except StopIteration:\u001b[0m\n",
      "\u001b[1m                return False\u001b[0m\n",
      "\u001b[1m            else:\u001b[0m\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\n",
      "\u001b[1m        else:\u001b[0m\n",
      "\u001b[1m            if value is None:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpba4bb8li'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db9d8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpba4bb8li'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db9d8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpba4bb8li'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db9d8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpba4bb8li'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a00088>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218e10>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a00088>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp14ihyoac'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db400>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp14ihyoac'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db400>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp14ihyoac'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db400>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp14ihyoac'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a00988>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218e48>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a00988>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_bgglewq'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db620>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_bgglewq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db620>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_bgglewq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db620>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_bgglewq'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13dbf6c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218ef0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13dbf6c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpji5zxz1d'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db840>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpji5zxz1d'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db840>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpji5zxz1d'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db840>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpji5zxz1d'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c149fc108>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14218dd8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c149fc108>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp5r7n3ddu'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd08>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp5r7n3ddu'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd08>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp5r7n3ddu'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd08>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp5r7n3ddu'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a3cfc8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464b00>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a3cfc8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpsomcxnfo'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbbf8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpsomcxnfo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbbf8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpsomcxnfo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbbf8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpsomcxnfo'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c1415a708>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144641d0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c1415a708>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpauthjsvr'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbe18>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpauthjsvr'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbe18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpauthjsvr'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbe18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpauthjsvr'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14367688>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464c18>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14367688>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpaefksdug'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbc80>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpaefksdug'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbc80>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpaefksdug'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbc80>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpaefksdug'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c16912b48>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464a58>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c16912b48>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpelwfp7pt'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbea0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpelwfp7pt'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbea0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpelwfp7pt'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbea0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpelwfp7pt'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c168d7688>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144647b8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c168d7688>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmphw2_f42t'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbf28>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmphw2_f42t'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbf28>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmphw2_f42t'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbf28>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmphw2_f42t'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1ce788>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464b38>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1ce788>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf4pdq2p3'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbae8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf4pdq2p3'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf4pdq2p3'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf4pdq2p3'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1ce108>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144640b8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1ce108>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0rs07xah'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db510>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0rs07xah'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db510>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0rs07xah'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db510>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0rs07xah'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c141a4a48>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464320>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c141a4a48>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmplii3zbc2'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db1e0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmplii3zbc2'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db1e0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmplii3zbc2'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1db1e0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmplii3zbc2'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a28288>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464208>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a28288>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp76fqe8wc'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd90>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp76fqe8wc'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd90>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp76fqe8wc'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c2e1dbd90>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp76fqe8wc'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1cbc08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144643c8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1cbc08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_j8jxow2'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7e18>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_j8jxow2'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7e18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_j8jxow2'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7e18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp_j8jxow2'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1cb2c8>)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464240>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1cb2c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpupn0_hm4'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7158>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpupn0_hm4'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpupn0_hm4'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpupn0_hm4'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1cbf48>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464550>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1cbf48>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdtgvjoab'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7268>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdtgvjoab'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7268>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdtgvjoab'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7268>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdtgvjoab'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a17c08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464358>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a17c08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpwn5eob85'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7a60>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpwn5eob85'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7a60>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpwn5eob85'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7a60>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpwn5eob85'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a17e08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464160>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a17e08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpyhhnmr8n'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7950>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpyhhnmr8n'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7950>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpyhhnmr8n'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7950>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpyhhnmr8n'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a17948>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464400>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a17948>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2est2r6a'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7ae8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2est2r6a'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7ae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2est2r6a'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148b7ae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2est2r6a'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a05208>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144646d8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a05208>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0647asy'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acae8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0647asy'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0647asy'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acae8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0647asy'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a05a48>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464c50>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a05a48>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjmsl2dt5'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac400>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjmsl2dt5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac400>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjmsl2dt5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac400>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjmsl2dt5'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a050c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464198>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a050c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjjt20xkb'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac7b8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjjt20xkb'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac7b8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjjt20xkb'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac7b8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjjt20xkb'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c168fc688>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464ef0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c168fc688>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp494zsyoo'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac730>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp494zsyoo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac730>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp494zsyoo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac730>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp494zsyoo'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c168fcec8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144649e8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c168fcec8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0d6s_raw'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac158>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0d6s_raw'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0d6s_raw'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0d6s_raw'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a14f88>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c144644e0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a14f88>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfmy8eukl'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac840>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfmy8eukl'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac840>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfmy8eukl'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac840>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfmy8eukl'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a14a08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464a90>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a14a08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0h6g9wyo'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac2f0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0h6g9wyo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac2f0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0h6g9wyo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac2f0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp0h6g9wyo'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a14888>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464390>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a14888>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp1sdnpaky'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd08>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp1sdnpaky'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd08>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp1sdnpaky'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd08>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp1sdnpaky'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14141ac8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464b70>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14141ac8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw39j2vwx'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148aca60>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw39j2vwx'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148aca60>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw39j2vwx'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148aca60>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpw39j2vwx'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13a97a08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c14464668>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13a97a08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2zavo0tc'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac620>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2zavo0tc'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac620>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2zavo0tc'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac620>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2zavo0tc'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c16935108>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc710>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c16935108>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpu02me_b3'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac1e0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpu02me_b3'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac1e0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpu02me_b3'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac1e0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpu02me_b3'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c16935d08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bcac8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c16935d08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjzsvqymo'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ace18>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjzsvqymo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ace18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjzsvqymo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ace18>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpjzsvqymo'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a4d1c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc160>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a4d1c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvlthgy8d'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acea0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvlthgy8d'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acea0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvlthgy8d'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acea0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpvlthgy8d'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13acbac8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bcfd0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13acbac8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3j2h_xyw'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac0d0>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3j2h_xyw'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac0d0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3j2h_xyw'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac0d0>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3j2h_xyw'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13acb3c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bcf28>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13acb3c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpd5iqj6aq'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd90>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpd5iqj6aq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd90>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpd5iqj6aq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acd90>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpd5iqj6aq'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13acb148>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc518>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13acb148>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2xrrudiq'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac378>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2xrrudiq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac378>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2xrrudiq'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac378>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp2xrrudiq'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13abf9c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc4e0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13abf9c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfq4sjvrp'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac488>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfq4sjvrp'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac488>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfq4sjvrp'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac488>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpfq4sjvrp'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13abff08>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bceb8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13abff08>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpj7oyj_l5'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac048>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpj7oyj_l5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac048>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpj7oyj_l5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac048>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpj7oyj_l5'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13abfb88>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc6d8>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13abfb88>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi42a2192'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac510>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi42a2192'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac510>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi42a2192'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac510>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpi42a2192'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13abf388>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc1d0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13abf388>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn2gd27e9'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac9d8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn2gd27e9'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac9d8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn2gd27e9'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac9d8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn2gd27e9'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c13abf348>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc828>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c13abf348>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf0rwjpbx'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac8c8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf0rwjpbx'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac8c8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf0rwjpbx'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac8c8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpf0rwjpbx'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c16973688>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc2b0>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c16973688>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpgfx56rhl'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac6a8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpgfx56rhl'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac6a8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpgfx56rhl'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac6a8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpgfx56rhl'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1ab088>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bcd68>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1ab088>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3129vmxo'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac598>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3129vmxo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac598>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3129vmxo'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148ac598>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmp3129vmxo'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c2e1ab488>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bce48>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c2e1ab488>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpccmdccgu'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acbf8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpccmdccgu'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acbf8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpccmdccgu'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c148acbf8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpccmdccgu'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1bf88>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc588>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a1bf88>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c142287b8>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c142287b8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c142287b8>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmprg_5zvl5'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1b1c8>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bca90>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a1b1c8>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdqwts4dd'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228158>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdqwts4dd'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdqwts4dd'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228158>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpdqwts4dd'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           yield path\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib.ExitStack object at 0x1c13f77470>\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1be48>)\r\n",
      "received_exc = True\r\n",
      "_fix_exception_context = <function ExitStack.__exit__.<locals>._fix_exception_context at 0x1c149f6400>\r\n",
      "suppressed_exc = False\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, *exc_details):\u001b[0m\r\n",
      "\u001b[1m        received_exc = exc_details[0] is not None\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # We manipulate the exception state so it behaves as though\u001b[0m\r\n",
      "\u001b[1m        # we were actually nesting multiple with statements\u001b[0m\r\n",
      "\u001b[1m        frame_exc = sys.exc_info()[1]\u001b[0m\r\n",
      "\u001b[1m        def _fix_exception_context(new_exc, old_exc):\u001b[0m\r\n",
      "\u001b[1m            # Context may not be correct, so find the end of the chain\u001b[0m\r\n",
      "\u001b[1m            while 1:\u001b[0m\r\n",
      "\u001b[1m                exc_context = new_exc.__context__\u001b[0m\r\n",
      "\u001b[1m                if exc_context is old_exc:\u001b[0m\r\n",
      "\u001b[1m                    # Context is already set correctly (see issue 20317)\u001b[0m\r\n",
      "\u001b[1m                    return\u001b[0m\r\n",
      "\u001b[1m                if exc_context is None or exc_context is frame_exc:\u001b[0m\r\n",
      "\u001b[1m                    break\u001b[0m\r\n",
      "\u001b[1m                new_exc = exc_context\u001b[0m\r\n",
      "\u001b[1m            # Change the end of the chain to point to the exception\u001b[0m\r\n",
      "\u001b[1m            # we expect it to reference\u001b[0m\r\n",
      "\u001b[1m            new_exc.__context__ = old_exc\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # Callbacks are invoked in LIFO order to match the behaviour of\u001b[0m\r\n",
      "\u001b[1m        # nested context managers\u001b[0m\r\n",
      "\u001b[1m        suppressed_exc = False\u001b[0m\r\n",
      "\u001b[1m        pending_raise = False\u001b[0m\r\n",
      "\u001b[1m        while self._exit_callbacks:\u001b[0m\r\n",
      "\u001b[1m            cb = self._exit_callbacks.pop()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               if cb(*exc_details):\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "exc_details = (<class 'OSError'>, OSError(24, 'Too many open files'), <traceback object at 0x1c14a1b488>)\r\n",
      "\r\n",
      "\u001b[1m    def _exit_wrapper(*exc_details):\u001b[0m\r\n",
      "\u001b[1m>       return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "self = <contextlib._GeneratorContextManager object at 0x1c148bc748>\r\n",
      "type = <class 'OSError'>, value = OSError(24, 'Too many open files')\r\n",
      "traceback = <traceback object at 0x1c14a1b488>\r\n",
      "\r\n",
      "\u001b[1m    def __exit__(self, type, value, traceback):\u001b[0m\r\n",
      "\u001b[1m        if type is None:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                next(self.gen)\u001b[0m\r\n",
      "\u001b[1m            except StopIteration:\u001b[0m\r\n",
      "\u001b[1m                return False\u001b[0m\r\n",
      "\u001b[1m            else:\u001b[0m\r\n",
      "\u001b[1m                raise RuntimeError(\"generator didn't stop\")\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m            if value is None:\u001b[0m\r\n",
      "\u001b[1m                # Need to force instantiation so we can reliably\u001b[0m\r\n",
      "\u001b[1m                # tell if we get the same exception back\u001b[0m\r\n",
      "\u001b[1m                value = type()\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "suffix = '.nc', allow_cleanup_failure = False\r\n",
      "\r\n",
      "\u001b[1m    @contextlib.contextmanager\u001b[0m\r\n",
      "\u001b[1m    def create_tmp_file(suffix='.nc', allow_cleanup_failure=False):\u001b[0m\r\n",
      "\u001b[1m        temp_dir = tempfile.mkdtemp()\u001b[0m\r\n",
      "\u001b[1m        path = os.path.join(temp_dir, 'temp-%s%s' % (next(_counter), suffix))\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            yield path\u001b[0m\r\n",
      "\u001b[1m        finally:\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m>               shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0odbq5s'\r\n",
      "ignore_errors = False\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228730>\r\n",
      "\r\n",
      "\u001b[1m    def rmtree(path, ignore_errors=False, onerror=None):\u001b[0m\r\n",
      "\u001b[1m        \"\"\"Recursively delete a directory tree.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        If ignore_errors is set, errors are ignored; otherwise, if onerror\u001b[0m\r\n",
      "\u001b[1m        is set, it is called to handle the error with arguments (func,\u001b[0m\r\n",
      "\u001b[1m        path, exc_info) where func is platform and implementation dependent;\u001b[0m\r\n",
      "\u001b[1m        path is the argument to that function that caused it to fail; and\u001b[0m\r\n",
      "\u001b[1m        exc_info is a tuple returned by sys.exc_info().  If ignore_errors\u001b[0m\r\n",
      "\u001b[1m        is false and onerror is None, an exception is raised.\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        \"\"\"\u001b[0m\r\n",
      "\u001b[1m        if ignore_errors:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                pass\u001b[0m\r\n",
      "\u001b[1m        elif onerror is None:\u001b[0m\r\n",
      "\u001b[1m            def onerror(*args):\u001b[0m\r\n",
      "\u001b[1m                raise\u001b[0m\r\n",
      "\u001b[1m        if _use_fd_functions:\u001b[0m\r\n",
      "\u001b[1m            # While the unsafe rmtree works fine on bytes, the fd based does not.\u001b[0m\r\n",
      "\u001b[1m            if isinstance(path, bytes):\u001b[0m\r\n",
      "\u001b[1m                path = os.fsdecode(path)\u001b[0m\r\n",
      "\u001b[1m            # Note: To guard against symlink races, we use the standard\u001b[0m\r\n",
      "\u001b[1m            # lstat()/open()/fstat() trick.\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                orig_st = os.lstat(path)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                fd = os.open(path, os.O_RDONLY)\u001b[0m\r\n",
      "\u001b[1m            except Exception:\u001b[0m\r\n",
      "\u001b[1m                onerror(os.lstat, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                return\u001b[0m\r\n",
      "\u001b[1m            try:\u001b[0m\r\n",
      "\u001b[1m                if os.path.samestat(orig_st, os.fstat(fd)):\u001b[0m\r\n",
      "\u001b[1m                    _rmtree_safe_fd(fd, path, onerror)\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        os.rmdir(path)\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.rmdir, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m                else:\u001b[0m\r\n",
      "\u001b[1m                    try:\u001b[0m\r\n",
      "\u001b[1m                        # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                        raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m                    except OSError:\u001b[0m\r\n",
      "\u001b[1m                        onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            finally:\u001b[0m\r\n",
      "\u001b[1m                os.close(fd)\u001b[0m\r\n",
      "\u001b[1m        else:\u001b[0m\r\n",
      "\u001b[1m>           return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0odbq5s'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228730>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m>           onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0odbq5s'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228730>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpo0odbq5s'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\r\n",
      "\u001b[33mDuring handling of the above exception, another exception occurred:\u001b[0m\r\n",
      "\r\n",
      "readengine = 'netcdf4', nfiles = 100, autoclose = False, parallel = False\r\n",
      "chunks = None\r\n",
      "\r\n",
      "\u001b[1m    def test_open_mfdataset_manyfiles(readengine, nfiles, autoclose, parallel,\u001b[0m\r\n",
      "\u001b[1m                                      chunks):\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        # skip certain combinations\u001b[0m\r\n",
      "\u001b[1m        skip_if_not_engine(readengine)\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        if not has_dask and parallel:\u001b[0m\r\n",
      "\u001b[1m            pytest.skip('parallel requires dask')\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        if readengine == 'h5netcdf' and autoclose:\u001b[0m\r\n",
      "\u001b[1m            pytest.skip('h5netcdf does not support autoclose yet')\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        if ON_WINDOWS:\u001b[0m\r\n",
      "\u001b[1m            pytest.skip('Skipping on Windows')\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m        randdata = np.random.randn(nfiles)\u001b[0m\r\n",
      "\u001b[1m        original = Dataset({'foo': ('x', randdata)})\u001b[0m\r\n",
      "\u001b[1m        # test standard open_mfdataset approach with too many files\u001b[0m\r\n",
      "\u001b[1m        with create_tmp_files(nfiles) as tmpfiles:\u001b[0m\r\n",
      "\u001b[1m            writeengine = (readengine if readengine != 'pynio' else 'netcdf4')\u001b[0m\r\n",
      "\u001b[1m            # split into multiple sets of temp files\u001b[0m\r\n",
      "\u001b[1m            for ii in original.x.values:\u001b[0m\r\n",
      "\u001b[1m                subds = original.isel(x=slice(ii, ii + 1))\u001b[0m\r\n",
      "\u001b[1m                subds.to_netcdf(tmpfiles[ii], engine=writeengine)\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m            # check that calculation on opened datasets works properly\u001b[0m\r\n",
      "\u001b[1m            actual = open_mfdataset(tmpfiles, engine=readengine, parallel=parallel,\u001b[0m\r\n",
      "\u001b[1m                                    autoclose=autoclose, chunks=chunks)\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m            # check that using open_mfdataset returns dask arrays for variables\u001b[0m\r\n",
      "\u001b[1m            assert isinstance(actual['foo'].data, dask_array_type)\u001b[0m\r\n",
      "\u001b[1m    \u001b[0m\r\n",
      "\u001b[1m>           assert_identical(original, actual)\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:1984: \r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: in __exit__\r\n",
      "\u001b[1m    self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:900: in create_tmp_files\r\n",
      "\u001b[1m    yield files\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:380: in __exit__\r\n",
      "\u001b[1m    raise exc_details[1]\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: in __exit__\r\n",
      "\u001b[1m    self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:885: in create_tmp_file\r\n",
      "\u001b[1m    yield path\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:365: in __exit__\r\n",
      "\u001b[1m    if cb(*exc_details):\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:284: in _exit_wrapper\r\n",
      "\u001b[1m    return cm_exit(cm, *exc_details)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/contextlib.py\u001b[0m:99: in __exit__\r\n",
      "\u001b[1m    self.gen.throw(type, value, traceback)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/site-packages/xarray/tests/test_backends.py\u001b[0m:888: in create_tmp_file\r\n",
      "\u001b[1m    shutil.rmtree(temp_dir)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:494: in rmtree\r\n",
      "\u001b[1m    return _rmtree_unsafe(path, onerror)\u001b[0m\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:376: in _rmtree_unsafe\r\n",
      "\u001b[1m    onerror(os.listdir, path, sys.exc_info())\u001b[0m\r\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n",
      "\r\n",
      "path = '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn0alhpcc'\r\n",
      "onerror = <function rmtree.<locals>.onerror at 0x1c14228b70>\r\n",
      "\r\n",
      "\u001b[1m    def _rmtree_unsafe(path, onerror):\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m            if os.path.islink(path):\u001b[0m\r\n",
      "\u001b[1m                # symlinks to directories are forbidden, see bug #1669\u001b[0m\r\n",
      "\u001b[1m                raise OSError(\"Cannot call rmtree on a symbolic link\")\u001b[0m\r\n",
      "\u001b[1m        except OSError:\u001b[0m\r\n",
      "\u001b[1m            onerror(os.path.islink, path, sys.exc_info())\u001b[0m\r\n",
      "\u001b[1m            # can't continue even if onerror hook returns\u001b[0m\r\n",
      "\u001b[1m            return\u001b[0m\r\n",
      "\u001b[1m        names = []\u001b[0m\r\n",
      "\u001b[1m        try:\u001b[0m\r\n",
      "\u001b[1m>           names = os.listdir(path)\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           OSError: [Errno 24] Too many open files: '/var/folders/wd/6mcc6nzd4qx7cd_y1brbrsqc0000gn/T/tmpn0alhpcc'\u001b[0m\r\n",
      "\r\n",
      "\u001b[1m\u001b[31m../../anaconda3/lib/python3.6/shutil.py\u001b[0m:374: OSError\r\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\r\n",
      "<unknown>:198: DeprecationWarning: invalid escape sequence \\S\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/scipy/io/netcdf.py:317: RuntimeWarning: Cannot close a netcdf_file opened with mmap=True, when netcdf_variables or arrays referring to its data still exist. All data arrays obtained from such files refer directly to data on disk, and must be copied before the file can be cleanly closed. (See netcdf_file docstring for more information on mmap.)\r\n",
      "  ), category=RuntimeWarning)\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/coding/cftimeindex.py:160: FutureWarning: CFTimeIndex.data is deprecated and will be removed in a future version\r\n",
      "  if self.data:\r\n",
      "\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/tests/test_ufuncs.py:179: PendingDeprecationWarning: xarray.ufuncs will be deprecated when xarray no longer supports versions of numpy older than v1.13. Instead, use numpy ufuncs directly.\r\n",
      "  assert_identical(cos_pickled(a), xu.cos(a))\r\n",
      "/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/xarray/tests/test_ufuncs.py:179: PendingDeprecationWarning: xarray.ufuncs will be deprecated when xarray no longer supports versions of numpy older than v1.13. Instead, use numpy ufuncs directly.\r\n",
      "  assert_identical(cos_pickled(a), xu.cos(a))\r\n",
      "\r\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n",
      "\u001b[31m\u001b[1m 1 failed, 6258 passed, 586 skipped, 10 xfailed, 4 xpassed, 263 warnings in 106.29 seconds \u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!py.test --pyargs xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4Tv0PqwWfO2"
   },
   "source": [
    "###Importing dependencies for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3G1ohsM5NDrf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXbl4K8SNDrh"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_nfwsM_Wnfd"
   },
   "source": [
    "###Creating a random numpy array of (4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7FCmMnTNDrk"
   },
   "outputs": [],
   "source": [
    "data = np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNNpWmYUNDrm"
   },
   "outputs": [],
   "source": [
    "locs = {'IA','IL','IN'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GWbS8k5NDrn"
   },
   "outputs": [],
   "source": [
    "times = pd.date_range('2000-01-01',periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neHoDvbNNDrp"
   },
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQnOs3CIWu8Z"
   },
   "source": [
    "###Creating a DataArray of the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ll4AXx7NDrt"
   },
   "outputs": [],
   "source": [
    "foo = xr.DataArray(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53M5hTS1W3HV"
   },
   "source": [
    "###Describing & Evaluating the 'foo' and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3qCH_igNDrv",
    "outputId": "5ffc5ab9-064c-4e56-b49e-98ee46ebd869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (dim_0: 4, dim_1: 3)>\n",
       "array([[0.159038, 0.524521, 0.362766],\n",
       "       [0.271877, 0.955653, 0.153606],\n",
       "       [0.073735, 0.889903, 0.289227],\n",
       "       [0.21128 , 0.33466 , 0.53737 ]])\n",
       "Dimensions without coordinates: dim_0, dim_1"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vx4RPV2hNDrx",
    "outputId": "df4445b5-c26a-4d2b-8776-c8a1f51b545a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15903754, 0.52452053, 0.36276556],\n",
       "       [0.27187684, 0.95565284, 0.15360555],\n",
       "       [0.07373492, 0.88990319, 0.28922653],\n",
       "       [0.21127977, 0.33466035, 0.53737023]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLWdQiXENDr1",
    "outputId": "9c290824-2aec-4124-e2ec-87e392ae435c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dim_0', 'dim_1')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ITKSP12NDr3",
    "outputId": "744ed968-4a57-4272-e6b2-e42ea296cdaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coordinates:\n",
       "    *empty*"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UND_zIzLNDr6",
    "outputId": "587b1749-f4cb-4e50-c18a-c69786a1a5ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtNno7-fNDr8"
   },
   "outputs": [],
   "source": [
    "foo.name='foo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3311Iq4LNDr_"
   },
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZm3xcvPNDsA"
   },
   "outputs": [],
   "source": [
    "foo = xr.DataArray('MYD03.A2002185.0000.061.2017362174430.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw-PXENYNDsD",
    "outputId": "d0e71760-a8f9-42ec-e16e-e0471c93eae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array('MYD03.A2002185.0000.061.2017362174430.hdf', dtype='<U41')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Xmo_HoXNDsF",
    "outputId": "293af475-65d3-4684-f923-58f605d756e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8jkyu5tNDsI",
    "outputId": "1b568dae-79c3-46d6-b72a-260a7047abfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('MYD03.A2002185.0000.061.2017362174430.hdf', dtype='<U41')"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqOsPvtoXJP9"
   },
   "source": [
    "###Attempting to open the HDF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2ycIQ4XNDsK",
    "outputId": "e4c9a556-734d-4ae0-dbfb-a3fb42373a87"
   },
   "outputs": [
    {
     "ename": "MissingDimensionsError",
     "evalue": "'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDimensionsError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-44a62cca9f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, lock)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mconcat_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             drop_variables=drop_variables)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0m_protect_dataset_variables_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         decode_coords, drop_variables=drop_variables)\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mcoord_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncoord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mexpand_variable_dicts\u001b[0;34m(list_of_variable_dicts)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mvar_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0msanitized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'dimensions %r. xarray disallows such variables because they '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;34m'conflict with the coordinates used to label '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 'dimensions.' % (name, obj.dims))\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingDimensionsError\u001b[0m: 'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions."
     ]
    }
   ],
   "source": [
    "fio = xr.open_dataset('MYD03.A2002185.0000.061.2017362174430.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFJeUyn3XSeU"
   },
   "source": [
    "###Issue : There doesnt seem to be a direct support for HDF files with the Dataset function.\n",
    "###Feedback: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFtzhCucNDsN",
    "outputId": "998a74a3-fff4-496f-faea-23624bd70434"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8a5fa38bd53f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0m_assert_compat_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mcoerce_pandas_values\u001b[0;34m(objects)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "gem=xr.Dataset('MYD03.A2002185.0000.061.2017362174430.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_DeMxv0NDsQ",
    "outputId": "19126b97-ac30-4c4f-fd36-e7329975d31a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8e5391c8556e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/saviosebastian/Desktop/XArraysTest/MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "tim = xr.save_mfdataset('/Users/saviosebastian/Desktop/XArraysTest/MYD03.A2002185.0000.061.2017362174430.hdf',paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qObZT0ANDsT"
   },
   "outputs": [],
   "source": [
    "paths = ['%s.nc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLbbtjTxYJyV"
   },
   "source": [
    "###Attempting to read the Dataset through netCDF package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86enCB3kNDsU",
    "outputId": "c7ef1a7c-bdf5-49df-ae40-37c3a38a8019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYD03.A2002185.0000.061.2017362174430.hdf\n",
      "MYD06_L2.A2002185.0000.061.2018003215042.hdf\n",
      "------------------------------------\n",
      "[[-- -- -- ... -- -- --]\n",
      " [-- -- -- ... -- -- --]\n",
      " [-- -- -- ... -- -- --]\n",
      " ...\n",
      " [15.015028953552246 15.064740180969238 15.11345100402832 ...\n",
      "  38.32621383666992 38.37139129638672 38.41727066040039]\n",
      " [15.014087677001953 15.063920021057129 15.112410545349121 ...\n",
      "  38.32081985473633 38.366085052490234 38.41202926635742]\n",
      " [15.01251220703125 15.062713623046875 15.111570358276367 ...\n",
      "  38.31563949584961 38.36115646362305 38.4079704284668]]\n",
      "------------------------------------\n",
      "2762160\n",
      "------------------------------------\n",
      "1354\n",
      "2040\n",
      "odict_keys(['Latitude', 'Longitude', 'Scan Offset', 'Track Offset', 'Height Offset', 'Height', 'SensorZenith', 'SensorAzimuth', 'Range', 'SolarZenith', 'SolarAzimuth', 'Land/SeaMask', 'WaterPresent', 'gflags', 'Scan number', 'EV frames', 'SD frames', 'SV frames', 'EV start time', 'SD start time', 'SV start time', 'EV center time', 'Mirror side', 'SD Sun zenith', 'SD Sun azimuth', 'Moon Vector', 'L1 scan quality', 'Geo scan quality', 'orb_pos', 'orb_vel', 'T_inst2ECR', 'attitude_angles', 'sun_ref', 'num_impulse', 'impulse_enc', 'impulse_time', 'Scan Type', 'thermal_correction', 'attitude_quality', 'ephemeris_quality', 'Focal_length', 'band_position', 'detector_space', 'detector_offsets', 'T_offset', 'num_samples'])\n",
      "(2040, 1354, 2)\n",
      "------------------------------------\n",
      "odict_keys(['Latitude', 'Longitude', 'Scan_Start_Time', 'Solar_Zenith', 'Solar_Zenith_Day', 'Solar_Zenith_Night', 'Solar_Azimuth', 'Solar_Azimuth_Day', 'Solar_Azimuth_Night', 'Sensor_Zenith', 'Sensor_Zenith_Day', 'Sensor_Zenith_Night', 'Sensor_Azimuth', 'Sensor_Azimuth_Day', 'Sensor_Azimuth_Night', 'Brightness_Temperature', 'Surface_Temperature', 'Surface_Pressure', 'Cloud_Height_Method', 'Cloud_Top_Height', 'Cloud_Top_Height_Nadir', 'Cloud_Top_Height_Nadir_Day', 'Cloud_Top_Height_Nadir_Night', 'Cloud_Top_Pressure', 'Cloud_Top_Pressure_Nadir', 'Cloud_Top_Pressure_Night', 'Cloud_Top_Pressure_Nadir_Night', 'Cloud_Top_Pressure_Day', 'Cloud_Top_Pressure_Nadir_Day', 'Cloud_Top_Temperature', 'Cloud_Top_Temperature_Nadir', 'Cloud_Top_Temperature_Night', 'Cloud_Top_Temperature_Nadir_Night', 'Cloud_Top_Temperature_Day', 'Cloud_Top_Temperature_Nadir_Day', 'Tropopause_Height', 'Cloud_Fraction', 'Cloud_Fraction_Nadir', 'Cloud_Fraction_Night', 'Cloud_Fraction_Nadir_Night', 'Cloud_Fraction_Day', 'Cloud_Fraction_Nadir_Day', 'Cloud_Effective_Emissivity', 'Cloud_Effective_Emissivity_Nadir', 'Cloud_Effective_Emissivity_Night', 'Cloud_Effective_Emissivity_Nadir_Night', 'Cloud_Effective_Emissivity_Day', 'Cloud_Effective_Emissivity_Nadir_Day', 'Cloud_Top_Pressure_Infrared', 'Spectral_Cloud_Forcing', 'Cloud_Top_Pressure_From_Ratios', 'Radiance_Variance', 'Cloud_Phase_Infrared', 'Cloud_Phase_Infrared_Night', 'Cloud_Phase_Infrared_Day', 'Cloud_Phase_Infrared_1km', 'IRP_CTH_Consistency_Flag_1km', 'os_top_flag_1km', 'cloud_top_pressure_1km', 'cloud_top_height_1km', 'cloud_top_temperature_1km', 'cloud_emissivity_1km', 'cloud_top_method_1km', 'surface_temperature_1km', 'cloud_emiss11_1km', 'cloud_emiss12_1km', 'cloud_emiss13_1km', 'cloud_emiss85_1km', 'Cloud_Effective_Radius', 'Cloud_Effective_Radius_PCL', 'Cloud_Effective_Radius_16', 'Cloud_Effective_Radius_16_PCL', 'Cloud_Effective_Radius_37', 'Cloud_Effective_Radius_37_PCL', 'Cloud_Optical_Thickness', 'Cloud_Optical_Thickness_PCL', 'Cloud_Optical_Thickness_16', 'Cloud_Optical_Thickness_16_PCL', 'Cloud_Optical_Thickness_37', 'Cloud_Optical_Thickness_37_PCL', 'Cloud_Effective_Radius_1621', 'Cloud_Effective_Radius_1621_PCL', 'Cloud_Optical_Thickness_1621', 'Cloud_Optical_Thickness_1621_PCL', 'Cloud_Water_Path', 'Cloud_Water_Path_PCL', 'Cloud_Water_Path_1621', 'Cloud_Water_Path_1621_PCL', 'Cloud_Water_Path_16', 'Cloud_Water_Path_16_PCL', 'Cloud_Water_Path_37', 'Cloud_Water_Path_37_PCL', 'Cloud_Effective_Radius_Uncertainty', 'Cloud_Effective_Radius_Uncertainty_16', 'Cloud_Effective_Radius_Uncertainty_37', 'Cloud_Optical_Thickness_Uncertainty', 'Cloud_Optical_Thickness_Uncertainty_16', 'Cloud_Optical_Thickness_Uncertainty_37', 'Cloud_Water_Path_Uncertainty', 'Cloud_Effective_Radius_Uncertainty_1621', 'Cloud_Optical_Thickness_Uncertainty_1621', 'Cloud_Water_Path_Uncertainty_1621', 'Cloud_Water_Path_Uncertainty_16', 'Cloud_Water_Path_Uncertainty_37', 'Above_Cloud_Water_Vapor_094', 'IRW_Low_Cloud_Temperature_From_COP', 'Cloud_Phase_Optical_Properties', 'Cloud_Multi_Layer_Flag', 'Cirrus_Reflectance', 'Cirrus_Reflectance_Flag', 'Cloud_Mask_5km', 'Quality_Assurance_5km', 'Cloud_Mask_1km', 'Extinction_Efficiency_Ice', 'Asymmetry_Parameter_Ice', 'Single_Scatter_Albedo_Ice', 'Extinction_Efficiency_Liq', 'Asymmetry_Parameter_Liq', 'Single_Scatter_Albedo_Liq', 'Cloud_Mask_SPI', 'Retrieval_Failure_Metric', 'Retrieval_Failure_Metric_16', 'Retrieval_Failure_Metric_37', 'Retrieval_Failure_Metric_1621', 'Atm_Corr_Refl', 'Quality_Assurance_1km', 'Statistics_1km_sds'])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import sys\n",
    "\n",
    "num_args = len(sys.argv)\n",
    "if (num_args < 3) : \n",
    "\tprint (\"Insufficient arguments\")\n",
    "\texit()\n",
    "\n",
    "mod03_name = 'MYD03.A2002185.0000.061.2017362174430.hdf'\n",
    "mod06_name = 'MYD06_L2.A2002185.0000.061.2018003215042.hdf'\n",
    "\n",
    "print(mod03_name)\n",
    "print(mod06_name)\n",
    "\n",
    "rootgrp = Dataset(mod03_name, \"r\", format=\"NETCDF3\")\n",
    "latitude = rootgrp.variables[\"Latitude\"][:,:] \n",
    "longitude = rootgrp.variables[\"Longitude\"][:,:]\n",
    "print(\"------------------------------------\")\n",
    "print (longitude)\n",
    "print(\"------------------------------------\")\n",
    "print (longitude.size)\n",
    "print(\"------------------------------------\")\n",
    "data_ht = latitude.shape[0]\n",
    "data_wid = longitude.shape[1]\n",
    "print (data_wid)\n",
    "print (data_ht)\n",
    "\n",
    "print (rootgrp.variables.keys())\n",
    "\n",
    "#for name, variable in rootgrp.variables.items():\n",
    "#\tprint(\"=== VARNAME: \", name, \" =======\")\n",
    "#\tfor attrname in variable.ncattrs():\n",
    "#\t\tprint(\"{} -- {}\".format(attrname, getattr(variable, attrname)))\n",
    "\n",
    "\n",
    "rootgrp.close()\n",
    "\n",
    "rootgrp = Dataset(mod06_name, \"r\", format=\"NETCDF3\")\n",
    "cloud_mask_allbytes = rootgrp.variables[\"Cloud_Mask_1km\"][:,:,:] \n",
    "print(cloud_mask_allbytes.shape)\n",
    "print(\"------------------------------------\")\n",
    "print (rootgrp.variables.keys())\n",
    "rootgrp.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFkuwelrNDsW",
    "outputId": "ca6de0a9-62a0-49a7-f1c3-1d70a962346d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unrecognized engine for open_dataset: 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6a47f61caf0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'netCDF4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             raise ValueError('unrecognized engine for open_dataset: %r'\n\u001b[0;32m--> 341\u001b[0;31m                              % engine)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unrecognized engine for open_dataset: 'netCDF4'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_dataset('MYD03.A2002185.0000.061.2017362174430.hdf',engine='netCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc2G4NfzNDsZ",
    "outputId": "9702e481-fd2f-41cf-d949-868713100170"
   },
   "outputs": [
    {
     "ename": "MissingDimensionsError",
     "evalue": "'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDimensionsError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ddf0e69faf3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_disk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, lock)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mconcat_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             drop_variables=drop_variables)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0m_protect_dataset_variables_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         decode_coords, drop_variables=drop_variables)\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mcoord_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncoord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mexpand_variable_dicts\u001b[0;34m(list_of_variable_dicts)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mvar_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0msanitized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'dimensions %r. xarray disallows such variables because they '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;34m'conflict with the coordinates used to label '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 'dimensions.' % (name, obj.dims))\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingDimensionsError\u001b[0m: 'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions."
     ]
    }
   ],
   "source": [
    "ds_disk = xr.open_dataset('MYD03.A2002185.0000.061.2017362174430.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqfDMxgYNDsb"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7eAjmyxYZWl"
   },
   "source": [
    "###Attempting to read a default NetCDF4 NC file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wJG3oeONDse"
   },
   "outputs": [],
   "source": [
    "ds_disk = xr.open_dataset('sresa1b_ncar_ccsm3-example.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8pI9NZENDsf",
    "outputId": "4a6eab2d-e1c8-4040-83a4-ac4775c647dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, lat: 128, lon: 256, plev: 17, time: 1)\n",
       "Coordinates:\n",
       "  * lat        (lat) float32 -88.927734 -87.538704 ... 87.538704 88.927734\n",
       "  * lon        (lon) float32 0.0 1.40625 2.8125 ... 355.78125 357.1875 358.59375\n",
       "  * plev       (plev) float64 1e+05 9.25e+04 8.5e+04 7e+04 ... 3e+03 2e+03 1e+03\n",
       "  * time       (time) datetime64[ns] 2000-05-16T12:00:00\n",
       "Dimensions without coordinates: bnds\n",
       "Data variables:\n",
       "    area       (lat, lon) float32 ...\n",
       "    lat_bnds   (lat, bnds) float64 ...\n",
       "    lon_bnds   (lon, bnds) float64 ...\n",
       "    msk_rgn    (lat, lon) int32 ...\n",
       "    pr         (time, lat, lon) float32 ...\n",
       "    tas        (time, lat, lon) float32 ...\n",
       "    time_bnds  (time, bnds) float64 ...\n",
       "    ua         (time, plev, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    CVS_Id:              $Id$\n",
       "    creation_date:       \n",
       "    prg_ID:              Source file unknown Version unknown Date unknown\n",
       "    cmd_ln:              bds -x 256 -y 128 -m 23 -o /data/zender/data/dst_T85.nc\n",
       "    history:             Tue Oct 25 15:08:51 2005: ncks -O -x -v va -m sresa1...\n",
       "    table_id:            Table A1\n",
       "    title:               model output prepared for IPCC AR4\n",
       "    institution:         NCAR (National Center for Atmospheric \\nResearch, Bo...\n",
       "    source:              CCSM3.0, version beta19 (2004): \\natmosphere: CAM3.0...\n",
       "    contact:             ccsm@ucar.edu\n",
       "    project_id:          IPCC Fourth Assessment\n",
       "    Conventions:         CF-1.0\n",
       "    references:          Collins, W.D., et al., 2005:\\n The Community Climate...\n",
       "    acknowledgment:       Any use of CCSM data should acknowledge the contrib...\n",
       "    realization:         1\n",
       "    experiment_id:       720 ppm stabilization experiment (SRESA1B)\n",
       "    comment:             This simulation was initiated from year 2000 of \\n C...\n",
       "    model_name_english:  NCAR CCSM"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFiVUrIENDsm",
    "outputId": "fda4edfc-57de-497d-9c9d-d360027f4845"
   },
   "outputs": [
    {
     "ename": "MissingDimensionsError",
     "evalue": "'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDimensionsError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ec574047ceba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MYD03.A2002185.0000.061.2017362174430.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, lock)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mconcat_characters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             drop_variables=drop_variables)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0m_protect_dataset_variables_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_characters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_and_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         decode_coords, drop_variables=drop_variables)\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mcoord_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncoord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mexpand_variable_dicts\u001b[0;34m(list_of_variable_dicts)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mvar_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0msanitized_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'dimensions %r. xarray disallows such variables because they '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;34m'conflict with the coordinates used to label '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 'dimensions.' % (name, obj.dims))\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingDimensionsError\u001b[0m: 'Scan Type' has more than 1-dimension and the same name as one of its dimensions ('nscans', 'Scan Type'). xarray disallows such variables because they conflict with the coordinates used to label dimensions."
     ]
    }
   ],
   "source": [
    "foo = xr.open_dataset('MYD03.A2002185.0000.061.2017362174430.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HntUexe_NDso",
    "outputId": "a52f2c03-6925-4686-de21-0ea013a93528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000164"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.nbytes / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9o2pxeKNDsw",
    "outputId": "51c2a5c0-ed9a-4e49-a159-731697b2fc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module xarray.tutorial:\n",
      "\n",
      "load_dataset(name, cache=True, cache_dir='~/.xarray_tutorial_data', github_url='https://github.com/pydata/xarray-data', branch='master', **kws)\n",
      "    Load a dataset from the online repository (requires internet).\n",
      "    \n",
      "    If a local copy is found then always use that to avoid network traffic.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        Name of the netcdf file containing the dataset\n",
      "        ie. 'air_temperature'\n",
      "    cache_dir : string, optional\n",
      "        The directory in which to search for and write cached data.\n",
      "    cache : boolean, optional\n",
      "        If True, then cache data locally for use on subsequent calls\n",
      "    github_url : string\n",
      "        Github repository where the data is stored\n",
      "    branch : string\n",
      "        The git branch to download from\n",
      "    kws : dict, optional\n",
      "        Passed to xarray.open_dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xr.tutorial.load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYV4-aoCNDs1",
    "outputId": "8e1194f9-24da-4dc9-9a92-248567455fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYD03.A2002185.0000.061.2017362174430.hdf\r\n",
      "MYD06_L2.A2002185.0000.061.2018003215042.hdf\r\n",
      "SamVersionOfSavioCode.ipynb\r\n",
      "SamVersionOfSavioCode1.ipynb\r\n",
      "XArray-Test.ipynb\r\n",
      "sresa1b_ncar_ccsm3-example.nc\r\n",
      "test_echam_spectral.nc\r\n",
      "test_hgroups.nc\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_ozmcO0NDtA"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import os,datetime,sys,fnmatch\n",
    "from jdcal import gcal2jd\n",
    "#from plot_global_map import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-xNNBytNDtC"
   },
   "outputs": [],
   "source": [
    "MOD03_path = '/Users/saviosebastian/Desktop/XArraysTest/'\n",
    "MOD06_path = '/Users/saviosebastian/Desktop/XArraysTest/'\n",
    "satellite = 'Aqua'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47HC-y5RNDtF"
   },
   "outputs": [],
   "source": [
    "MOD03_fp = 'MYD03.A*.hdf'\n",
    "MOD06_fp = 'MYD06_L2.A*.hdf'\n",
    "MOD03_fn, MOD06_fn =[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1034
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 370,
     "status": "error",
     "timestamp": 1537890615390,
     "user": {
      "displayName": "Savio Sebastian",
      "photoUrl": "",
      "userId": "11901389089153341952"
     },
     "user_tz": 240
    },
    "id": "SkltYxmONDtH",
    "outputId": "aadfd09b-b32c-4f0a-b526-0c297de18d67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 657, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"<decorator-gen-121>\", line 2, in initialize\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 87, in catch_config_error\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 462, in initialize\n",
      "    self.init_gui_pylab()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 403, in init_gui_pylab\n",
      "    InteractiveShellApp.init_gui_pylab(self)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/shellapp.py\", line 213, in init_gui_pylab\n",
      "    r = enable(key)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-93836a901d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjdcal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgcal2jd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#from plot_global_map import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jdcal'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import os,datetime,sys,fnmatch\n",
    "from jdcal import gcal2jd\n",
    "#from plot_global_map import *\n",
    "import math\n",
    "\n",
    "def read_MODIS_level2_data(MOD06_file,MOD03_file):\n",
    "    print(MOD06_file)\n",
    "    print(MOD03_file)\n",
    "    print('reading the cloud mask from MOD06_L2 product')\n",
    "    MOD06 = Dataset(MOD06_file, 'r')\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(MOD06)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    CM1km = MOD06.variables['Cloud_Mask_1km']\n",
    "    print(CM1km)\n",
    "    CM   = (np.array(CM1km[:,:,0],dtype='byte') & 0b00000110) >>1\n",
    "    print('level-2 cloud mask array shape',CM.shape)\n",
    "\n",
    "    MOD03 = Dataset(MOD03_file,'r')\n",
    "    print('reading the lat-lon from MOD03 product')\n",
    "    lat  = MOD03.variables['Latitude']\n",
    "    print(lat)\n",
    "    lon  = MOD03.variables['Longitude']\n",
    "    print('level-2 lat-lon array shape',lat.shape)\n",
    "\n",
    "    return lat,lon,CM\n",
    "\n",
    "def value_locate(refx, x):\n",
    "    refx = np.array(refx)\n",
    "    x = np.array(x)\n",
    "    loc = np.zeros(len(x), dtype='int')\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        ix = x[i]\n",
    "        ind = ((refx - ix) <= 0).nonzero()[0]\n",
    "        if len(ind) == 0:\n",
    "            loc[i] = -1\n",
    "        else: loc[i] = ind[-1]\n",
    "\n",
    "    return loc\n",
    "\n",
    "def division(n, d):\n",
    "\n",
    "    div = np.zeros(len(d))\n",
    "    for i in range(len(d)):\n",
    "        if d[i] >0:\n",
    "          div[i]=n[i]/d[i]\n",
    "        else: div[i]=None \n",
    "\n",
    "    return div\n",
    "\n",
    "\n",
    "# beginning of the program\n",
    "if __name__ == '__main__':\n",
    "    import itertools\n",
    "    MOD03_path = '/Users/saviosebastian/Desktop/XArraysTest/'\n",
    "    MOD06_path = '/Users/saviosebastian/Desktop/XArraysTest/'\n",
    "    satellite = 'Aqua'\n",
    "\n",
    "    yr = [2008]\n",
    "    mn = [1] #np.arange(1,13)  #[1]\n",
    "    dy = [1] #np.arange(1,32) # [1] #np.arange(1,31)\n",
    "    # latitude and longtitude boundaries of level-3 grid\n",
    "    lat_bnd = np.arange(-90,91,1)\n",
    "    lon_bnd = np.arange(-180,180,1)\n",
    "    nlat = 180\n",
    "    nlon = 360\n",
    "\n",
    "    TOT_pix      = np.zeros(nlat*nlon)\n",
    "    CLD_pix      = np.zeros(nlat*nlon)\n",
    "\n",
    "    #for y,m,d in  itertools.product(yr,mn, dy):\n",
    "        #-------------find the MODIS prodcts--------------#\n",
    "    #    date = datetime.datetime(y,m,d)\n",
    "    #    JD01, JD02 = gcal2jd(y,1,1)\n",
    "    #    JD1, JD2 = gcal2jd(y,m,d)\n",
    "    #    JD = np.int((JD2+JD1)-(JD01+JD02) + 1)\n",
    "    #    granule_time = datetime.datetime(y,m,d,0,0)\n",
    "    #    while granule_time <= datetime.datetime(y,m,d,23,55):  # 23,55\n",
    "    #        print('granule time:',granule_time)\n",
    "    MOD03_fp = 'MYD03.A*.hdf'\n",
    "    MOD06_fp = 'MYD06_L2.A*.hdf'\n",
    "    MOD03_fn, MOD06_fn =[],[]\n",
    "    for MOD06_flist in  os.listdir(MOD06_path):\n",
    "        if fnmatch.fnmatch(MOD06_flist, MOD06_fp):\n",
    "           MOD06_fn = MOD06_flist\n",
    "    for MOD03_flist in  os.listdir(MOD03_path):\n",
    "        if fnmatch.fnmatch(MOD03_flist, MOD03_fp):\n",
    "           MOD03_fn = MOD03_flist\n",
    "    if MOD03_fn and MOD06_fn: # if both MOD06 and MOD03 products are in the directory\n",
    "                print('reading level 2 geolocation and cloud data')\n",
    "                print(MOD06_fn)\n",
    "                Lat,Lon,CM = read_MODIS_level2_data(MOD06_path+MOD06_fn,MOD03_path+MOD03_fn)\n",
    "                Lat=np.ravel(Lat)\n",
    "                Lon=np.ravel(Lon)\n",
    "                CM=np.ravel(CM)\n",
    "                print('Total Number of pixels in this granule (cloud mask CM>=0)',np.sum(CM>=0))\n",
    "                print('Total Number of cloudy pixels (cloud mask CM<=1)',np.sum(CM<=1))\n",
    "                print('cloud fraction of this granule',np.sum(CM<=1)/np.sum(CM>=0))\n",
    "                print('projecting granule on level3 lat lon grids')\n",
    "                lat_index = value_locate(lat_bnd,Lat)\n",
    "                lon_index = value_locate(lon_bnd,Lon)\n",
    "                latlon_index = lat_index*nlon + lon_index\n",
    "                print(latlon_index)\n",
    "                print('computing simple level3 statistics')\n",
    "                latlon_index_unique = np.unique(latlon_index)\n",
    "                print('this granule occupies',latlon_index_unique.size,'1x1 degree box')\n",
    "                for i in np.arange(latlon_index_unique.size):\n",
    "                    j=latlon_index_unique[i]\n",
    "                    TOT_pix[j] = TOT_pix[j]+np.sum(CM[np.where(latlon_index == j)]>=0)\n",
    "                    CLD_pix[j] = CLD_pix[j]+np.sum(CM[np.where(latlon_index == j)]<=1) \n",
    "                \n",
    "                #granule_time += datetime.timedelta(minutes=5)\n",
    "\n",
    "    print('derive the averaged Level-3 cloud fraction')\n",
    "    total_cloud_fraction  =  division(CLD_pix,TOT_pix).reshape([nlat,nlon])\n",
    "    print(np.nansum(total_cloud_fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPHfpRBeNDtI"
   },
   "outputs": [],
   "source": [
    "MOD06 = Dataset('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf', 'r')\n",
    "CM1km = MOD06.variables['Cloud_Mask_1km']\n",
    "print(CM1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD9PUlLhNDtL",
    "outputId": "86173c57-792f-4a08-99fc-fb3ea41bf00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "int8 Cloud_Mask_1km(Cell_Along_Swath_1km:mod06, Cell_Across_Swath_1km:mod06, Cloud_Mask_1km_Num_Bytes:mod06)\n",
      "    valid_range: [ 0 -1]\n",
      "    _FillValue: 0\n",
      "    long_name: MODIS Cloud Mask, L2 MOD06 QA Plan\n",
      "    units: none\n",
      "    scale_factor: 1.0\n",
      "    add_offset: 0.0\n",
      "    Parameter_Type: MODIS Input\n",
      "    Cell_Along_Swath_Sampling: [   1 2040    1]\n",
      "    Cell_Across_Swath_Sampling: [   1 1354    1]\n",
      "    Geolocation_Pointer: External MODIS geolocation product\n",
      "    description: See MODIS atmosphere QA plan for details                                            \n",
      "\n",
      "unlimited dimensions: \n",
      "current shape = (2040, 1354, 2)\n",
      "filling on\n"
     ]
    }
   ],
   "source": [
    "print(CM1km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQumXG_gNDtN",
    "outputId": "efa1d035-0040-48be-9f78-5c384887a2cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Cell_Along_Swath_5km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cell_Along_Swath_5km:mod06', size = 408\n",
      "), ('Cell_Across_Swath_5km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cell_Across_Swath_5km:mod06', size = 270\n",
      "), ('Band_Number:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Band_Number:mod06', size = 7\n",
      "), ('Band_Forcing:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Band_Forcing:mod06', size = 5\n",
      "), ('Band_Ratio:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Band_Ratio:mod06', size = 5\n",
      "), ('Cell_Along_Swath_1km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cell_Along_Swath_1km:mod06', size = 2040\n",
      "), ('Cell_Across_Swath_1km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cell_Across_Swath_1km:mod06', size = 1354\n",
      "), ('Cloud_Mask_5km_Num_Bytes:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cloud_Mask_5km_Num_Bytes:mod06', size = 2\n",
      "), ('QA_Parameter_5km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'QA_Parameter_5km:mod06', size = 10\n",
      "), ('Cloud_Mask_1km_Num_Bytes:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'Cloud_Mask_1km_Num_Bytes:mod06', size = 2\n",
      "), ('RadTran_NRE_Ice:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'RadTran_NRE_Ice:mod06', size = 12\n",
      "), ('RadTran_NWL:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'RadTran_NWL:mod06', size = 7\n",
      "), ('RadTran_NRE_Liq:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'RadTran_NRE_Liq:mod06', size = 18\n",
      "), ('SPI_nband:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'SPI_nband:mod06', size = 2\n",
      "), ('RFM_nband:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'RFM_nband:mod06', size = 3\n",
      "), ('ACR_nband:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'ACR_nband:mod06', size = 6\n",
      "), ('QA_Parameter_1km:mod06', <class 'netCDF4._netCDF4.Dimension'>: name = 'QA_Parameter_1km:mod06', size = 9\n",
      "), ('fakeDim17', <class 'netCDF4._netCDF4.Dimension'>: name = 'fakeDim17', size = 17\n",
      ")])\n"
     ]
    }
   ],
   "source": [
    "print(MOD06.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuS9C0UxNDtP",
    "outputId": "f58d0329-409a-4d1c-f3c9-40f8417d3286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n"
     ]
    }
   ],
   "source": [
    "print(MOD06.file_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQhdZL77NDtQ",
    "outputId": "6ae40c22-e38b-420c-b777-25e794b4d6f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODIS Level 2 Cloud Properties                                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MOD06.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jI-JomBoNDtU",
    "outputId": "35db1fc2-8fdb-4d52-f6d6-13771b64820b"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'xarray' has no attribute 'Dataframes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0373d4a341c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'xarray' has no attribute 'Dataframes'"
     ]
    }
   ],
   "source": [
    "help(xr.Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeXBDVGfNDtV",
    "outputId": "fd23dbb6-7279-4395-eb75-636ce52a3b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module xarray.core.dataset:\n",
      "\n",
      "class Dataset(collections.abc.Mapping, xarray.core.common.ImplementsDatasetReduce, xarray.core.common.DataWithCoords, xarray.core.formatting.ReprMixin)\n",
      " |  A multi-dimensional, in memory, array database.\n",
      " |  \n",
      " |  A dataset resembles an in-memory representation of a NetCDF file, and\n",
      " |  consists of variables, coordinates and attributes which together form a\n",
      " |  self describing dataset.\n",
      " |  \n",
      " |  Dataset implements the mapping interface with keys given by variable names\n",
      " |  and values given by DataArray objects for each variable name.\n",
      " |  \n",
      " |  One dimensional variables with name equal to their dimension are index\n",
      " |  coordinates used for label based indexing.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      xarray.core.common.ImplementsDatasetReduce\n",
      " |      xarray.core.common.DataWithCoords\n",
      " |      xarray.core.arithmetic.SupportsArithmetic\n",
      " |      xarray.core.common.AttrAccessMixin\n",
      " |      xarray.core.formatting.ReprMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(...)\n",
      " |      abs(a) -- Same as abs(a).\n",
      " |  \n",
      " |  __add__ = add(...)\n",
      " |      add(a, b) -- Same as a + b.\n",
      " |  \n",
      " |  __and__ = and_(...)\n",
      " |      and_(a, b) -- Same as a & b.\n",
      " |  \n",
      " |  __array__(self, dtype=None)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |      The 'in' operator will return true or false depending on whether\n",
      " |      'key' is an array in the dataset or not.\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __dask_graph__(self)\n",
      " |  \n",
      " |  __dask_keys__(self)\n",
      " |  \n",
      " |  __dask_postcompute__(self)\n",
      " |  \n",
      " |  __dask_postpersist__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, memo=None)\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |      Remove a variable from this dataset.\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __eq__ = array_eq(self, other)\n",
      " |  \n",
      " |  __floordiv__ = floordiv(...)\n",
      " |      floordiv(a, b) -- Same as a // b.\n",
      " |  \n",
      " |  __ge__ = ge(...)\n",
      " |      ge(a, b) -- Same as a>=b.\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Access variables or coordinates this dataset as a\n",
      " |      :py:class:`~xarray.DataArray`.\n",
      " |      \n",
      " |      Indexing with a list of names will return a new ``Dataset`` object.\n",
      " |  \n",
      " |  __gt__ = gt(...)\n",
      " |      gt(a, b) -- Same as a>b.\n",
      " |  \n",
      " |  __iadd__ = iadd(...)\n",
      " |      a = iadd(a, b) -- Same as a += b.\n",
      " |  \n",
      " |  __iand__ = iand(...)\n",
      " |      a = iand(a, b) -- Same as a &= b.\n",
      " |  \n",
      " |  __ifloordiv__ = ifloordiv(...)\n",
      " |      a = ifloordiv(a, b) -- Same as a //= b.\n",
      " |  \n",
      " |  __imod__ = imod(...)\n",
      " |      a = imod(a, b) -- Same as a %= b.\n",
      " |  \n",
      " |  __imul__ = imul(...)\n",
      " |      a = imul(a, b) -- Same as a *= b.\n",
      " |  \n",
      " |  __init__(self, data_vars=None, coords=None, attrs=None, compat='broadcast_equals')\n",
      " |      To load data from a file or file-like object, use the `open_dataset`\n",
      " |      function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data_vars : dict-like, optional\n",
      " |          A mapping from variable names to :py:class:`~xarray.DataArray`\n",
      " |          objects, :py:class:`~xarray.Variable` objects or tuples of the\n",
      " |          form ``(dims, data[, attrs])`` which can be used as arguments to\n",
      " |          create a new ``Variable``. Each dimension must have the same length\n",
      " |          in all variables in which it appears.\n",
      " |      coords : dict-like, optional\n",
      " |          Another mapping in the same form as the `variables` argument,\n",
      " |          except the each item is saved on the dataset as a \"coordinate\".\n",
      " |          These variables have an associated meaning: they describe\n",
      " |          constant/fixed/independent quantities, unlike the\n",
      " |          varying/measured/dependent quantities that belong in `variables`.\n",
      " |          Coordinates values may be given by 1-dimensional arrays or scalars,\n",
      " |          in which case `dims` do not need to be supplied: 1D arrays will be\n",
      " |          assumed to give index values along the dimension with the same\n",
      " |          name.\n",
      " |      attrs : dict-like, optional\n",
      " |          Global attributes to save on this dataset.\n",
      " |      compat : {'broadcast_equals', 'equals', 'identical'}, optional\n",
      " |          String indicating how to compare variables of the same name for\n",
      " |          potential conflicts when initializing this dataset:\n",
      " |      \n",
      " |          - 'broadcast_equals': all values must be equal when variables are\n",
      " |            broadcast against each other to ensure common dimensions.\n",
      " |          - 'equals': all values and dimensions must be the same.\n",
      " |          - 'identical': all values, dimensions and attributes must be the\n",
      " |            same.\n",
      " |  \n",
      " |  __invert__ = invert(...)\n",
      " |      invert(a) -- Same as ~a.\n",
      " |  \n",
      " |  __ior__ = ior(...)\n",
      " |      a = ior(a, b) -- Same as a |= b.\n",
      " |  \n",
      " |  __ipow__ = ipow(...)\n",
      " |      a = ipow(a, b) -- Same as a **= b.\n",
      " |  \n",
      " |  __isub__ = isub(...)\n",
      " |      a = isub(a, b) -- Same as a -= b.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __itruediv__ = itruediv(...)\n",
      " |      a = itruediv(a, b) -- Same as a /= b\n",
      " |  \n",
      " |  __ixor__ = ixor(...)\n",
      " |      a = ixor(a, b) -- Same as a ^= b.\n",
      " |  \n",
      " |  __le__ = le(...)\n",
      " |      le(a, b) -- Same as a<=b.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__ = lt(...)\n",
      " |      lt(a, b) -- Same as a<b.\n",
      " |  \n",
      " |  __mod__ = mod(...)\n",
      " |      mod(a, b) -- Same as a % b.\n",
      " |  \n",
      " |  __mul__ = mul(...)\n",
      " |      mul(a, b) -- Same as a * b.\n",
      " |  \n",
      " |  __ne__ = array_ne(self, other)\n",
      " |  \n",
      " |  __neg__ = neg(...)\n",
      " |      neg(a) -- Same as -a.\n",
      " |  \n",
      " |  __or__ = or_(...)\n",
      " |      or_(a, b) -- Same as a | b.\n",
      " |  \n",
      " |  __pos__ = pos(...)\n",
      " |      pos(a) -- Same as +a.\n",
      " |  \n",
      " |  __pow__ = pow(...)\n",
      " |      pow(a, b) -- Same as a ** b.\n",
      " |  \n",
      " |  __radd__ = add(...)\n",
      " |      add(a, b) -- Same as a + b.\n",
      " |  \n",
      " |  __rand__ = and_(...)\n",
      " |      and_(a, b) -- Same as a & b.\n",
      " |  \n",
      " |  __rfloordiv__ = floordiv(...)\n",
      " |      floordiv(a, b) -- Same as a // b.\n",
      " |  \n",
      " |  __rmod__ = mod(...)\n",
      " |      mod(a, b) -- Same as a % b.\n",
      " |  \n",
      " |  __rmul__ = mul(...)\n",
      " |      mul(a, b) -- Same as a * b.\n",
      " |  \n",
      " |  __ror__ = or_(...)\n",
      " |      or_(a, b) -- Same as a | b.\n",
      " |  \n",
      " |  __rpow__ = pow(...)\n",
      " |      pow(a, b) -- Same as a ** b.\n",
      " |  \n",
      " |  __rsub__ = sub(...)\n",
      " |      sub(a, b) -- Same as a - b.\n",
      " |  \n",
      " |  __rtruediv__ = truediv(...)\n",
      " |      truediv(a, b) -- Same as a / b.\n",
      " |  \n",
      " |  __rxor__ = xor(...)\n",
      " |      xor(a, b) -- Same as a ^ b.\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |      Add an array to this dataset.\n",
      " |      \n",
      " |      If value is a `DataArray`, call its `select_vars()` method, rename it\n",
      " |      to `key` and merge the contents of the resulting dataset into this\n",
      " |      dataset.\n",
      " |      \n",
      " |      If value is an `Variable` object (or tuple of form\n",
      " |      ``(dims, data[, attrs])``), add it to this dataset as a new\n",
      " |      variable.\n",
      " |  \n",
      " |  __sub__ = sub(...)\n",
      " |      sub(a, b) -- Same as a - b.\n",
      " |  \n",
      " |  __truediv__ = truediv(...)\n",
      " |      truediv(a, b) -- Same as a / b.\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |  \n",
      " |  __xor__ = xor(...)\n",
      " |      xor(a, b) -- Same as a ^ b.\n",
      " |  \n",
      " |  all(self, dim=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `all` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `all`.  By default `all` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `all` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `all` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  any(self, dim=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `any` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `any`.  By default `any` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `any` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `any` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  apply(self, func, keep_attrs=False, args=(), **kwargs)\n",
      " |      Apply a function over the data variables in this dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function which can be called in the form `f(x, **kwargs)` to\n",
      " |          transform each DataArray `x` in this dataset into another\n",
      " |          DataArray.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one. If False, the new object will\n",
      " |          be returned without attributes.\n",
      " |      args : tuple, optional\n",
      " |          Positional arguments passed on to `func`.\n",
      " |      **kwargs : dict\n",
      " |          Keyword arguments passed on to `func`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      applied : Dataset\n",
      " |          Resulting dataset from applying ``func`` over each data variable.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(np.random.randn(2, 3))\n",
      " |      >>> ds = xr.Dataset({'foo': da, 'bar': ('x', [-1, 2])})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Dimensions without coordinates: dim_0, dim_1, x\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948\n",
      " |          bar      (x) int64 -1 2\n",
      " |      >>> ds.apply(np.fabs)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Dimensions without coordinates: dim_0, dim_1, x\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948\n",
      " |          bar      (x) float64 1.0 2.0\n",
      " |  \n",
      " |  argmax(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `argmax` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `argmax`.  By default `argmax` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `argmax` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `argmax` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  argmin(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `argmin` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `argmin`.  By default `argmin` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `argmin` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `argmin` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  argsort(self, *args, **kwargs)\n",
      " |      a.argsort(axis=-1, kind='quicksort', order=None)\n",
      " |      \n",
      " |      Returns the indices that would sort this array.\n",
      " |      \n",
      " |      Refer to `numpy.argsort` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argsort : equivalent function\n",
      " |  \n",
      " |  assign(self, variables=None, **variables_kwargs)\n",
      " |      Assign new data variables to a Dataset, returning a new object\n",
      " |      with all the original variables in addition to the new ones.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      variables : mapping, value pairs\n",
      " |          Mapping from variables names to the new values. If the new values\n",
      " |          are callable, they are computed on the Dataset and assigned to new\n",
      " |          data variables. If the values are not callable, (e.g. a DataArray,\n",
      " |          scalar, or array), they are simply assigned.\n",
      " |      **variables_kwargs:\n",
      " |          The keyword arguments form of ``variables``.\n",
      " |          One of variables or variables_kwarg must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ds : Dataset\n",
      " |          A new Dataset with the new variables in addition to all the\n",
      " |          existing variables.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since ``kwargs`` is a dictionary, the order of your arguments may not\n",
      " |      be preserved, and so the order of the new variables is not well\n",
      " |      defined. Assigning multiple variables within the same ``assign`` is\n",
      " |      possible, but you cannot reference other variables created within the\n",
      " |      same ``assign`` call.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.assign\n",
      " |  \n",
      " |  astype(self, *args, **kwargs)\n",
      " |      a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n",
      " |      \n",
      " |      Copy of the array, cast to a specified type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or dtype\n",
      " |          Typecode or data-type to which the array is cast.\n",
      " |      order : {'C', 'F', 'A', 'K'}, optional\n",
      " |          Controls the memory layout order of the result.\n",
      " |          'C' means C order, 'F' means Fortran order, 'A'\n",
      " |          means 'F' order if all the arrays are Fortran contiguous,\n",
      " |          'C' order otherwise, and 'K' means as close to the\n",
      " |          order the array elements appear in memory as possible.\n",
      " |          Default is 'K'.\n",
      " |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      " |          Controls what kind of data casting may occur. Defaults to 'unsafe'\n",
      " |          for backwards compatibility.\n",
      " |      \n",
      " |            * 'no' means the data types should not be cast at all.\n",
      " |            * 'equiv' means only byte-order changes are allowed.\n",
      " |            * 'safe' means only casts which can preserve values are allowed.\n",
      " |            * 'same_kind' means only safe casts or casts within a kind,\n",
      " |              like float64 to float32, are allowed.\n",
      " |            * 'unsafe' means any data conversions may be done.\n",
      " |      subok : bool, optional\n",
      " |          If True, then sub-classes will be passed-through (default), otherwise\n",
      " |          the returned array will be forced to be a base-class array.\n",
      " |      copy : bool, optional\n",
      " |          By default, astype always returns a newly allocated array. If this\n",
      " |          is set to false, and the `dtype`, `order`, and `subok`\n",
      " |          requirements are satisfied, the input array is returned instead\n",
      " |          of a copy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      arr_t : ndarray\n",
      " |          Unless `copy` is False and the other conditions for returning the input\n",
      " |          array are satisfied (see description for `copy` input parameter), `arr_t`\n",
      " |          is a new array of the same shape as the input array, with dtype, order\n",
      " |          given by `dtype`, `order`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Starting in NumPy 1.9, astype method now returns an error if the string\n",
      " |      dtype to cast to is not long enough in 'safe' casting mode to hold the max\n",
      " |      value of integer/float array that is being casted. Previously the casting\n",
      " |      was allowed even if the result was truncated.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ComplexWarning\n",
      " |          When casting from complex to float or int. To avoid this,\n",
      " |          one should use ``a.real.astype(t)``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1, 2, 2.5])\n",
      " |      >>> x\n",
      " |      array([ 1. ,  2. ,  2.5])\n",
      " |      \n",
      " |      >>> x.astype(int)\n",
      " |      array([1, 2, 2])\n",
      " |  \n",
      " |  bfill(self, dim, limit=None)\n",
      " |      Fill NaN values by propogating values backward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int, default None\n",
      " |          The maximum number of consecutive NaN values to backward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  broadcast_equals(self, other)\n",
      " |      Two Datasets are broadcast equal if they are equal after\n",
      " |      broadcasting all variables against each other.\n",
      " |      \n",
      " |      For example, variables that are scalar in one dataset but non-scalar in\n",
      " |      the other dataset can still be broadcast equal if the the non-scalar\n",
      " |      variable is a constant.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.equals\n",
      " |      Dataset.identical\n",
      " |  \n",
      " |  chunk(self, chunks=None, name_prefix='xarray-', token=None, lock=False)\n",
      " |      Coerce all arrays in this dataset into dask arrays with the given\n",
      " |      chunks.\n",
      " |      \n",
      " |      Non-dask arrays in this dataset will be converted to dask arrays. Dask\n",
      " |      arrays will be rechunked to the given chunk sizes.\n",
      " |      \n",
      " |      If neither chunks is not provided for one or more dimensions, chunk\n",
      " |      sizes along that dimension will not be updated; non-dask arrays will be\n",
      " |      converted into dask arrays with a single block.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunks : int or dict, optional\n",
      " |          Chunk sizes along each dimension, e.g., ``5`` or\n",
      " |          ``{'x': 5, 'y': 5}``.\n",
      " |      name_prefix : str, optional\n",
      " |          Prefix for the name of any new dask arrays.\n",
      " |      token : str, optional\n",
      " |          Token uniquely identifying this dataset.\n",
      " |      lock : optional\n",
      " |          Passed on to :py:func:`dask.array.from_array`, if the array is not\n",
      " |          already as dask array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      chunked : xarray.Dataset\n",
      " |  \n",
      " |  clip(self, *args, **kwargs)\n",
      " |      a.clip(min=None, max=None, out=None)\n",
      " |      \n",
      " |      Return an array whose values are limited to ``[min, max]``.\n",
      " |      One of max or min must be given.\n",
      " |      \n",
      " |      Refer to `numpy.clip` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.clip : equivalent function\n",
      " |  \n",
      " |  combine_first(self, other)\n",
      " |      Combine two Datasets, default to data_vars of self.\n",
      " |      \n",
      " |      The new coordinates follow the normal broadcasting and alignment rules\n",
      " |      of ``join='outer'``.  Vacant cells in the expanded coordinates are\n",
      " |      filled with np.nan.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataArray\n",
      " |          Used to fill all matching missing values in this array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  compute(self, **kwargs)\n",
      " |      Manually trigger loading of this dataset's data from disk or a\n",
      " |      remote source into memory and return a new dataset. The original is\n",
      " |      left unaltered.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.array.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.compute\n",
      " |  \n",
      " |  conj(self, *args, **kwargs)\n",
      " |      a.conj()\n",
      " |      \n",
      " |      Complex-conjugate all elements.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  conjugate(self, *args, **kwargs)\n",
      " |      a.conjugate()\n",
      " |      \n",
      " |      Return the complex conjugate, element-wise.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  copy(self, deep=False, data=None)\n",
      " |      Returns a copy of this dataset.\n",
      " |      \n",
      " |      If `deep=True`, a deep copy is made of each of the component variables.\n",
      " |      Otherwise, a shallow copy of each of the component variable is made, so\n",
      " |      that the underlying memory region of the new dataset is the same as in\n",
      " |      the original dataset.\n",
      " |      \n",
      " |      Use `data` to create a new object with the same structure as\n",
      " |      original but entirely new data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional\n",
      " |          Whether each component variable is loaded into memory and copied onto\n",
      " |          the new object. Default is True.\n",
      " |      data : dict-like, optional\n",
      " |          Data to use in the new object. Each item in `data` must have same\n",
      " |          shape as corresponding data variable in original. When `data` is\n",
      " |          used, `deep` is ignored for the data variables and only used for\n",
      " |          coords.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : Dataset\n",
      " |          New object with dimensions, attributes, coordinates, name, encoding,\n",
      " |          and optionally data copied from original.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Shallow copy versus deep copy\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.random.randn(2, 3))\n",
      " |      >>> ds = xr.Dataset({'foo': da, 'bar': ('x', [-1, 2])}, \n",
      " |                          coords={'x': ['one', 'two']})\n",
      " |      >>> ds.copy()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003\n",
      " |          bar      (x) int64 -1 2\n",
      " |      >>> ds_0 = ds.copy(deep=False)\n",
      " |      >>> ds_0['foo'][0, 0] = 7\n",
      " |      >>> ds_0\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n",
      " |          bar      (x) int64 -1 2\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      Changing the data using the ``data`` argument maintains the \n",
      " |      structure of the original object, but with the new data. Original\n",
      " |      object is unaffected.\n",
      " |      \n",
      " |      >>> ds.copy(data={'foo': np.arange(6).reshape(2, 3), 'bar': ['a', 'b']})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n",
      " |          bar      (x) <U1 'a' 'b'\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.copy\n",
      " |  \n",
      " |  count(self, dim=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `count` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `count`.  By default `count` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `count` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `count` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  cumprod(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Apply `cumprod` along some dimension of Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension over which to apply `cumprod`.\n",
      " |              axis : int or sequence of int, optional\n",
      " |                  Axis over which to apply `cumprod`. Only one of the 'dim'\n",
      " |                  and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumprod`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : Dataset\n",
      " |          New Dataset object with `cumprod` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  cumsum(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Apply `cumsum` along some dimension of Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension over which to apply `cumsum`.\n",
      " |              axis : int or sequence of int, optional\n",
      " |                  Axis over which to apply `cumsum`. Only one of the 'dim'\n",
      " |                  and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumsum`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : Dataset\n",
      " |          New Dataset object with `cumsum` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  diff(self, dim, n=1, label='upper')\n",
      " |      Calculate the n-th order discrete difference along given axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          Dimension over which to calculate the finite difference.\n",
      " |      n : int, optional\n",
      " |          The number of times values are differenced.\n",
      " |      label : str, optional\n",
      " |          The new coordinate in dimension ``dim`` will have the\n",
      " |          values of either the minuend's or subtrahend's coordinate\n",
      " |          for values 'upper' and 'lower', respectively.  Other\n",
      " |          values are not supported.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      difference : same type as caller\n",
      " |          The n-th order finite difference of this object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset({'foo': ('x', [5, 5, 6, 6])})\n",
      " |      >>> ds.diff('x')\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 3)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 1 2 3\n",
      " |      Data variables:\n",
      " |          foo      (x) int64 0 1 0\n",
      " |      >>> ds.diff('x', 2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2)\n",
      " |      Coordinates:\n",
      " |      * x        (x) int64 2 3\n",
      " |      Data variables:\n",
      " |      foo      (x) int64 1 -1\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.differentiate\n",
      " |  \n",
      " |  differentiate(self, coord, edge_order=1, datetime_unit=None)\n",
      " |      Differentiate with the second order accurate central\n",
      " |      differences.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This feature is limited to simple cartesian geometry, i.e. coord\n",
      " |          must be one dimensional.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coord: str\n",
      " |          The coordinate to be used to compute the gradient.\n",
      " |      edge_order: 1 or 2. Default 1\n",
      " |          N-th order accurate differences at the boundaries.\n",
      " |      datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n",
      " |          'us', 'ns', 'ps', 'fs', 'as'}\n",
      " |          Unit to compute gradient. Only valid for datetime coordinate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      differentiated: Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.gradient: corresponding numpy function\n",
      " |  \n",
      " |  drop(self, labels, dim=None)\n",
      " |      Drop variables or index labels from this dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : scalar or list of scalars\n",
      " |          Name(s) of variables or index labels to drop.\n",
      " |      dim : None or str, optional\n",
      " |          Dimension along which to drop index labels. By default (if\n",
      " |          ``dim is None``), drops variables rather than index labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dropped : Dataset\n",
      " |  \n",
      " |  dropna(self, dim, how='any', thresh=None, subset=None)\n",
      " |      Returns a new dataset with dropped labels for missing values along\n",
      " |      the provided dimension.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Dimension along which to drop missing values. Dropping along\n",
      " |          multiple dimensions simultaneously is not yet supported.\n",
      " |      how : {'any', 'all'}, optional\n",
      " |          * any : if any NA values are present, drop that label\n",
      " |          * all : if all values are NA, drop that label\n",
      " |      thresh : int, default None\n",
      " |          If supplied, require this many non-NA values.\n",
      " |      subset : sequence, optional\n",
      " |          Subset of variables to check for missing values. By default, all\n",
      " |          variables in the dataset are checked.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  dump_to_store(self, store, encoder=None, sync=True, encoding=None, unlimited_dims=None, compute=True)\n",
      " |      Store dataset contents to a backends.*DataStore object.\n",
      " |  \n",
      " |  equals(self, other)\n",
      " |      Two Datasets are equal if they have matching variables and\n",
      " |      coordinates, all of which are equal.\n",
      " |      \n",
      " |      Datasets can still be equal (like pandas objects) if they have NaN\n",
      " |      values in the same locations.\n",
      " |      \n",
      " |      This method is necessary because `v1 == v2` for ``Dataset``\n",
      " |      does element-wise comparisons (like numpy.ndarrays).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.broadcast_equals\n",
      " |      Dataset.identical\n",
      " |  \n",
      " |  expand_dims(self, dim, axis=None)\n",
      " |      Return a new object with an additional axis (or axes) inserted at the\n",
      " |      corresponding position in the array shape.\n",
      " |      \n",
      " |      If dim is already a scalar coordinate, it will be promoted to a 1D\n",
      " |      coordinate consisting of a single value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str.\n",
      " |          Dimensions to include on the new variable.\n",
      " |          dimensions are inserted with length 1.\n",
      " |      axis : integer, list (or tuple) of integers, or None\n",
      " |          Axis position(s) where new axis is to be inserted (position(s) on\n",
      " |          the result array). If a list (or tuple) of integers is passed,\n",
      " |          multiple axes are inserted. In this case, dim arguments should be\n",
      " |          the same length list. If axis=None is passed, all the axes will\n",
      " |          be inserted to the start of the result array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      expanded : same type as caller\n",
      " |          This object, but with an additional dimension(s).\n",
      " |  \n",
      " |  ffill(self, dim, limit=None)\n",
      " |      Fill NaN values by propogating values forward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int, default None\n",
      " |          The maximum number of consecutive NaN values to forward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  fillna(self, value)\n",
      " |      Fill missing values in this object.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic, except the result is aligned to this\n",
      " |      object (``join='left'``) instead of aligned to the intersection of\n",
      " |      index coordinates (``join='inner'``).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar, ndarray, DataArray, dict or Dataset\n",
      " |          Used to fill all matching missing values in this dataset's data\n",
      " |          variables. Scalars, ndarrays or DataArrays arguments are used to\n",
      " |          fill all data with aligned coordinates (for DataArrays).\n",
      " |          Dictionaries or datasets match data variables and then align\n",
      " |          coordinates if necessary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  filter_by_attrs(self, **kwargs)\n",
      " |      Returns a ``Dataset`` with variables that match specific conditions.\n",
      " |      \n",
      " |      Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n",
      " |      containing only the variables for which all the filter tests pass.\n",
      " |      These tests are either ``key=value`` for which the attribute ``key``\n",
      " |      has the exact value ``value`` or the callable passed into\n",
      " |      ``key=callable`` returns True. The callable will be passed a single\n",
      " |      value, either the value of the attribute ``key`` or ``None`` if the\n",
      " |      DataArray does not have an attribute with the name ``key``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : key=value\n",
      " |          key : str\n",
      " |              Attribute name.\n",
      " |          value : callable or obj\n",
      " |              If value is a callable, it should return a boolean in the form\n",
      " |              of bool = func(attr) where attr is da.attrs[key].\n",
      " |              Otherwise, value will be compared to the each\n",
      " |              DataArray's attrs[key].\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      new : Dataset\n",
      " |          New dataset with variables filtered by attribute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> # Create an example dataset:\n",
      " |      >>> import numpy as np\n",
      " |      >>> import pandas as pd\n",
      " |      >>> import xarray as xr\n",
      " |      >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n",
      " |      >>> precip = 10 * np.random.rand(2, 2, 3)\n",
      " |      >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
      " |      >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n",
      " |      >>> dims = ['x', 'y', 'time']\n",
      " |      >>> temp_attr = dict(standard_name='air_potential_temperature')\n",
      " |      >>> precip_attr = dict(standard_name='convective_precipitation_flux')\n",
      " |      >>> ds = xr.Dataset({\n",
      " |      ...         'temperature': (dims,  temp, temp_attr),\n",
      " |      ...         'precipitation': (dims, precip, precip_attr)},\n",
      " |      ...                 coords={\n",
      " |      ...         'lon': (['x', 'y'], lon),\n",
      " |      ...         'lat': (['x', 'y'], lat),\n",
      " |      ...         'time': pd.date_range('2014-09-06', periods=3),\n",
      " |      ...         'reference_time': pd.Timestamp('2014-09-05')})\n",
      " |      >>> # Get variables matching a specific standard_name.\n",
      " |      >>> ds.filter_by_attrs(standard_name='convective_precipitation_flux')\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (time: 3, x: 2, y: 2)\n",
      " |      Coordinates:\n",
      " |        * x               (x) int64 0 1\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * y               (y) int64 0 1\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |          lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |      Data variables:\n",
      " |          precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...\n",
      " |      >>> # Get all variables that have a standard_name attribute.\n",
      " |      >>> standard_name = lambda v: v is not None\n",
      " |      >>> ds.filter_by_attrs(standard_name=standard_name)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (time: 3, x: 2, y: 2)\n",
      " |      Coordinates:\n",
      " |          lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * x               (x) int64 0 1\n",
      " |        * y               (y) int64 0 1\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |      Data variables:\n",
      " |          temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...\n",
      " |          precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...\n",
      " |  \n",
      " |  identical(self, other)\n",
      " |      Like equals, but also checks all dataset attributes and the\n",
      " |      attributes on all variables and coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.broadcast_equals\n",
      " |      Dataset.equals\n",
      " |  \n",
      " |  info(self, buf=None)\n",
      " |      Concise summary of a Dataset variables and attributes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : writable buffer, defaults to sys.stdout\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.assign\n",
      " |      netCDF's ncdump\n",
      " |  \n",
      " |  interp(self, coords=None, method='linear', assume_sorted=False, kwargs={}, **coords_kwargs)\n",
      " |      Multidimensional interpolation of Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coords : dict, optional\n",
      " |          Mapping from dimension names to the new coordinates.\n",
      " |          New coordinate can be a scalar, array-like or DataArray.\n",
      " |          If DataArrays are passed as new coordates, their dimensions are\n",
      " |          used for the broadcasting.\n",
      " |      method: string, optional.\n",
      " |          {'linear', 'nearest'} for multidimensional array,\n",
      " |          {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n",
      " |          for 1-dimensional array. 'linear' is used by default.\n",
      " |      assume_sorted: boolean, optional\n",
      " |          If False, values of coordinates that are interpolated over can be\n",
      " |          in any order and they are sorted first. If True, interpolated\n",
      " |          coordinates are assumed to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs: dictionary, optional\n",
      " |          Additional keyword passed to scipy's interpolator.\n",
      " |      **coords_kwarg : {dim: coordinate, ...}, optional\n",
      " |          The keyword arguments form of ``coords``.\n",
      " |          One of coords or coords_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated: xr.Dataset\n",
      " |          New dataset on the new coordinates.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      scipy is required.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      scipy.interpolate.interp1d\n",
      " |      scipy.interpolate.interpn\n",
      " |  \n",
      " |  interp_like(self, other, method='linear', assume_sorted=False, kwargs={})\n",
      " |      Interpolate this object onto the coordinates of another object,\n",
      " |      filling the out of range values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to an 1d array-like, which provides coordinates upon\n",
      " |          which to index the variables in this dataset.\n",
      " |      method: string, optional.\n",
      " |          {'linear', 'nearest'} for multidimensional array,\n",
      " |          {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n",
      " |          for 1-dimensional array. 'linear' is used by default.\n",
      " |      assume_sorted: boolean, optional\n",
      " |          If False, values of coordinates that are interpolated over can be\n",
      " |          in any order and they are sorted first. If True, interpolated\n",
      " |          coordinates are assumed to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs: dictionary, optional\n",
      " |          Additional keyword passed to scipy's interpolator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated: xr.Dataset\n",
      " |          Another dataset by interpolating this dataset's data along the\n",
      " |          coordinates of the other object.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      scipy is required.\n",
      " |      If the dataset has object-type coordinates, reindex is used for these\n",
      " |      coordinates instead of the interpolation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.interp\n",
      " |      Dataset.reindex_like\n",
      " |  \n",
      " |  interpolate_na(self, dim=None, method='linear', limit=None, use_coordinate=True, **kwargs)\n",
      " |      Interpolate values according to different methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to interpolate.\n",
      " |      method : {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
      " |                'polynomial', 'barycentric', 'krog', 'pchip',\n",
      " |                'spline'}, optional\n",
      " |          String indicating which method to use for interpolation:\n",
      " |      \n",
      " |          - 'linear': linear interpolation (Default). Additional keyword\n",
      " |            arguments are passed to ``numpy.interp``\n",
      " |          - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
      " |            'polynomial': are passed to ``scipy.interpolate.interp1d``. If\n",
      " |            method=='polynomial', the ``order`` keyword argument must also be\n",
      " |            provided.\n",
      " |          - 'barycentric', 'krog', 'pchip', 'spline': use their respective\n",
      " |            ``scipy.interpolate`` classes.\n",
      " |      use_coordinate : boolean or str, default True\n",
      " |          Specifies which index to use as the x values in the interpolation\n",
      " |          formulated as `y = f(x)`. If False, values are treated as if\n",
      " |          eqaully-spaced along `dim`. If True, the IndexVariable `dim` is\n",
      " |          used. If use_coordinate is a string, it specifies the name of a\n",
      " |          coordinate variariable to use as the index.\n",
      " |      limit : int, default None\n",
      " |          Maximum number of consecutive NaNs to fill. Must be greater than 0\n",
      " |          or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.interp\n",
      " |      scipy.interpolate\n",
      " |  \n",
      " |  isel(self, indexers=None, drop=False, **indexers_kwargs)\n",
      " |      Returns a new dataset with each array indexed along the specified\n",
      " |      dimension(s).\n",
      " |      \n",
      " |      This method selects values from each array using its `__getitem__`\n",
      " |      method, except this method does not require knowing the order of\n",
      " |      each array's dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by integers, slice objects or arrays.\n",
      " |          indexer can be a integer, slice, array-like or DataArray.\n",
      " |          If DataArrays are passed as indexers, xarray-style indexing will be\n",
      " |          carried out. See :ref:`indexing` for the details.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      drop : bool, optional\n",
      " |          If ``drop=True``, drop coordinates variables indexed by integers\n",
      " |          instead of making them scalar.\n",
      " |      **indexers_kwarg : {dim: indexer, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          array and dimension is indexed by the appropriate indexers.\n",
      " |          If indexer DataArrays have coordinates that do not conflict with\n",
      " |          this object, then these coordinates will be attached.\n",
      " |          In general, each array's data will be a view of the array's data\n",
      " |          in this dataset, unless vectorized indexing was triggered by using\n",
      " |          an array indexer, in which case the data will be a copy.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel\n",
      " |      DataArray.isel\n",
      " |  \n",
      " |  isel_points(self, dim='points', **indexers)\n",
      " |      Returns a new dataset with each array indexed pointwise along the\n",
      " |      specified dimension(s).\n",
      " |      \n",
      " |      This method selects pointwise values from each array and is akin to\n",
      " |      the NumPy indexing behavior of `arr[[0, 1], [0, 1]]`, except this\n",
      " |      method does not require knowing the order of each array's dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or DataArray or pandas.Index or other list-like object, optional\n",
      " |          Name of the dimension to concatenate along. If dim is provided as a\n",
      " |          string, it must be a new dimension name, in which case it is added\n",
      " |          along axis=0. If dim is provided as a DataArray or Index or\n",
      " |          list-like object, its name, which must not be present in the\n",
      " |          dataset, is used as the dimension to concatenate along and the\n",
      " |          values are added as a coordinate.\n",
      " |      **indexers : {dim: indexer, ...}\n",
      " |          Keyword arguments with names matching dimensions and values given\n",
      " |          by array-like objects. All indexers must be the same length and\n",
      " |          1 dimensional.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          array and dimension is indexed by the appropriate indexers. With\n",
      " |          pointwise indexing, the new Dataset will always be a copy of the\n",
      " |          original.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel\n",
      " |      Dataset.isel\n",
      " |      Dataset.sel_points\n",
      " |      DataArray.isel_points\n",
      " |  \n",
      " |  isnull(self, *args, **kwargs)\n",
      " |  \n",
      " |  load(self, **kwargs)\n",
      " |      Manually trigger loading of this dataset's data from disk or a\n",
      " |      remote source into memory and return this dataset.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.array.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.compute\n",
      " |  \n",
      " |  max(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `max` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `max`.  By default `max` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `max` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `max` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  mean(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `mean` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `mean`.  By default `mean` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `mean` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `mean` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  median(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `median` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `median`.  By default `median` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `median` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `median` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  merge(self, other, inplace=False, overwrite_vars=frozenset(), compat='no_conflicts', join='outer')\n",
      " |      Merge the arrays of two datasets into a single dataset.\n",
      " |      \n",
      " |      This method generally not allow for overriding data, with the exception\n",
      " |      of attributes, which are ignored on the second dataset. Variables with\n",
      " |      the same name are checked for conflicts via the equals or identical\n",
      " |      methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or castable to Dataset\n",
      " |          Dataset or variables to merge with this dataset.\n",
      " |      inplace : bool, optional\n",
      " |          If True, merge the other dataset into this dataset in-place.\n",
      " |          Otherwise, return a new dataset object.\n",
      " |      overwrite_vars : str or sequence, optional\n",
      " |          If provided, update variables of these name(s) without checking for\n",
      " |          conflicts in this dataset.\n",
      " |      compat : {'broadcast_equals', 'equals', 'identical',\n",
      " |                'no_conflicts'}, optional\n",
      " |          String indicating how to compare variables of the same name for\n",
      " |          potential conflicts:\n",
      " |      \n",
      " |          - 'broadcast_equals': all values must be equal when variables are\n",
      " |            broadcast against each other to ensure common dimensions.\n",
      " |          - 'equals': all values and dimensions must be the same.\n",
      " |          - 'identical': all values, dimensions and attributes must be the\n",
      " |            same.\n",
      " |          - 'no_conflicts': only values which are not null in both datasets\n",
      " |            must be equal. The returned dataset then contains the combination\n",
      " |            of all non-null values.\n",
      " |      join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n",
      " |          Method for joining ``self`` and ``other`` along shared dimensions:\n",
      " |      \n",
      " |          - 'outer': use the union of the indexes\n",
      " |          - 'inner': use the intersection of the indexes\n",
      " |          - 'left': use indexes from ``self``\n",
      " |          - 'right': use indexes from ``other``\n",
      " |          - 'exact': error instead of aligning non-equal indexes\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      merged : Dataset\n",
      " |          Merged dataset.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      MergeError\n",
      " |          If any variables conflict (see ``compat``).\n",
      " |  \n",
      " |  min(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `min` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `min`.  By default `min` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `min` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `min` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  notnull(self, *args, **kwargs)\n",
      " |  \n",
      " |  persist(self, **kwargs)\n",
      " |      Trigger computation, keeping data as dask arrays\n",
      " |      \n",
      " |      This operation can be used to trigger computation on underlying dask\n",
      " |      arrays, similar to ``.compute()``.  However this operation keeps the\n",
      " |      data as dask arrays.  This is particularly useful when using the\n",
      " |      dask.distributed scheduler and you want to load a large amount of data\n",
      " |      into distributed memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.persist``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.persist\n",
      " |  \n",
      " |  prod(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `prod` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `prod`.  By default `prod` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `prod` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `prod` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  quantile(self, q, dim=None, interpolation='linear', numeric_only=False, keep_attrs=False)\n",
      " |      Compute the qth quantile of the data along the specified dimension.\n",
      " |      \n",
      " |      Returns the qth quantiles(s) of the array elements for each variable\n",
      " |      in the Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : float in range of [0,1] (or sequence of floats)\n",
      " |          Quantile to compute, which must be between 0 and 1 inclusive.\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply quantile.\n",
      " |      interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n",
      " |          This optional parameter specifies the interpolation method to\n",
      " |          use when the desired quantile lies between two data points\n",
      " |          ``i < j``:\n",
      " |      \n",
      " |              * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n",
      " |                the fractional part of the index surrounded by ``i`` and\n",
      " |                ``j``.\n",
      " |              * lower: ``i``.\n",
      " |              * higher: ``j``.\n",
      " |              * nearest: ``i`` or ``j``, whichever is nearest.\n",
      " |              * midpoint: ``(i + j) / 2``.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      numeric_only : bool, optional\n",
      " |          If True, only apply ``func`` to variables with a numeric dtype.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      quantiles : Dataset\n",
      " |          If `q` is a single quantile, then the result is a scalar for each\n",
      " |          variable in data_vars. If multiple percentiles are given, first\n",
      " |          axis of the result corresponds to the quantile and a quantile\n",
      " |          dimension is added to the return Dataset. The other dimensions are\n",
      " |          the dimensions that remain after the reduction of the array.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.nanpercentile, pandas.Series.quantile, DataArray.quantile\n",
      " |  \n",
      " |  rank(self, dim, pct=False, keep_attrs=False)\n",
      " |      Ranks the data.\n",
      " |      \n",
      " |      Equal values are assigned a rank that is the average of the ranks that\n",
      " |      would have been otherwise assigned to all of the values within that set.\n",
      " |      Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n",
      " |      \n",
      " |      NaNs in the input array are returned as NaNs.\n",
      " |      \n",
      " |      The `bottleneck` library is required.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Dimension over which to compute rank.\n",
      " |      pct : bool, optional\n",
      " |          If True, compute percentage ranks, otherwise compute integer ranks.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ranked : Dataset\n",
      " |          Variables that do not depend on `dim` are dropped.\n",
      " |  \n",
      " |  reduce(self, func, dim=None, keep_attrs=False, numeric_only=False, allow_lazy=False, **kwargs)\n",
      " |      Reduce this dataset by applying `func` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function which can be called in the form\n",
      " |          `f(x, axis=axis, **kwargs)` to return the result of reducing an\n",
      " |          np.ndarray over an integer valued axis.\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `func`.  By default `func` is\n",
      " |          applied over all dimensions.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      numeric_only : bool, optional\n",
      " |          If True, only apply ``func`` to variables with a numeric dtype.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          Dataset with this object's DataArrays replaced with new DataArrays\n",
      " |          of summarized data and the indicated dimension(s) removed.\n",
      " |  \n",
      " |  reindex(self, indexers=None, method=None, tolerance=None, copy=True, **indexers_kwargs)\n",
      " |      Conform this object onto a new set of indexes, filling in\n",
      " |      missing values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict. optional\n",
      " |          Dictionary with keys given by dimension names and values given by\n",
      " |          arrays of coordinates tick labels. Any mis-matched coordinate values\n",
      " |          will be filled in with NaN, and any mis-matched dimension names will\n",
      " |          simply be ignored.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for filling index values in ``indexers`` not found in\n",
      " |          this dataset:\n",
      " |      \n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value (requires pandas>=0.16)\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Requires pandas>=0.17.\n",
      " |      copy : bool, optional\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      **indexers_kwarg : {dim: indexer, ...}, optional\n",
      " |          Keyword arguments in the same form as ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reindex_like\n",
      " |      align\n",
      " |      pandas.Index.get_indexer\n",
      " |  \n",
      " |  reindex_like(self, other, method=None, tolerance=None, copy=True)\n",
      " |      Conform this object onto the indexes of another object, filling\n",
      " |      in missing values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to pandas.Index objects, which provides coordinates upon\n",
      " |          which to index the variables in this dataset. The indexes on this\n",
      " |          other object need not be the same as the indexes on this\n",
      " |          dataset. Any mis-matched index values will be filled in with\n",
      " |          NaN, and any mis-matched dimension names will simply be ignored.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for filling index values from other not found in this\n",
      " |          dataset:\n",
      " |      \n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value (requires pandas>=0.16)\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Requires pandas>=0.17.\n",
      " |      copy : bool, optional\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : Dataset\n",
      " |          Another dataset, with this dataset's data but coordinates from the\n",
      " |          other object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reindex\n",
      " |      align\n",
      " |  \n",
      " |  rename(self, name_dict=None, inplace=False, **names)\n",
      " |      Returns a new object with renamed variables and dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name_dict : dict-like, optional\n",
      " |          Dictionary whose keys are current variable or dimension names and\n",
      " |          whose values are the desired names.\n",
      " |      inplace : bool, optional\n",
      " |          If True, rename variables and dimensions in-place. Otherwise,\n",
      " |          return a new dataset object.\n",
      " |      **names, optional\n",
      " |          Keyword form of ``name_dict``.\n",
      " |          One of name_dict or names must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          Dataset with renamed variables and dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.swap_dims\n",
      " |      DataArray.rename\n",
      " |  \n",
      " |  reorder_levels(self, dim_order=None, inplace=False, **dim_order_kwargs)\n",
      " |      Rearrange index levels using input order.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order : optional\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by lists representing new level orders. Every given dimension\n",
      " |          must have a multi-index.\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify the dataset in-place. Otherwise, return a new\n",
      " |          DataArray object.\n",
      " |      **dim_order_kwargs: optional\n",
      " |          The keyword arguments form of ``dim_order``.\n",
      " |          One of dim_order or dim_order_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced\n",
      " |          coordinates.\n",
      " |  \n",
      " |  reset_coords(self, names=None, drop=False, inplace=False)\n",
      " |      Given names of coordinates, reset them to become variables\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : str or list of str, optional\n",
      " |          Name(s) of non-index coordinates in this dataset to reset into\n",
      " |          variables. By default, all non-index coordinates are reset.\n",
      " |      drop : bool, optional\n",
      " |          If True, remove coordinates instead of converting them into\n",
      " |          variables.\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify this dataset inplace. Otherwise, create a new\n",
      " |          object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  reset_index(self, dims_or_levels, drop=False, inplace=False)\n",
      " |      Reset the specified index(es) or multi-index level(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_or_levels : str or list\n",
      " |          Name(s) of the dimension(s) and/or multi-index level(s) that will\n",
      " |          be reset.\n",
      " |      drop : bool, optional\n",
      " |          If True, remove the specified indexes and/or multi-index levels\n",
      " |          instead of extracting them as new coordinates (default: False).\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify the dataset in-place. Otherwise, return a new\n",
      " |          Dataset object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.set_index\n",
      " |  \n",
      " |  roll(self, shifts=None, roll_coords=None, **shifts_kwargs)\n",
      " |      Roll this dataset by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Unlike shift, roll may rotate all variables, including coordinates\n",
      " |      if specified. The direction of rotation is consistent with\n",
      " |      :py:func:`numpy.roll`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      shifts : dict, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by integers to rotate each of the given dimensions. Positive\n",
      " |          offsets roll to the right; negative offsets roll to the left.\n",
      " |      roll_coords : bool\n",
      " |          Indicates whether to  roll the coordinates by the offset\n",
      " |          The current default of roll_coords (None, equivalent to True) is\n",
      " |          deprecated and will change to False in a future version.\n",
      " |          Explicitly pass roll_coords to silence the warning.\n",
      " |      **shifts_kwargs : {dim: offset, ...}, optional\n",
      " |          The keyword arguments form of ``shifts``.\n",
      " |          One of shifts or shifts_kwargs must be provided.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      rolled : Dataset\n",
      " |          Dataset with the same coordinates and attributes but rolled\n",
      " |          variables.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      shift\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> ds = xr.Dataset({'foo': ('x', list('abcde'))})\n",
      " |      >>> ds.roll(x=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 3 4 0 1 2\n",
      " |      Data variables:\n",
      " |          foo      (x) object 'd' 'e' 'a' 'b' 'c'\n",
      " |  \n",
      " |  round(self, *args, **kwargs)\n",
      " |  \n",
      " |  sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs)\n",
      " |      Returns a new dataset with each array indexed by tick labels\n",
      " |      along the specified dimension(s).\n",
      " |      \n",
      " |      In contrast to `Dataset.isel`, indexers for this method should use\n",
      " |      labels instead of integers.\n",
      " |      \n",
      " |      Under the hood, this method is powered by using pandas's powerful Index\n",
      " |      objects. This makes label based indexing essentially just as fast as\n",
      " |      using integer indexing.\n",
      " |      \n",
      " |      It also means this method uses pandas's (well documented) logic for\n",
      " |      indexing. This means you can use string shortcuts for datetime indexes\n",
      " |      (e.g., '2000-01' to select all values in January 2000). It also means\n",
      " |      that slices are treated as inclusive of both the start and stop values,\n",
      " |      unlike normal Python indexing.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by scalars, slices or arrays of tick labels. For dimensions with\n",
      " |          multi-index, the indexer may also be a dict-like object with keys\n",
      " |          matching index level names.\n",
      " |          If DataArrays are passed as indexers, xarray-style indexing will be\n",
      " |          carried out. See :ref:`indexing` for the details.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for inexact matches (requires pandas>=0.16):\n",
      " |      \n",
      " |          * None (default): only exact matches\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Requires pandas>=0.17.\n",
      " |      drop : bool, optional\n",
      " |          If ``drop=True``, drop coordinates variables in `indexers` instead\n",
      " |          of making them scalar.\n",
      " |      **indexers_kwarg : {dim: indexer, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          variable and dimension is indexed by the appropriate indexers.\n",
      " |          If indexer DataArrays have coordinates that do not conflict with\n",
      " |          this object, then these coordinates will be attached.\n",
      " |          In general, each array's data will be a view of the array's data\n",
      " |          in this dataset, unless vectorized indexing was triggered by using\n",
      " |          an array indexer, in which case the data will be a copy.\n",
      " |      \n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.isel\n",
      " |      DataArray.sel\n",
      " |  \n",
      " |  sel_points(self, dim='points', method=None, tolerance=None, **indexers)\n",
      " |      Returns a new dataset with each array indexed pointwise by tick\n",
      " |      labels along the specified dimension(s).\n",
      " |      \n",
      " |      In contrast to `Dataset.isel_points`, indexers for this method should\n",
      " |      use labels instead of integers.\n",
      " |      \n",
      " |      In contrast to `Dataset.sel`, this method selects points along the\n",
      " |      diagonal of multi-dimensional arrays, not the intersection.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or DataArray or pandas.Index or other list-like object, optional\n",
      " |          Name of the dimension to concatenate along. If dim is provided as a\n",
      " |          string, it must be a new dimension name, in which case it is added\n",
      " |          along axis=0. If dim is provided as a DataArray or Index or\n",
      " |          list-like object, its name, which must not be present in the\n",
      " |          dataset, is used as the dimension to concatenate along and the\n",
      " |          values are added as a coordinate.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for inexact matches (requires pandas>=0.16):\n",
      " |      \n",
      " |          * None (default): only exact matches\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Requires pandas>=0.17.\n",
      " |      **indexers : {dim: indexer, ...}\n",
      " |          Keyword arguments with names matching dimensions and values given\n",
      " |          by array-like objects. All indexers must be the same length and\n",
      " |          1 dimensional.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          array and dimension is indexed by the appropriate indexers. With\n",
      " |          pointwise indexing, the new Dataset will always be a copy of the\n",
      " |          original.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel\n",
      " |      Dataset.isel\n",
      " |      Dataset.isel_points\n",
      " |      DataArray.sel_points\n",
      " |  \n",
      " |  set_coords(self, names, inplace=False)\n",
      " |      Given names of one or more variables, set them as coordinates\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : str or list of str\n",
      " |          Name(s) of variables in this dataset to convert into coordinates.\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify this dataset inplace. Otherwise, create a new\n",
      " |          object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  set_index(self, indexes=None, append=False, inplace=False, **indexes_kwargs)\n",
      " |      Set Dataset (multi-)indexes using one or more existing coordinates or\n",
      " |      variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexes : {dim: index, ...}\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by (lists of) the names of existing coordinates or variables to set\n",
      " |          as new (multi-)index.\n",
      " |      append : bool, optional\n",
      " |          If True, append the supplied index(es) to the existing index(es).\n",
      " |          Otherwise replace the existing index(es) (default).\n",
      " |      inplace : bool, optional\n",
      " |          If True, set new index(es) in-place. Otherwise, return a new\n",
      " |          Dataset object.\n",
      " |      **indexes_kwargs: optional\n",
      " |          The keyword arguments form of ``indexes``.\n",
      " |          One of indexes or indexes_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reset_index\n",
      " |  \n",
      " |  shift(self, shifts=None, **shifts_kwargs)\n",
      " |      Shift this dataset by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Only data variables are moved; coordinates stay in place. This is\n",
      " |      consistent with the behavior of ``shift`` in pandas.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shifts : Mapping with the form of {dim: offset}\n",
      " |          Integer offset to shift along each of the given dimensions.\n",
      " |          Positive offsets shift to the right; negative offsets shift to the\n",
      " |          left.\n",
      " |      **shifts_kwargs:\n",
      " |          The keyword arguments form of ``shifts``.\n",
      " |          One of shifts or shifts_kwarg must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      shifted : Dataset\n",
      " |          Dataset with the same coordinates and attributes but shifted data\n",
      " |          variables.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      roll\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> ds = xr.Dataset({'foo': ('x', list('abcde'))})\n",
      " |      >>> ds.shift(x=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3 4\n",
      " |      Data variables:\n",
      " |          foo      (x) object nan nan 'a' 'b' 'c'\n",
      " |  \n",
      " |  sortby(self, variables, ascending=True)\n",
      " |      Sort object by labels or values (along an axis).\n",
      " |      \n",
      " |      Sorts the dataset, either along specified dimensions,\n",
      " |      or according to values of 1-D dataarrays that share dimension\n",
      " |      with calling object.\n",
      " |      \n",
      " |      If the input variables are dataarrays, then the dataarrays are aligned\n",
      " |      (via left-join) to the calling object prior to sorting by cell values.\n",
      " |      NaNs are sorted to the end, following Numpy convention.\n",
      " |      \n",
      " |      If multiple sorts along the same dimension is\n",
      " |      given, numpy's lexsort is performed along that dimension:\n",
      " |      https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n",
      " |      and the FIRST key in the sequence is used as the primary sort key,\n",
      " |      followed by the 2nd key, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      variables: str, DataArray, or list of either\n",
      " |          1D DataArray objects or name(s) of 1D variable(s) in\n",
      " |          coords/data_vars whose values are used to sort the dataset.\n",
      " |      ascending: boolean, optional\n",
      " |          Whether to sort by ascending or descending order.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sorted: Dataset\n",
      " |          A new dataset where all the specified dims are sorted by dim\n",
      " |          labels.\n",
      " |  \n",
      " |  stack(self, dimensions=None, **dimensions_kwargs)\n",
      " |      Stack any number of existing dimensions into a single new dimension.\n",
      " |      \n",
      " |      New dimensions will be added at the end, and the corresponding\n",
      " |      coordinate variables will be combined into a MultiIndex.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n",
      " |          Names of new dimensions, and the existing dimensions that they\n",
      " |          replace.\n",
      " |      **dimensions_kwargs:\n",
      " |          The keyword arguments form of ``dimensions``.\n",
      " |          One of dimensions or dimensions_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stacked : Dataset\n",
      " |          Dataset with stacked data.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.unstack\n",
      " |  \n",
      " |  std(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `std` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `std`.  By default `std` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `std` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `std` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  sum(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `sum` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `sum`.  By default `sum` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      min_count : int, default None\n",
      " |          The required number of valid values to perform the operation.\n",
      " |          If fewer than min_count non-NA values are present the result will\n",
      " |          be NA. New in version 0.10.8: Added with the default being None.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `sum` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `sum` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  swap_dims(self, dims_dict, inplace=False)\n",
      " |      Returns a new object with swapped dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_dict : dict-like\n",
      " |          Dictionary whose keys are current dimension names and whose values\n",
      " |          are new names. Each value must already be a variable in the\n",
      " |          dataset.\n",
      " |      inplace : bool, optional\n",
      " |          If True, swap dimensions in-place. Otherwise, return a new dataset\n",
      " |          object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          Dataset with swapped dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      \n",
      " |      Dataset.rename\n",
      " |      DataArray.swap_dims\n",
      " |  \n",
      " |  to_array(self, dim='variable', name=None)\n",
      " |      Convert this dataset into an xarray.DataArray\n",
      " |      \n",
      " |      The data variables of this dataset will be broadcast against each other\n",
      " |      and stacked along the first axis of the new array. All coordinates of\n",
      " |      this dataset will remain coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          Name of the new dimension.\n",
      " |      name : str, optional\n",
      " |          Name of the new data array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array : xarray.DataArray\n",
      " |  \n",
      " |  to_dask_dataframe(self, dim_order=None, set_index=False)\n",
      " |      Convert this dataset into a dask.dataframe.DataFrame.\n",
      " |      \n",
      " |      The dimensions, coordinates and data variables in this dataset form\n",
      " |      the columns of the DataFrame.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order : list, optional\n",
      " |          Hierarchical dimension order for the resulting dataframe. All\n",
      " |          arrays are transposed to this order and then written out as flat\n",
      " |          vectors in contiguous order, so the last dimension in this list\n",
      " |          will be contiguous in the resulting DataFrame. This has a major\n",
      " |          influence on which operations are efficient on the resulting dask\n",
      " |          dataframe.\n",
      " |      \n",
      " |          If provided, must include all dimensions on this dataset. By\n",
      " |          default, dimensions are sorted alphabetically.\n",
      " |      set_index : bool, optional\n",
      " |          If set_index=True, the dask DataFrame is indexed by this dataset's\n",
      " |          coordinate. Since dask DataFrames to not support multi-indexes,\n",
      " |          set_index only works if the dataset only contains one dimension.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dask.dataframe.DataFrame\n",
      " |  \n",
      " |  to_dataframe(self)\n",
      " |      Convert this dataset into a pandas.DataFrame.\n",
      " |      \n",
      " |      Non-index variables in this dataset form the columns of the\n",
      " |      DataFrame. The DataFrame is be indexed by the Cartesian product of\n",
      " |      this dataset's indices.\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |      Convert this dataset to a dictionary following xarray naming\n",
      " |      conventions.\n",
      " |      \n",
      " |      Converts all variables and attributes to native Python objects\n",
      " |      Useful for coverting to json. To avoid datetime incompatibility\n",
      " |      use decode_times=False kwarg in xarrray.open_dataset.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.from_dict\n",
      " |  \n",
      " |  to_netcdf(self, path=None, mode='w', format=None, group=None, engine=None, encoding=None, unlimited_dims=None, compute=True)\n",
      " |      Write dataset contents to a netCDF file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, Path or file-like object, optional\n",
      " |          Path to which to save this dataset. File-like objects are only\n",
      " |          supported by the scipy engine. If no path is provided, this\n",
      " |          function returns the resulting netCDF file as bytes; in this case,\n",
      " |          we need to use scipy, which does not support netCDF version 4 (the\n",
      " |          default format becomes NETCDF3_64BIT).\n",
      " |      mode : {'w', 'a'}, optional\n",
      " |          Write ('w') or append ('a') mode. If mode='w', any existing file at\n",
      " |          this location will be overwritten. If mode='a', existing variables\n",
      " |          will be overwritten.\n",
      " |      format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT','NETCDF3_CLASSIC'}, optional\n",
      " |          File format for the resulting netCDF file:\n",
      " |      \n",
      " |          * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n",
      " |            features.\n",
      " |          * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n",
      " |            netCDF 3 compatible API features.\n",
      " |          * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n",
      " |            which fully supports 2+ GB files, but is only compatible with\n",
      " |            clients linked against netCDF version 3.6.0 or later.\n",
      " |          * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n",
      " |            handle 2+ GB files very well.\n",
      " |      \n",
      " |          All formats are supported by the netCDF4-python library.\n",
      " |          scipy.io.netcdf only supports the last two formats.\n",
      " |      \n",
      " |          The default format is NETCDF4 if you are saving a file to disk and\n",
      " |          have the netCDF4-python library available. Otherwise, xarray falls\n",
      " |          back to using scipy to write netCDF files and defaults to the\n",
      " |          NETCDF3_64BIT format (scipy does not support netCDF4).\n",
      " |      group : str, optional\n",
      " |          Path to the netCDF4 group in the given file to open (only works for\n",
      " |          format='NETCDF4'). The group(s) will be created if necessary.\n",
      " |      engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n",
      " |          Engine to use when writing netCDF files. If not provided, the\n",
      " |          default engine is chosen based on available dependencies, with a\n",
      " |          preference for 'netcdf4' if writing to a file on disk.\n",
      " |      encoding : dict, optional\n",
      " |          Nested dictionary with variable names as keys and dictionaries of\n",
      " |          variable specific encodings as values, e.g.,\n",
      " |          ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,\n",
      " |                             'zlib': True}, ...}``\n",
      " |      \n",
      " |          The `h5netcdf` engine supports both the NetCDF4-style compression\n",
      " |          encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py\n",
      " |          ones ``{'compression': 'gzip', 'compression_opts': 9}``.\n",
      " |          This allows using any compression plugin installed in the HDF5\n",
      " |          library, e.g. LZF.\n",
      " |      \n",
      " |      unlimited_dims : sequence of str, optional\n",
      " |          Dimension(s) that should be serialized as unlimited dimensions.\n",
      " |          By default, no dimensions are treated as unlimited dimensions.\n",
      " |          Note that unlimited_dims may also be set via\n",
      " |          ``dataset.encoding['unlimited_dims']``.\n",
      " |      compute: boolean\n",
      " |          If true compute immediately, otherwise return a\n",
      " |          ``dask.delayed.Delayed`` object that can be computed later.\n",
      " |  \n",
      " |  to_zarr(self, store=None, mode='w-', synchronizer=None, group=None, encoding=None, compute=True)\n",
      " |      Write dataset contents to a zarr group.\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |                The Zarr backend is new and experimental. Please report any\n",
      " |                unexpected behavior via github issues.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      store : MutableMapping or str, optional\n",
      " |          Store or path to directory in file system.\n",
      " |      mode : {'w', 'w-'}\n",
      " |          Persistence mode: 'w' means create (overwrite if exists);\n",
      " |          'w-' means create (fail if exists).\n",
      " |      synchronizer : object, optional\n",
      " |          Array synchronizer\n",
      " |      group : str, obtional\n",
      " |          Group path. (a.k.a. `path` in zarr terminology.)\n",
      " |      encoding : dict, optional\n",
      " |          Nested dictionary with variable names as keys and dictionaries of\n",
      " |          variable specific encodings as values, e.g.,\n",
      " |          ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``\n",
      " |      compute: boolean\n",
      " |          If true compute immediately, otherwise return a\n",
      " |          ``dask.delayed.Delayed`` object that can be computed later.\n",
      " |  \n",
      " |  transpose(self, *dims)\n",
      " |      Return a new Dataset object with all array dimensions transposed.\n",
      " |      \n",
      " |      Although the order of dimensions on each array will change, the dataset\n",
      " |      dimensions themselves will remain in fixed (sorted) order.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *dims : str, optional\n",
      " |          By default, reverse the dimensions on each array. Otherwise,\n",
      " |          reorder the dimensions to this order.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      transposed : Dataset\n",
      " |          Each array in the dataset (including) coordinates will be\n",
      " |          transposed to the given order.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Although this operation returns a view of each array's data, it\n",
      " |      is not lazy -- the data will be fully loaded into memory.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.transpose\n",
      " |      DataArray.transpose\n",
      " |  \n",
      " |  unstack(self, dim=None)\n",
      " |      Unstack existing dimensions corresponding to MultiIndexes into\n",
      " |      multiple new dimensions.\n",
      " |      \n",
      " |      New dimensions will be added at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to unstack. By default unstacks all\n",
      " |          MultiIndexes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      unstacked : Dataset\n",
      " |          Dataset with unstacked data.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.stack\n",
      " |  \n",
      " |  update(self, other, inplace=True)\n",
      " |      Update this dataset's variables with those from another dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or castable to Dataset\n",
      " |          Dataset or variables with which to update this dataset.\n",
      " |      inplace : bool, optional\n",
      " |          If True, merge the other dataset into this dataset in-place.\n",
      " |          Otherwise, return a new dataset object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      updated : Dataset\n",
      " |          Updated dataset.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If any dimensions would have inconsistent sizes in the updated\n",
      " |          dataset.\n",
      " |  \n",
      " |  var(self, dim=None, keep_attrs=False, skipna=None, **kwargs)\n",
      " |      Reduce this Dataset's data by applying `var` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |                  Dimension(s) over which to apply `var`.  By default `var` is\n",
      " |                  applied over all dimensions.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `var` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset object with `var` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_dataframe(dataframe) from abc.ABCMeta\n",
      " |      Convert a pandas.DataFrame into an xarray.Dataset\n",
      " |      \n",
      " |      Each column will be converted into an independent variable in the\n",
      " |      Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n",
      " |      into a tensor product of one-dimensional indices (filling in missing\n",
      " |      values with NaN). This method will produce a Dataset very similar to\n",
      " |      that on which the 'to_dataframe' method was called, except with\n",
      " |      possibly redundant dimensions (since all dataset variables will have\n",
      " |      the same dimensionality).\n",
      " |  \n",
      " |  from_dict(d) from abc.ABCMeta\n",
      " |      Convert a dictionary into an xarray.Dataset.\n",
      " |      \n",
      " |      Input dict can take several forms::\n",
      " |      \n",
      " |          d = {'t': {'dims': ('t'), 'data': t},\n",
      " |               'a': {'dims': ('t'), 'data': x},\n",
      " |               'b': {'dims': ('t'), 'data': y}}\n",
      " |      \n",
      " |          d = {'coords': {'t': {'dims': 't', 'data': t,\n",
      " |                                'attrs': {'units':'s'}}},\n",
      " |               'attrs': {'title': 'air temperature'},\n",
      " |               'dims': 't',\n",
      " |               'data_vars': {'a': {'dims': 't', 'data': x, },\n",
      " |                             'b': {'dims': 't', 'data': y}}}\n",
      " |      \n",
      " |      where 't' is the name of the dimesion, 'a' and 'b' are names of data\n",
      " |      variables and t, x, and y are lists, numpy.arrays or pandas objects.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      d : dict, with a minimum structure of {'var_0': {'dims': [..],                                                          'data': [..]},                                                ...}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : xarray.Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.to_dict\n",
      " |      DataArray.from_dict\n",
      " |  \n",
      " |  load_store(store, decoder=None) from abc.ABCMeta\n",
      " |      Create a new dataset from the contents of a backends.*DataStore\n",
      " |      object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  T\n",
      " |  \n",
      " |  __dask_optimize__\n",
      " |  \n",
      " |  __dask_scheduler__\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  attrs\n",
      " |      Dictionary of global attributes on this dataset\n",
      " |  \n",
      " |  chunks\n",
      " |      Block dimensions for this dataset's data or None if it's not a dask\n",
      " |      array.\n",
      " |  \n",
      " |  coords\n",
      " |      Dictionary of xarray.DataArray objects corresponding to coordinate\n",
      " |      variables\n",
      " |  \n",
      " |  data_vars\n",
      " |      Dictionary of xarray.DataArray objects corresponding to data variables\n",
      " |  \n",
      " |  dims\n",
      " |      Mapping from dimension names to lengths.\n",
      " |      \n",
      " |      Cannot be modified directly, but is updated when adding new variables.\n",
      " |      \n",
      " |      Note that type of this object differs from `DataArray.dims`.\n",
      " |      See `Dataset.sizes` and `DataArray.sizes` for consistently named\n",
      " |      properties.\n",
      " |  \n",
      " |  encoding\n",
      " |      Dictionary of global encoding attributes on this dataset\n",
      " |  \n",
      " |  imag\n",
      " |  \n",
      " |  indexes\n",
      " |      OrderedDict of pandas.Index objects used for label based indexing\n",
      " |  \n",
      " |  loc\n",
      " |      Attribute for location based indexing. Only supports __getitem__,\n",
      " |      and only when the key is a dict of the form {dim: labels}.\n",
      " |  \n",
      " |  nbytes\n",
      " |  \n",
      " |  real\n",
      " |  \n",
      " |  sizes\n",
      " |      Mapping from dimension names to lengths.\n",
      " |      \n",
      " |      Cannot be modified directly, but is updated when adding new variables.\n",
      " |      \n",
      " |      This is an alias for `Dataset.dims` provided for the benefit of\n",
      " |      consistency with `DataArray.sizes`.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.sizes\n",
      " |  \n",
      " |  variables\n",
      " |      Low level interface to Dataset contents as dict of Variable objects.\n",
      " |      \n",
      " |      This ordered dictionary is frozen to prevent mutation that could\n",
      " |      violate Dataset invariants. It contains all variable objects\n",
      " |      constituting the Dataset, including both data variables and\n",
      " |      coordinates.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __array_priority__ = 50\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(self)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(self)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  values(self)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __reversed__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.DataWithCoords:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  assign_attrs(self, *args, **kwargs)\n",
      " |      Assign new attrs to this object.\n",
      " |      \n",
      " |      Returns a new object equivalent to self.attrs.update(*args, **kwargs).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : positional arguments passed into ``attrs.update``.\n",
      " |      kwargs : keyword arguments passed into ``attrs.update``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new attrs in addition to the existing data.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |  \n",
      " |  assign_coords(self, **kwargs)\n",
      " |      Assign new coordinates to this object.\n",
      " |      \n",
      " |      Returns a new object with all the original data in addition to the new\n",
      " |      coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kwargs : keyword, value pairs\n",
      " |          keywords are the variables names. If the values are callable, they\n",
      " |          are computed on this object and assigned to new coordinate\n",
      " |          variables. If the values are not callable, (e.g. a DataArray,\n",
      " |          scalar, or array), they are simply assigned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new coordinates in addition to the existing\n",
      " |          data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Convert longitude coordinates from 0-359 to -180-179:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.random.rand(4),\n",
      " |      ...                   coords=[np.array([358, 359, 0, 1])],\n",
      " |      ...                   dims='lon')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.28298 , 0.667347, 0.657938, 0.177683])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 358 359 0 1\n",
      " |      >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.28298 , 0.667347, 0.657938, 0.177683])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 -2 -1 0 1\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since ``kwargs`` is a dictionary, the order of your arguments may not\n",
      " |      be preserved, and so the order of the new variables is not well\n",
      " |      defined. Assigning multiple variables within the same ``assign_coords``\n",
      " |      is possible, but you cannot reference other variables created within\n",
      " |      the same ``assign_coords`` call.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close any files linked to this object\n",
      " |  \n",
      " |  get_index(self, key)\n",
      " |      Get an index for a dimension, with fall-back to a default RangeIndex\n",
      " |  \n",
      " |  groupby(self, group, squeeze=True)\n",
      " |      Returns a GroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : str, DataArray or IndexVariable\n",
      " |          Array whose unique values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      squeeze : boolean, optional\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : GroupBy\n",
      " |          A `GroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Calculate daily anomalies for daily data:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n",
      " |      ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n",
      " |      ...                           freq='D')],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 1827)>\n",
      " |      array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |      >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n",
      " |      <xarray.DataArray (time: 1827)>\n",
      " |      array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n",
      " |      Coordinates:\n",
      " |        * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |          dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.groupby.DataArrayGroupBy\n",
      " |      core.groupby.DatasetGroupBy\n",
      " |  \n",
      " |  groupby_bins(self, group, bins, right=True, labels=None, precision=3, include_lowest=False, squeeze=True)\n",
      " |      Returns a GroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Rather than using all unique values of `group`, the values are discretized\n",
      " |      first by applying `pandas.cut` [1]_ to `group`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : str, DataArray or IndexVariable\n",
      " |          Array whose binned values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      bins : int or array of scalars\n",
      " |          If bins is an int, it defines the number of equal-width bins in the\n",
      " |          range of x. However, in this case, the range of x is extended by .1%\n",
      " |          on each side to include the min or max values of x. If bins is a\n",
      " |          sequence it defines the bin edges allowing for non-uniform bin\n",
      " |          width. No extension of the range of x is done in this case.\n",
      " |      right : boolean, optional\n",
      " |          Indicates whether the bins include the rightmost edge or not. If\n",
      " |          right == True (the default), then the bins [1,2,3,4] indicate\n",
      " |          (1,2], (2,3], (3,4].\n",
      " |      labels : array or boolean, default None\n",
      " |          Used as labels for the resulting bins. Must be of the same length as\n",
      " |          the resulting bins. If False, string bin labels are assigned by\n",
      " |          `pandas.cut`.\n",
      " |      precision : int\n",
      " |          The precision at which to store and display the bins labels.\n",
      " |      include_lowest : bool\n",
      " |          Whether the first interval should be left-inclusive or not.\n",
      " |      squeeze : boolean, optional\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : GroupBy\n",
      " |          A `GroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |          The name of the group has the added suffix `_bins` in order to\n",
      " |          distinguish it from the original variable.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
      " |  \n",
      " |  isin(self, test_elements)\n",
      " |      Tests each value in the array for whether it is in the supplied list.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      test_elements : array_like\n",
      " |          The values against which to test each value of `element`.\n",
      " |          This argument is flattened if an array or array_like.\n",
      " |          See numpy notes for behavior with non-array-like parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      isin : same as object, bool\n",
      " |          Has the same shape as this object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> array = xr.DataArray([1, 2, 3], dims='x')\n",
      " |      >>> array.isin([1, 3])\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ True, False,  True])\n",
      " |      Dimensions without coordinates: x\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.isin\n",
      " |  \n",
      " |  pipe(self, func, *args, **kwargs)\n",
      " |      Apply func(self, *args, **kwargs)\n",
      " |      \n",
      " |      This method replicates the pandas method of the same name.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          function to apply to this xarray object (Dataset/DataArray).\n",
      " |          ``args``, and ``kwargs`` are passed into ``func``.\n",
      " |          Alternatively a ``(callable, data_keyword)`` tuple where\n",
      " |          ``data_keyword`` is a string indicating the keyword of\n",
      " |          ``callable`` that expects the xarray object.\n",
      " |      args : positional arguments passed into ``func``.\n",
      " |      kwargs : a dictionary of keyword arguments passed into ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : the return type of ``func``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      Use ``.pipe`` when chaining together functions that expect\n",
      " |      xarray or pandas objects, e.g., instead of writing\n",
      " |      \n",
      " |      >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n",
      " |      \n",
      " |      You can write\n",
      " |      \n",
      " |      >>> (ds.pipe(h)\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe(f, arg2=b, arg3=c)\n",
      " |      ... )\n",
      " |      \n",
      " |      If you have a function that takes the data as (say) the second\n",
      " |      argument, pass a tuple indicating which keyword expects the\n",
      " |      data. For example, suppose ``f`` takes its data as ``arg2``:\n",
      " |      \n",
      " |      >>> (ds.pipe(h)\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n",
      " |      ...  )\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.pipe\n",
      " |  \n",
      " |  resample(self, freq=None, dim=None, how=None, skipna=None, closed=None, label=None, base=0, keep_attrs=False, **indexer)\n",
      " |      Returns a Resample object for performing resampling operations.\n",
      " |      \n",
      " |      Handles both downsampling and upsampling. If any intervals contain no\n",
      " |      values from the original object, they will be given the value ``NaN``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      skipna : bool, optional\n",
      " |          Whether to skip missing values when aggregating in downsampling.\n",
      " |      closed : 'left' or 'right', optional\n",
      " |          Side of each interval to treat as closed.\n",
      " |      label : 'left or 'right', optional\n",
      " |          Side of each interval to use for labeling.\n",
      " |      base : int, optional\n",
      " |          For frequencies that evenly subdivide 1 day, the \"origin\" of the\n",
      " |          aggregated intervals. For example, for '24H' frequency, base could\n",
      " |          range from 0 through 23.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the object's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      **indexer : {dim: freq}\n",
      " |          Dictionary with a key indicating the dimension name to resample\n",
      " |          over and a value corresponding to the resampling frequency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      resampled : same type as caller\n",
      " |          This object resampled.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Downsample monthly time-series data to seasonal data:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n",
      " |      ...                   coords=[pd.date_range('15/12/1999',\n",
      " |      ...                           periods=12, freq=pd.DateOffset(months=1))],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      >>> da.resample(time=\"QS-DEC\").mean()\n",
      " |      <xarray.DataArray (time: 4)>\n",
      " |      array([ 1.,  4.,  7., 10.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n",
      " |      \n",
      " |      Upsample monthly time-series data to daily data:\n",
      " |      \n",
      " |      >>> da.resample(time='1D').interpolate('linear')\n",
      " |      <xarray.DataArray (time: 337)>\n",
      " |      array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      \n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
      " |  \n",
      " |  rolling(self, dim=None, min_periods=None, center=False, **dim_kwargs)\n",
      " |      Rolling window object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim: dict, optional\n",
      " |          Mapping from the dimension name to create the rolling iterator\n",
      " |          along (e.g. `time`) to its moving window size.\n",
      " |      min_periods : int, default None\n",
      " |          Minimum number of observations in window required to have a value\n",
      " |          (otherwise result is NA). The default, None, is equivalent to\n",
      " |          setting min_periods equal to the size of the window.\n",
      " |      center : boolean, default False\n",
      " |          Set the labels at the center of the window.\n",
      " |      **dim_kwargs : optional\n",
      " |          The keyword arguments form of ``dim``.\n",
      " |          One of dim or dim_kwarg must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Rolling object (core.rolling.DataArrayRolling for DataArray,\n",
      " |      core.rolling.DatasetRolling for Dataset.)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n",
      " |      ...                   coords=[pd.date_range('15/12/1999',\n",
      " |      ...                           periods=12, freq=pd.DateOffset(months=1))],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      >>> da.rolling(time=3, center=True).mean()\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      \n",
      " |      Remove the NaNs using ``dropna()``:\n",
      " |      \n",
      " |      >>> da.rolling(time=3, center=True).mean().dropna('time')\n",
      " |      <xarray.DataArray (time: 10)>\n",
      " |      array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.rolling.DataArrayRolling\n",
      " |      core.rolling.DatasetRolling\n",
      " |  \n",
      " |  squeeze(self, dim=None, drop=False, axis=None)\n",
      " |      Return a new object with squeezed data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : None or str or tuple of str, optional\n",
      " |          Selects a subset of the length one dimensions. If a dimension is\n",
      " |          selected with length greater than one, an error is raised. If\n",
      " |          None, all length one dimensions are squeezed.\n",
      " |      drop : bool, optional\n",
      " |          If ``drop=True``, drop squeezed coordinates instead of making them\n",
      " |          scalar.\n",
      " |      axis : int, optional\n",
      " |          Select the dimension to squeeze. Added for compatibility reasons.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      squeezed : same type as caller\n",
      " |          This object, but with with all or a subset of the dimensions of\n",
      " |          length 1 removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.squeeze\n",
      " |  \n",
      " |  where(self, cond, other=<NA>, drop=False)\n",
      " |      Filter elements from this object according to a condition.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : DataArray or Dataset with boolean dtype\n",
      " |          Locations at which to preserve this object's values.\n",
      " |      other : scalar, DataArray or Dataset, optional\n",
      " |          Value to use for locations in this object where ``cond`` is False.\n",
      " |          By default, these locations filled with NA.\n",
      " |      drop : boolean, optional\n",
      " |          If True, coordinate labels that only correspond to False values of\n",
      " |          the condition are dropped from the result. Mutually exclusive with\n",
      " |          ``other``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=('x', 'y'))\n",
      " |      >>> a.where(a.x + a.y < 4)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[  0.,   1.,   2.,   3.,  nan],\n",
      " |             [  5.,   6.,   7.,  nan,  nan],\n",
      " |             [ 10.,  11.,  nan,  nan,  nan],\n",
      " |             [ 15.,  nan,  nan,  nan,  nan],\n",
      " |             [ nan,  nan,  nan,  nan,  nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      >>> a.where(a.x + a.y < 5, -1)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[ 0,  1,  2,  3,  4],\n",
      " |             [ 5,  6,  7,  8, -1],\n",
      " |             [10, 11, 12, -1, -1],\n",
      " |             [15, 16, -1, -1, -1],\n",
      " |             [20, -1, -1, -1, -1]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      >>> a.where(a.x + a.y < 4, drop=True)\n",
      " |      <xarray.DataArray (x: 4, y: 4)>\n",
      " |      array([[  0.,   1.,   2.,   3.],\n",
      " |             [  5.,   6.,   7.,  nan],\n",
      " |             [ 10.,  11.,  nan,  nan],\n",
      " |             [ 15.,  nan,  nan,  nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.where : corresponding numpy function\n",
      " |      where : equivalent function\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.arithmetic.SupportsArithmetic:\n",
      " |  \n",
      " |  __array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n",
      " |  \n",
      " |  __div__ = not_implemented(*args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.AttrAccessMixin:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.formatting.ReprMixin:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xr.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxd9SFekNDtb",
    "outputId": "993fa81e-3568-4bcc-a263-0ceb09f92ee1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-8830c1d5069d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MOD06'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0m_assert_compat_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mcoerce_pandas_values\u001b[0;34m(objects)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "foo = xr.Dataset('MOD06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfSTC2AQNDte"
   },
   "outputs": [],
   "source": [
    "da = xr.DataArray('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2vTl-GYNDtf",
    "outputId": "ae585278-9688-4129-bbba-c12b5b017d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray ()>\n",
       "array('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf',\n",
       "      dtype='<U86')"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rm9bAaQNDtj",
    "outputId": "58f24d49-e782-4635-8413-81ef24ffcc4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ImplementsArrayReduce._reduce_method.<locals>.wrapped_func of <xarray.DataArray ()>\n",
       "array('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf',\n",
       "      dtype='<U86')>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WT6cscfhNDtk",
    "outputId": "e256644b-88c1-4bca-e4d9-97c1a8a3ff21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf',\n",
       "      dtype='<U86')"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRf-AFlaNDtm",
    "outputId": "f4cd1388-b53a-4a87-f157-994ba25ec8e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U86')"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4gu8GNrNDtp",
    "outputId": "c54e74df-aac3-4ee2-e58a-1f4385594798"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000344"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.nbytes / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgVpHPx6NDtr",
    "outputId": "edd001ee-be52-4f73-bfb3-78f0fe3dc9dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Pc7oYSNDtt",
    "outputId": "d84a3915-48ae-4c12-e9bf-beb2ecc3cf7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf',\n",
       "      dtype='<U86')"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3JsgoWSNDtu",
    "outputId": "4ebce5f4-f560-4ca2-bf7f-78ffc71dcec6"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-8f1b7af88084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs, compat)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_init_vars_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_set_init_vars_and_dims\u001b[0;34m(self, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         variables, coord_names, dims = merge_data_and_coords(\n\u001b[0;32m--> 383\u001b[0;31m             data_vars, coords, compat=compat)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     return merge_core(objs, compat, join, explicit_coords=explicit_coords,\n\u001b[0;32m--> 370\u001b[0;31m                       indexes=indexes)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objs, compat, join, priority_arg, explicit_coords, indexes)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0m_assert_compat_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mcoerced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoerce_pandas_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0maligned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mexpanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_variable_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mcoerce_pandas_values\u001b[0;34m(objects)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPANDAS_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "ds = xr.Dataset('/Users/saviosebastian/Desktop/XArraysTest/MYD06_L2.A2002185.0000.061.2018003215042.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FMooxGVNDtx"
   },
   "outputs": [],
   "source": [
    "locs = ['IA', 'IL', 'IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnxqtqyANDty"
   },
   "outputs": [],
   "source": [
    "times = pd.date_range('2000-01-01', periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23EDi-XgNDt0",
    "outputId": "7fdd9988-20fd-48bf-8f38-0f1e9f3d06a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF4):\n",
      "    HDFEOSVersion: HDFEOS_V2.19\n",
      "    StructMetadata.0: GROUP=SwathStructure\n",
      "\tGROUP=SWATH_1\n",
      "\t\tSwathName=\"mod06\"\n",
      "\t\tGROUP=Dimension\n",
      "\t\t\tOBJECT=Dimension_1\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_5km\"\n",
      "\t\t\t\tSize=270\n",
      "\t\t\tEND_OBJECT=Dimension_1\n",
      "\t\t\tOBJECT=Dimension_2\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_5km\"\n",
      "\t\t\t\tSize=408\n",
      "\t\t\tEND_OBJECT=Dimension_2\n",
      "\t\t\tOBJECT=Dimension_3\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_1km\"\n",
      "\t\t\t\tSize=1354\n",
      "\t\t\tEND_OBJECT=Dimension_3\n",
      "\t\t\tOBJECT=Dimension_4\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_1km\"\n",
      "\t\t\t\tSize=2040\n",
      "\t\t\tEND_OBJECT=Dimension_4\n",
      "\t\t\tOBJECT=Dimension_5\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_hkm\"\n",
      "\t\t\t\tSize=2708\n",
      "\t\t\tEND_OBJECT=Dimension_5\n",
      "\t\t\tOBJECT=Dimension_6\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_hkm\"\n",
      "\t\t\t\tSize=4060\n",
      "\t\t\tEND_OBJECT=Dimension_6\n",
      "\t\t\tOBJECT=Dimension_7\n",
      "\t\t\t\tDimensionName=\"Band_Number\"\n",
      "\t\t\t\tSize=7\n",
      "\t\t\tEND_OBJECT=Dimension_7\n",
      "\t\t\tOBJECT=Dimension_8\n",
      "\t\t\t\tDimensionName=\"Band_Ratio\"\n",
      "\t\t\t\tSize=5\n",
      "\t\t\tEND_OBJECT=Dimension_8\n",
      "\t\t\tOBJECT=Dimension_9\n",
      "\t\t\t\tDimensionName=\"Band_Forcing\"\n",
      "\t\t\t\tSize=5\n",
      "\t\t\tEND_OBJECT=Dimension_9\n",
      "\t\t\tOBJECT=Dimension_10\n",
      "\t\t\t\tDimensionName=\"Band_Difference\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_10\n",
      "\t\t\tOBJECT=Dimension_11\n",
      "\t\t\t\tDimensionName=\"QA_Parameter_5km\"\n",
      "\t\t\t\tSize=10\n",
      "\t\t\tEND_OBJECT=Dimension_11\n",
      "\t\t\tOBJECT=Dimension_12\n",
      "\t\t\t\tDimensionName=\"QA_Parameter_1km\"\n",
      "\t\t\t\tSize=9\n",
      "\t\t\tEND_OBJECT=Dimension_12\n",
      "\t\t\tOBJECT=Dimension_13\n",
      "\t\t\t\tDimensionName=\"Cloud_Mask_1km_Num_Bytes\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_13\n",
      "\t\t\tOBJECT=Dimension_14\n",
      "\t\t\t\tDimensionName=\"Cloud_Mask_5km_Num_Bytes\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_14\n",
      "\t\t\tOBJECT=Dimension_15\n",
      "\t\t\t\tDimensionName=\"RadTran_NWL\"\n",
      "\t\t\t\tSize=7\n",
      "\t\t\tEND_OBJECT=Dimension_15\n",
      "\t\t\tOBJECT=Dimension_16\n",
      "\t\t\t\tDimensionName=\"RadTran_NRE_Ice\"\n",
      "\t\t\t\tSize=12\n",
      "\t\t\tEND_OBJECT=Dimension_16\n",
      "\t\t\tOBJECT=Dimension_17\n",
      "\t\t\t\tDimensionName=\"RadTran_NRE_Liq\"\n",
      "\t\t\t\tSize=18\n",
      "\t\t\tEND_OBJECT=Dimension_17\n",
      "\t\t\tOBJECT=Dimension_18\n",
      "\t\t\t\tDimensionName=\"SPI_nband\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_18\n",
      "\t\t\tOBJECT=Dimension_19\n",
      "\t\t\t\tDimensionName=\"RFM_nband\"\n",
      "\t\t\t\tSize=3\n",
      "\t\t\tEND_OBJECT=Dimension_19\n",
      "\t\t\tOBJECT=Dimension_20\n",
      "\t\t\t\tDimensionName=\"ACR_nband\"\n",
      "\t\t\t\tSize=6\n",
      "\t\t\tEND_OBJECT=Dimension_20\n",
      "\t\t\tOBJECT=Dimension_21\n",
      "\t\t\t\tDimensionName=\"Statistic_Parameter_1km\"\n",
      "\t\t\t\tSize=17\n",
      "\t\t\tEND_OBJECT=Dimension_21\n",
      "\t\tEND_GROUP=Dimension\n",
      "\t\tGROUP=DimensionMap\n",
      "\t\t\tOBJECT=DimensionMap_1\n",
      "\t\t\t\tGeoDimension=\"Cell_Across_Swath_5km\"\n",
      "\t\t\t\tDataDimension=\"Cell_Across_Swath_1km\"\n",
      "\t\t\t\tOffset=2\n",
      "\t\t\t\tIncrement=5\n",
      "\t\t\tEND_OBJECT=DimensionMap_1\n",
      "\t\t\tOBJECT=DimensionMap_2\n",
      "\t\t\t\tGeoDimension=\"Cell_Along_Swath_5km\"\n",
      "\t\t\t\tDataDimension=\"Cell_Along_Swath_1km\"\n",
      "\t\t\t\tOffset=2\n",
      "\t\t\t\tIncrement=5\n",
      "\t\t\tEND_OBJECT=DimensionMap_2\n",
      "\t\tEND_GROUP=DimensionMap\n",
      "\t\tGROUP=IndexDimensionMap\n",
      "\t\tEND_GROUP=IndexDimensionMap\n",
      "\t\tGROUP=GeoField\n",
      "\t\t\tOBJECT=GeoField_1\n",
      "\t\t\t\tGeoFieldName=\"Latitude\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=GeoField_1\n",
      "\t\t\tOBJECT=GeoField_2\n",
      "\t\t\t\tGeoFieldName=\"Longitude\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=GeoField_2\n",
      "\t\tEND_GROUP=GeoField\n",
      "\t\tGROUP=DataField\n",
      "\t\t\tOBJECT=DataField_1\n",
      "\t\t\t\tDataFieldName=\"Band_Number\"\n",
      "\t\t\t\tDataType=DFNT_INT32\n",
      "\t\t\t\tDimList=(\"Band_Number\")\n",
      "\t\t\tEND_OBJECT=DataField_1\n",
      "\t\t\tOBJECT=DataField_2\n",
      "\t\t\t\tDataFieldName=\"Statistics_1km\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Statistic_Parameter_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_2\n",
      "\t\t\tOBJECT=DataField_3\n",
      "\t\t\t\tDataFieldName=\"Scan_Start_Time\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT64\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_3\n",
      "\t\t\tOBJECT=DataField_4\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_4\n",
      "\t\t\tOBJECT=DataField_5\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_5\n",
      "\t\t\tOBJECT=DataField_6\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_6\n",
      "\t\t\tOBJECT=DataField_7\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_7\n",
      "\t\t\tOBJECT=DataField_8\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_8\n",
      "\t\t\tOBJECT=DataField_9\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_9\n",
      "\t\t\tOBJECT=DataField_10\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_10\n",
      "\t\t\tOBJECT=DataField_11\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_11\n",
      "\t\t\tOBJECT=DataField_12\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_12\n",
      "\t\t\tOBJECT=DataField_13\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_13\n",
      "\t\t\tOBJECT=DataField_14\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_14\n",
      "\t\t\tOBJECT=DataField_15\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_15\n",
      "\t\t\tOBJECT=DataField_16\n",
      "\t\t\t\tDataFieldName=\"Brightness_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Number\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_16\n",
      "\t\t\tOBJECT=DataField_17\n",
      "\t\t\t\tDataFieldName=\"Surface_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_17\n",
      "\t\t\tOBJECT=DataField_18\n",
      "\t\t\t\tDataFieldName=\"Surface_Pressure\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_18\n",
      "\t\t\tOBJECT=DataField_19\n",
      "\t\t\t\tDataFieldName=\"Cloud_Height_Method\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_19\n",
      "\t\t\tOBJECT=DataField_20\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_20\n",
      "\t\t\tOBJECT=DataField_21\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_21\n",
      "\t\t\tOBJECT=DataField_22\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_22\n",
      "\t\t\tOBJECT=DataField_23\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_23\n",
      "\t\t\tOBJECT=DataField_24\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_24\n",
      "\t\t\tOBJECT=DataField_25\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_25\n",
      "\t\t\tOBJECT=DataField_26\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_26\n",
      "\t\t\tOBJECT=DataField_27\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_27\n",
      "\t\t\tOBJECT=DataField_28\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_28\n",
      "\t\t\tOBJECT=DataField_29\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_29\n",
      "\t\t\tOBJECT=DataField_30\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_30\n",
      "\t\t\tOBJECT=DataField_31\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_31\n",
      "\t\t\tOBJECT=DataField_32\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_32\n",
      "\t\t\tOBJECT=DataField_33\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_33\n",
      "\t\t\tOBJECT=DataField_34\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_34\n",
      "\t\t\tOBJECT=DataField_35\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_35\n",
      "\t\t\tOBJECT=DataField_36\n",
      "\t\t\t\tDataFieldName=\"Tropopause_Height\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_36\n",
      "\t\t\tOBJECT=DataField_37\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_37\n",
      "\t\t\tOBJECT=DataField_38\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_38\n",
      "\t\t\tOBJECT=DataField_39\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_39\n",
      "\t\t\tOBJECT=DataField_40\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_40\n",
      "\t\t\tOBJECT=DataField_41\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_41\n",
      "\t\t\tOBJECT=DataField_42\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_42\n",
      "\t\t\tOBJECT=DataField_43\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_43\n",
      "\t\t\tOBJECT=DataField_44\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_44\n",
      "\t\t\tOBJECT=DataField_45\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_45\n",
      "\t\t\tOBJECT=DataField_46\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_46\n",
      "\t\t\tOBJECT=DataField_47\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_47\n",
      "\t\t\tOBJECT=DataField_48\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_48\n",
      "\t\t\tOBJECT=DataField_49\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Infrared\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_49\n",
      "\t\t\tOBJECT=DataField_50\n",
      "\t\t\t\tDataFieldName=\"Spectral_Cloud_Forcing\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Forcing\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_50\n",
      "\t\t\tOBJECT=DataField_51\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_From_Ratios\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Ratio\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_51\n",
      "\t\t\tOBJECT=DataField_52\n",
      "\t\t\t\tDataFieldName=\"Radiance_Variance\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_52\n",
      "\t\t\tOBJECT=DataField_53\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_53\n",
      "\t\t\tOBJECT=DataField_54\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_54\n",
      "\t\t\tOBJECT=DataField_55\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_55\n",
      "\t\t\tOBJECT=DataField_56\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_56\n",
      "\t\t\tOBJECT=DataField_57\n",
      "\t\t\t\tDataFieldName=\"IRP_CTH_Consistency_Flag_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_57\n",
      "\t\t\tOBJECT=DataField_58\n",
      "\t\t\t\tDataFieldName=\"os_top_flag_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_58\n",
      "\t\t\tOBJECT=DataField_59\n",
      "\t\t\t\tDataFieldName=\"cloud_top_pressure_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_59\n",
      "\t\t\tOBJECT=DataField_60\n",
      "\t\t\t\tDataFieldName=\"cloud_top_height_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_60\n",
      "\t\t\tOBJECT=DataField_61\n",
      "\t\t\t\tDataFieldName=\"cloud_top_temperature_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_61\n",
      "\t\t\tOBJECT=DataField_62\n",
      "\t\t\t\tDataFieldName=\"cloud_emissivity_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_62\n",
      "\t\t\tOBJECT=DataField_63\n",
      "\t\t\t\tDataFieldName=\"cloud_top_method_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_63\n",
      "\t\t\tOBJECT=DataField_64\n",
      "\t\t\t\tDataFieldName=\"surface_temperature_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_64\n",
      "\t\t\tOBJECT=DataField_65\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss11_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_65\n",
      "\t\t\tOBJECT=DataField_66\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss12_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_66\n",
      "\t\t\tOBJECT=DataField_67\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss13_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_67\n",
      "\t\t\tOBJECT=DataField_68\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss85_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_68\n",
      "\t\t\tOBJECT=DataField_69\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_69\n",
      "\t\t\tOBJECT=DataField_70\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_70\n",
      "\t\t\tOBJECT=DataField_71\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_71\n",
      "\t\t\tOBJECT=DataField_72\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_72\n",
      "\t\t\tOBJECT=DataField_73\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_73\n",
      "\t\t\tOBJECT=DataField_74\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_74\n",
      "\t\t\tOBJECT=DataField_75\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_75\n",
      "\t\t\tOBJECT=DataField_76\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_76\n",
      "\t\t\tOBJECT=DataField_77\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_77\n",
      "\t\t\tOBJECT=DataField_78\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_78\n",
      "\t\t\tOBJECT=DataField_79\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_79\n",
      "\t\t\tOBJECT=DataField_80\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_80\n",
      "\t\t\tOBJECT=DataField_81\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_81\n",
      "\t\t\tOBJECT=DataField_82\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_82\n",
      "\t\t\tOBJECT=DataField_83\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_83\n",
      "\t\t\tOBJECT=DataField_84\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_84\n",
      "\t\t\tOBJECT=DataField_85\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_85\n",
      "\t\t\tOBJECT=DataField_86\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_86\n",
      "\t\t\tOBJECT=DataField_87\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_87\n",
      "\t\t\tOBJECT=DataField_88\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_88\n",
      "\t\t\tOBJECT=DataField_89\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_89\n",
      "\t\t\tOBJECT=DataField_90\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_90\n",
      "\t\t\tOBJECT=DataField_91\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_91\n",
      "\t\t\tOBJECT=DataField_92\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_92\n",
      "\t\t\tOBJECT=DataField_93\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_93\n",
      "\t\t\tOBJECT=DataField_94\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_94\n",
      "\t\t\tOBJECT=DataField_95\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_95\n",
      "\t\t\tOBJECT=DataField_96\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_96\n",
      "\t\t\tOBJECT=DataField_97\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_97\n",
      "\t\t\tOBJECT=DataField_98\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_98\n",
      "\t\t\tOBJECT=DataField_99\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_99\n",
      "\t\t\tOBJECT=DataField_100\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_100\n",
      "\t\t\tOBJECT=DataField_101\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_101\n",
      "\t\t\tOBJECT=DataField_102\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_102\n",
      "\t\t\tOBJECT=DataField_103\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_103\n",
      "\t\t\tOBJECT=DataField_104\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_104\n",
      "\t\t\tOBJECT=DataField_105\n",
      "\t\t\t\tDataFieldName=\"Above_Cloud_Water_Vapor_094\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_105\n",
      "\t\t\tOBJECT=DataField_106\n",
      "\t\t\t\tDataFieldName=\"IRW_Low_Cloud_Temperature_From_COP\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_106\n",
      "\t\t\tOBJECT=DataField_107\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Optical_Properties\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_107\n",
      "\t\t\tOBJECT=DataField_108\n",
      "\t\t\t\tDataFieldName=\"Cloud_Multi_Layer_Flag\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_108\n",
      "\t\t\tOBJECT=DataField_109\n",
      "\t\t\t\tDataFieldName=\"Cirrus_Reflectance\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_109\n",
      "\t\t\tOBJECT=DataField_110\n",
      "\t\t\t\tDataFieldName=\"Cirrus_Reflectance_Flag\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_110\n",
      "\t\t\tOBJECT=DataField_111\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_5km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\",\"Cloud_Mask_5km_Num_Bytes\")\n",
      "\t\t\tEND_OBJECT=DataField_111\n",
      "\t\t\tOBJECT=DataField_112\n",
      "\t\t\t\tDataFieldName=\"Quality_Assurance_5km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\",\"QA_Parameter_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_112\n",
      "\t\t\tOBJECT=DataField_113\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"Cloud_Mask_1km_Num_Bytes\")\n",
      "\t\t\tEND_OBJECT=DataField_113\n",
      "\t\t\tOBJECT=DataField_114\n",
      "\t\t\t\tDataFieldName=\"Extinction_Efficiency_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_114\n",
      "\t\t\tOBJECT=DataField_115\n",
      "\t\t\t\tDataFieldName=\"Asymmetry_Parameter_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_115\n",
      "\t\t\tOBJECT=DataField_116\n",
      "\t\t\t\tDataFieldName=\"Single_Scatter_Albedo_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_116\n",
      "\t\t\tOBJECT=DataField_117\n",
      "\t\t\t\tDataFieldName=\"Extinction_Efficiency_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_117\n",
      "\t\t\tOBJECT=DataField_118\n",
      "\t\t\t\tDataFieldName=\"Asymmetry_Parameter_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_118\n",
      "\t\t\tOBJECT=DataField_119\n",
      "\t\t\t\tDataFieldName=\"Single_Scatter_Albedo_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_119\n",
      "\t\t\tOBJECT=DataField_120\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_SPI\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"SPI_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_120\n",
      "\t\t\tOBJECT=DataField_121\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_121\n",
      "\t\t\tOBJECT=DataField_122\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_122\n",
      "\t\t\tOBJECT=DataField_123\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_123\n",
      "\t\t\tOBJECT=DataField_124\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_124\n",
      "\t\t\tOBJECT=DataField_125\n",
      "\t\t\t\tDataFieldName=\"Atm_Corr_Refl\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"ACR_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_125\n",
      "\t\t\tOBJECT=DataField_126\n",
      "\t\t\t\tDataFieldName=\"Quality_Assurance_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"QA_Parameter_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_126\n",
      "\t\tEND_GROUP=DataField\n",
      "\t\tGROUP=MergedFields\n",
      "\t\tEND_GROUP=MergedFields\n",
      "\tEND_GROUP=SWATH_1\n",
      "END_GROUP=SwathStructure\n",
      "GROUP=GridStructure\n",
      "END_GROUP=GridStructure\n",
      "GROUP=PointStructure\n",
      "END_GROUP=PointStructure\n",
      "END\n",
      "\n",
      "    Number_of_Instrument_Scans: 2040\n",
      "    Maximum_Number_of_1km_Frames: 1354\n",
      "    history: $Id: MOD06_L2.CDL.fs,v 1.13 2013/06/19 15:38:46 wind Exp $                          \n",
      "\n",
      "    title: MODIS Level 2 Cloud Properties                                                      \n",
      "\n",
      "    CoreMetadata.0: \n",
      "GROUP                  = INVENTORYMETADATA\n",
      "  GROUPTYPE            = MASTERGROUP\n",
      "\n",
      "  GROUP                  = ECSDATAGRANULE\n",
      "\n",
      "    OBJECT                 = REPROCESSINGPLANNED\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"further update is anticipated\"\n",
      "    END_OBJECT             = REPROCESSINGPLANNED\n",
      "\n",
      "    OBJECT                 = REPROCESSINGACTUAL\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"processed once\"\n",
      "    END_OBJECT             = REPROCESSINGACTUAL\n",
      "\n",
      "    OBJECT                 = LOCALGRANULEID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MYD06_L2.A2002185.0000.061.2018003215042.hdf\"\n",
      "    END_OBJECT             = LOCALGRANULEID\n",
      "\n",
      "    OBJECT                 = DAYNIGHTFLAG\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"Night\"\n",
      "    END_OBJECT             = DAYNIGHTFLAG\n",
      "\n",
      "    OBJECT                 = PRODUCTIONDATETIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2018-01-03T21:50:42.000Z\"\n",
      "    END_OBJECT             = PRODUCTIONDATETIME\n",
      "\n",
      "    OBJECT                 = LOCALVERSIONID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"061\"\n",
      "    END_OBJECT             = LOCALVERSIONID\n",
      "\n",
      "  END_GROUP              = ECSDATAGRANULE\n",
      "\n",
      "  GROUP                  = MEASUREDPARAMETER\n",
      "\n",
      "    OBJECT                 = MEASUREDPARAMETERCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = PARAMETERNAME\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"Cloud_Top_Pressure\"\n",
      "      END_OBJECT             = PARAMETERNAME\n",
      "\n",
      "      GROUP                  = QAFLAGS\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = AUTOMATICQUALITYFLAG\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"Passed\"\n",
      "        END_OBJECT             = AUTOMATICQUALITYFLAG\n",
      "\n",
      "        OBJECT                 = AUTOMATICQUALITYFLAGEXPLANATION\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"Passed: >10% useable; Failed: <10% useable\"\n",
      "        END_OBJECT             = AUTOMATICQUALITYFLAGEXPLANATION\n",
      "\n",
      "        OBJECT                 = SCIENCEQUALITYFLAG\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = \"Not Investigated\"\n",
      "          CLASS                = \"1\"\n",
      "        END_OBJECT             = SCIENCEQUALITYFLAG\n",
      "\n",
      "        OBJECT                 = SCIENCEQUALITYFLAGEXPLANATION\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = \"See http://modis-atmos.gsfc.nasa.gov/validation.html for more details on MODIS Atmosphere data quality.\"\n",
      "          CLASS                = \"1\"\n",
      "        END_OBJECT             = SCIENCEQUALITYFLAGEXPLANATION\n",
      "\n",
      "      END_GROUP              = QAFLAGS\n",
      "\n",
      "      GROUP                  = QASTATS\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = QAPERCENTMISSINGDATA\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = 63\n",
      "        END_OBJECT             = QAPERCENTMISSINGDATA\n",
      "\n",
      "      END_GROUP              = QASTATS\n",
      "\n",
      "    END_OBJECT             = MEASUREDPARAMETERCONTAINER\n",
      "\n",
      "  END_GROUP              = MEASUREDPARAMETER\n",
      "\n",
      "  GROUP                  = ORBITCALCULATEDSPATIALDOMAIN\n",
      "\n",
      "    OBJECT                 = ORBITCALCULATEDSPATIALDOMAINCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ORBITNUMBER\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = 885\n",
      "      END_OBJECT             = ORBITNUMBER\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGLONGITUDE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = 20.4199118302919\n",
      "      END_OBJECT             = EQUATORCROSSINGLONGITUDE\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGTIME\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"00:12:58.823356\"\n",
      "      END_OBJECT             = EQUATORCROSSINGTIME\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGDATE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"2002-07-04\"\n",
      "      END_OBJECT             = EQUATORCROSSINGDATE\n",
      "\n",
      "    END_OBJECT             = ORBITCALCULATEDSPATIALDOMAINCONTAINER\n",
      "\n",
      "  END_GROUP              = ORBITCALCULATEDSPATIALDOMAIN\n",
      "\n",
      "  GROUP                  = COLLECTIONDESCRIPTIONCLASS\n",
      "\n",
      "    OBJECT                 = SHORTNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MYD06_L2\"\n",
      "    END_OBJECT             = SHORTNAME\n",
      "\n",
      "    OBJECT                 = VERSIONID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = 61\n",
      "    END_OBJECT             = VERSIONID\n",
      "\n",
      "  END_GROUP              = COLLECTIONDESCRIPTIONCLASS\n",
      "\n",
      "  GROUP                  = INPUTGRANULE\n",
      "\n",
      "    OBJECT                 = INPUTPOINTER\n",
      "      NUM_VAL              = 50\n",
      "      VALUE                = (\"MYD021KM.A2002185.0000.061.2017363141721.hdf\", \"MYD03.A2002185.0000.061.2017362174430.hdf\", \"MYD35_L2.A2002185.0000.061.2017363141846.hdf\", \"gdas1.PGrbF00.020704.00z\", \"oisst.20020703\", \"goge1_2_img.v1\", \"modisdet.dry.101.lit_end.v3\", \"\n",
      "          modisdet.ozo.101.lit_end.v3\", \"modisdet.wts.101.lit_end.v3\", \"modisdet.wtl.101.lit_end.v3\", \"modisdet.wco.101.lit_end.v3\", \"MYDCSR_B.A2002185.061.2017363154239.hdf\", \"gdas1.PGrbF00.020704.06z\", \"eng.020704\", \"NISE_SSMIF13_20020704.HDFEOS\", \"MODIS_Ice_library.hdf.v5\", \"\n",
      "          MODIS_Water_library.hdf.v4\", \"MODIS_Ice_library_ws3.hdf.v3\", \"MODIS_Ice_library_ws3sd.hdf.v4\", \"MODIS_Ice_library_ws7.hdf.v3\", \"MODIS_Ice_library_ws7sd.hdf.v4\", \"MODIS_Ice_library_ws15.hdf.v3\", \"MODIS_Ice_library_ws15sd.hdf.v4\", \"MODIS_Water_library_ws3.hdf.v2\", \"\n",
      "          MODIS_Water_library_ws3sd.hdf.v3\", \"MODIS_Water_library_ws7.hdf.v2\", \"MODIS_Water_library_ws7sd.hdf.v3\", \"MODIS_Water_library_ws15.hdf.v2\", \"MODIS_Water_library_ws15sd.hdf.v3\", \"MODIS_Ice_WaterPhaseFunc.hdf.v3\", \"Transmittance.hdf.v2\", \"IGBP.EcoMap.NtoS.2004.149.v004.hdf\", \"\n",
      "          AlbSnwStst.ByNISE.W90.D90.WS.Hemi.2000-2004.YrAvg.hdf\", \"MCD43GF_wsa_Band1_185_2002_V006.hdf\", \"MCD43GF_wsa_Band2_185_2002_V006.hdf\", \"MCD43GF_wsa_Band5_185_2002_V006.hdf\", \"MCD43GF_wsa_Band6_185_2002_V006.hdf\", \"MCD43GF_wsa_Band7_185_2002_V006.hdf\")\n",
      "    END_OBJECT             = INPUTPOINTER\n",
      "\n",
      "  END_GROUP              = INPUTGRANULE\n",
      "\n",
      "  GROUP                  = SPATIALDOMAINCONTAINER\n",
      "\n",
      "    GROUP                  = HORIZONTALSPATIALDOMAINCONTAINER\n",
      "\n",
      "      GROUP                  = BOUNDINGRECTANGLE\n",
      "\n",
      "        OBJECT                 = WESTBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 15.0125121267976\n",
      "        END_OBJECT             = WESTBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = NORTHBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 47.4712121222453\n",
      "        END_OBJECT             = NORTHBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = EASTBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 46.3027701461145\n",
      "        END_OBJECT             = EASTBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = SOUTHBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 26.6036189855835\n",
      "        END_OBJECT             = SOUTHBOUNDINGCOORDINATE\n",
      "\n",
      "      END_GROUP              = BOUNDINGRECTANGLE\n",
      "\n",
      "    END_GROUP              = HORIZONTALSPATIALDOMAINCONTAINER\n",
      "\n",
      "  END_GROUP              = SPATIALDOMAINCONTAINER\n",
      "\n",
      "  GROUP                  = RANGEDATETIME\n",
      "\n",
      "    OBJECT                 = RANGEBEGINNINGDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2002-07-04\"\n",
      "    END_OBJECT             = RANGEBEGINNINGDATE\n",
      "\n",
      "    OBJECT                 = RANGEBEGINNINGTIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"00:00:00.000000\"\n",
      "    END_OBJECT             = RANGEBEGINNINGTIME\n",
      "\n",
      "    OBJECT                 = RANGEENDINGDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2002-07-04\"\n",
      "    END_OBJECT             = RANGEENDINGDATE\n",
      "\n",
      "    OBJECT                 = RANGEENDINGTIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"00:05:00.000000\"\n",
      "    END_OBJECT             = RANGEENDINGTIME\n",
      "\n",
      "  END_GROUP              = RANGEDATETIME\n",
      "\n",
      "  GROUP                  = PGEVERSIONCLASS\n",
      "\n",
      "    OBJECT                 = PGEVERSION\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"6.1.4\"\n",
      "    END_OBJECT             = PGEVERSION\n",
      "\n",
      "  END_GROUP              = PGEVERSIONCLASS\n",
      "\n",
      "  GROUP                  = ANCILLARYINPUTGRANULE\n",
      "\n",
      "    OBJECT                 = ANCILLARYINPUTGRANULECONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ANCILLARYINPUTTYPE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"Geolocation\"\n",
      "      END_OBJECT             = ANCILLARYINPUTTYPE\n",
      "\n",
      "      OBJECT                 = ANCILLARYINPUTPOINTER\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"MYD03.A2002185.0000.061.2017362174430.hdf\"\n",
      "      END_OBJECT             = ANCILLARYINPUTPOINTER\n",
      "\n",
      "    END_OBJECT             = ANCILLARYINPUTGRANULECONTAINER\n",
      "\n",
      "  END_GROUP              = ANCILLARYINPUTGRANULE\n",
      "\n",
      "  GROUP                  = ASSOCIATEDPLATFORMINSTRUMENTSENSOR\n",
      "\n",
      "    OBJECT                 = ASSOCIATEDPLATFORMINSTRUMENTSENSORCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDSENSORSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MODIS\"\n",
      "      END_OBJECT             = ASSOCIATEDSENSORSHORTNAME\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDPLATFORMSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"Aqua\"\n",
      "      END_OBJECT             = ASSOCIATEDPLATFORMSHORTNAME\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDINSTRUMENTSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MODIS\"\n",
      "      END_OBJECT             = ASSOCIATEDINSTRUMENTSHORTNAME\n",
      "\n",
      "    END_OBJECT             = ASSOCIATEDPLATFORMINSTRUMENTSENSORCONTAINER\n",
      "\n",
      "  END_GROUP              = ASSOCIATEDPLATFORMINSTRUMENTSENSOR\n",
      "\n",
      "  GROUP                  = ADDITIONALATTRIBUTES\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SuccessCloudTopPropRtrPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"   36.98\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"2\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"2\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SuccessCloudPhaseRtrPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"2\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"2\"\n",
      "          VALUE                = \"   36.66\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"4\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"4\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"LowCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"4\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"4\"\n",
      "          VALUE                = \"   32.12\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"5\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"5\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MidCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"5\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"5\"\n",
      "          VALUE                = \"    2.40\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"6\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"6\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"HighCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"6\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"6\"\n",
      "          VALUE                = \"    2.45\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"7\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"7\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"ThinCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"7\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"7\"\n",
      "          VALUE                = \"    9.74\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"8\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"8\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"ThickCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"8\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"8\"\n",
      "          VALUE                = \"   10.37\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"9\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"9\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"OpaqueCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"9\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"9\"\n",
      "          VALUE                = \"   16.86\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"10\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"10\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"CirrusCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"10\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"10\"\n",
      "          VALUE                = \"   20.12\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"11\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"11\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"IceCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"11\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"11\"\n",
      "          VALUE                = \"    1.27\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"12\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"12\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"WaterCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"12\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"12\"\n",
      "          VALUE                = \"   32.99\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"13\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"13\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MixedCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"13\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"13\"\n",
      "          VALUE                = \"    0.00\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"14\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"14\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"CloudPhaseUncertainPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"14\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"14\"\n",
      "          VALUE                = \"    2.40\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"15\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"15\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"OceanCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"15\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"15\"\n",
      "          VALUE                = \"   45.11\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"16\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"16\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"LandCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"16\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"16\"\n",
      "          VALUE                = \"   54.89\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"17\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"17\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SnowCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"17\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"17\"\n",
      "          VALUE                = \"    0.06\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"18\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"18\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"identifier_product_doi\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"18\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"18\"\n",
      "          VALUE                = \"10.5067/MODIS/MYD06_L2.061\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"19\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"19\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"identifier_product_doi_authority\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"19\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"19\"\n",
      "          VALUE                = \"http://dx.doi.org\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "  END_GROUP              = ADDITIONALATTRIBUTES\n",
      "\n",
      "END_GROUP              = INVENTORYMETADATA\n",
      "\n",
      "END\n",
      "\n",
      "    ArchiveMetadata.0: \n",
      "GROUP                  = ARCHIVEDMETADATA\n",
      "  GROUPTYPE            = MASTERGROUP\n",
      "\n",
      "  OBJECT                 = PROCESSINGENVIRONMENT\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"Linux minion7260 3.10.0-693.11.1.el7.x86_64 #1 SMP Mon Dec 4 23:52:40 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\"\n",
      "  END_OBJECT             = PROCESSINGENVIRONMENT\n",
      "\n",
      "  GROUP                  = ALGORITHMPACKAGE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEACCEPTANCEDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"June 1997\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEACCEPTANCEDATE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEMATURITYCODE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"at-launch\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEMATURITYCODE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGENAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"ATBD-MOD-04 and ATBD-MOD-05\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGENAME\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEVERSION\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEVERSION\n",
      "\n",
      "    OBJECT                 = INSTRUMENTNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"Moderate Resolution Imaging Spectroradiometer\"\n",
      "    END_OBJECT             = INSTRUMENTNAME\n",
      "\n",
      "    OBJECT                 = LONGNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MODIS/Aqua Clouds 5-Min L2 Swath 1km and 5km\"\n",
      "    END_OBJECT             = LONGNAME\n",
      "\n",
      "    OBJECT                 = LOCALINPUTGRANULEID\n",
      "      NUM_VAL              = 20\n",
      "      VALUE                = (\"MYD021KM.A2002185.0000.061.2017363141721.hdf\", \"MYD03.A2002185.0000.061.2017362174430.hdf\", \"MYD35_L2.A2002185.0000.061.2017363141846.hdf\", \"MYDCSR_B.A2002185.061.2017363154239.hdf\")\n",
      "    END_OBJECT             = LOCALINPUTGRANULEID\n",
      "\n",
      "  END_GROUP              = ALGORITHMPACKAGE\n",
      "\n",
      "  GROUP                  = GPOLYGON\n",
      "\n",
      "    OBJECT                 = GPOLYGONCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      GROUP                  = GRING\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = EXCLUSIONGRINGFLAG\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"N\"\n",
      "        END_OBJECT             = EXCLUSIONGRINGFLAG\n",
      "\n",
      "      END_GROUP              = GRING\n",
      "\n",
      "      GROUP                  = GRINGPOINT\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = GRINGPOINTLONGITUDE\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (16.6802142122724, 46.3037511907536, 38.7590005022001, 14.9898525858698)\n",
      "        END_OBJECT             = GRINGPOINTLONGITUDE\n",
      "\n",
      "        OBJECT                 = GRINGPOINTLATITUDE\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (47.4897565211684, 43.2358532665263, 26.5236230538033, 29.7440219071349)\n",
      "        END_OBJECT             = GRINGPOINTLATITUDE\n",
      "\n",
      "        OBJECT                 = GRINGPOINTSEQUENCENO\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (1, 2, 3, 4)\n",
      "        END_OBJECT             = GRINGPOINTSEQUENCENO\n",
      "\n",
      "      END_GROUP              = GRINGPOINT\n",
      "\n",
      "    END_OBJECT             = GPOLYGONCONTAINER\n",
      "\n",
      "  END_GROUP              = GPOLYGON\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_TOP_PROPERTY_IR\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_TOP_PROPERTY_IR\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_PHASE_IR\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_PHASE_IR\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_PROPERTY_VIS\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_PROPERTY_VIS\n",
      "\n",
      "  OBJECT                 = DESCRREVISION\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"6.1\"\n",
      "  END_OBJECT             = DESCRREVISION\n",
      "\n",
      "  OBJECT                 = PRODUCTIONHISTORY\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"PGE06:6.1.4\"\n",
      "  END_OBJECT             = PRODUCTIONHISTORY\n",
      "\n",
      "END_GROUP              = ARCHIVEDMETADATA\n",
      "\n",
      "END\n",
      "\n",
      "    Clear_Sky_Restoral_Status: y\n",
      "    Collection_4_Phase_Used: n\n",
      "    Ice_Phase_Forced: n\n",
      "    Water_Phase_Forced: n\n",
      "    identifier_product_doi: 10.5067/MODIS/MYD06_L2.061\n",
      "    identifier_product_doi_authority: http://dx.doi.org\n",
      "    dimensions(sizes): Cell_Along_Swath_5km:mod06(408), Cell_Across_Swath_5km:mod06(270), Band_Number:mod06(7), Band_Forcing:mod06(5), Band_Ratio:mod06(5), Cell_Along_Swath_1km:mod06(2040), Cell_Across_Swath_1km:mod06(1354), Cloud_Mask_5km_Num_Bytes:mod06(2), QA_Parameter_5km:mod06(10), Cloud_Mask_1km_Num_Bytes:mod06(2), RadTran_NRE_Ice:mod06(12), RadTran_NWL:mod06(7), RadTran_NRE_Liq:mod06(18), SPI_nband:mod06(2), RFM_nband:mod06(3), ACR_nband:mod06(6), QA_Parameter_1km:mod06(9), fakeDim17(17)\n",
      "    variables(dimensions): >f4 \u001b[4mLatitude\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >f4 \u001b[4mLongitude\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >f8 \u001b[4mScan_Start_Time\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mBrightness_Temperature\u001b[0m(Band_Number:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSurface_Temperature\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSurface_Pressure\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Height_Method\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mTropopause_Height\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Infrared\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSpectral_Cloud_Forcing\u001b[0m(Band_Forcing:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_From_Ratios\u001b[0m(Band_Ratio:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mRadiance_Variance\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mIRP_CTH_Consistency_Flag_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mos_top_flag_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_pressure_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_height_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_temperature_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mcloud_emissivity_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mcloud_top_method_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4msurface_temperature_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss11_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss12_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss13_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss85_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mAbove_Cloud_Water_Vapor_094\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mIRW_Low_Cloud_Temperature_From_COP\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Phase_Optical_Properties\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Multi_Layer_Flag\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCirrus_Reflectance\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCirrus_Reflectance_Flag\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Mask_5km\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06,Cloud_Mask_5km_Num_Bytes:mod06), int8 \u001b[4mQuality_Assurance_5km\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06,QA_Parameter_5km:mod06), int8 \u001b[4mCloud_Mask_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,Cloud_Mask_1km_Num_Bytes:mod06), >f4 \u001b[4mExtinction_Efficiency_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mAsymmetry_Parameter_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mSingle_Scatter_Albedo_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mExtinction_Efficiency_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >f4 \u001b[4mAsymmetry_Parameter_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >f4 \u001b[4mSingle_Scatter_Albedo_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >i2 \u001b[4mCloud_Mask_SPI\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,SPI_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mAtm_Corr_Refl\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,ACR_nband:mod06), int8 \u001b[4mQuality_Assurance_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,QA_Parameter_1km:mod06), >f4 \u001b[4mStatistics_1km_sds\u001b[0m(fakeDim17)\n",
      "    groups: \n",
      "\n",
      "---------------\n",
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF4):\n",
      "    HDFEOSVersion: HDFEOS_V2.19\n",
      "    StructMetadata.0: GROUP=SwathStructure\n",
      "\tGROUP=SWATH_1\n",
      "\t\tSwathName=\"mod06\"\n",
      "\t\tGROUP=Dimension\n",
      "\t\t\tOBJECT=Dimension_1\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_5km\"\n",
      "\t\t\t\tSize=270\n",
      "\t\t\tEND_OBJECT=Dimension_1\n",
      "\t\t\tOBJECT=Dimension_2\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_5km\"\n",
      "\t\t\t\tSize=408\n",
      "\t\t\tEND_OBJECT=Dimension_2\n",
      "\t\t\tOBJECT=Dimension_3\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_1km\"\n",
      "\t\t\t\tSize=1354\n",
      "\t\t\tEND_OBJECT=Dimension_3\n",
      "\t\t\tOBJECT=Dimension_4\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_1km\"\n",
      "\t\t\t\tSize=2040\n",
      "\t\t\tEND_OBJECT=Dimension_4\n",
      "\t\t\tOBJECT=Dimension_5\n",
      "\t\t\t\tDimensionName=\"Cell_Across_Swath_hkm\"\n",
      "\t\t\t\tSize=2708\n",
      "\t\t\tEND_OBJECT=Dimension_5\n",
      "\t\t\tOBJECT=Dimension_6\n",
      "\t\t\t\tDimensionName=\"Cell_Along_Swath_hkm\"\n",
      "\t\t\t\tSize=4060\n",
      "\t\t\tEND_OBJECT=Dimension_6\n",
      "\t\t\tOBJECT=Dimension_7\n",
      "\t\t\t\tDimensionName=\"Band_Number\"\n",
      "\t\t\t\tSize=7\n",
      "\t\t\tEND_OBJECT=Dimension_7\n",
      "\t\t\tOBJECT=Dimension_8\n",
      "\t\t\t\tDimensionName=\"Band_Ratio\"\n",
      "\t\t\t\tSize=5\n",
      "\t\t\tEND_OBJECT=Dimension_8\n",
      "\t\t\tOBJECT=Dimension_9\n",
      "\t\t\t\tDimensionName=\"Band_Forcing\"\n",
      "\t\t\t\tSize=5\n",
      "\t\t\tEND_OBJECT=Dimension_9\n",
      "\t\t\tOBJECT=Dimension_10\n",
      "\t\t\t\tDimensionName=\"Band_Difference\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_10\n",
      "\t\t\tOBJECT=Dimension_11\n",
      "\t\t\t\tDimensionName=\"QA_Parameter_5km\"\n",
      "\t\t\t\tSize=10\n",
      "\t\t\tEND_OBJECT=Dimension_11\n",
      "\t\t\tOBJECT=Dimension_12\n",
      "\t\t\t\tDimensionName=\"QA_Parameter_1km\"\n",
      "\t\t\t\tSize=9\n",
      "\t\t\tEND_OBJECT=Dimension_12\n",
      "\t\t\tOBJECT=Dimension_13\n",
      "\t\t\t\tDimensionName=\"Cloud_Mask_1km_Num_Bytes\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_13\n",
      "\t\t\tOBJECT=Dimension_14\n",
      "\t\t\t\tDimensionName=\"Cloud_Mask_5km_Num_Bytes\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_14\n",
      "\t\t\tOBJECT=Dimension_15\n",
      "\t\t\t\tDimensionName=\"RadTran_NWL\"\n",
      "\t\t\t\tSize=7\n",
      "\t\t\tEND_OBJECT=Dimension_15\n",
      "\t\t\tOBJECT=Dimension_16\n",
      "\t\t\t\tDimensionName=\"RadTran_NRE_Ice\"\n",
      "\t\t\t\tSize=12\n",
      "\t\t\tEND_OBJECT=Dimension_16\n",
      "\t\t\tOBJECT=Dimension_17\n",
      "\t\t\t\tDimensionName=\"RadTran_NRE_Liq\"\n",
      "\t\t\t\tSize=18\n",
      "\t\t\tEND_OBJECT=Dimension_17\n",
      "\t\t\tOBJECT=Dimension_18\n",
      "\t\t\t\tDimensionName=\"SPI_nband\"\n",
      "\t\t\t\tSize=2\n",
      "\t\t\tEND_OBJECT=Dimension_18\n",
      "\t\t\tOBJECT=Dimension_19\n",
      "\t\t\t\tDimensionName=\"RFM_nband\"\n",
      "\t\t\t\tSize=3\n",
      "\t\t\tEND_OBJECT=Dimension_19\n",
      "\t\t\tOBJECT=Dimension_20\n",
      "\t\t\t\tDimensionName=\"ACR_nband\"\n",
      "\t\t\t\tSize=6\n",
      "\t\t\tEND_OBJECT=Dimension_20\n",
      "\t\t\tOBJECT=Dimension_21\n",
      "\t\t\t\tDimensionName=\"Statistic_Parameter_1km\"\n",
      "\t\t\t\tSize=17\n",
      "\t\t\tEND_OBJECT=Dimension_21\n",
      "\t\tEND_GROUP=Dimension\n",
      "\t\tGROUP=DimensionMap\n",
      "\t\t\tOBJECT=DimensionMap_1\n",
      "\t\t\t\tGeoDimension=\"Cell_Across_Swath_5km\"\n",
      "\t\t\t\tDataDimension=\"Cell_Across_Swath_1km\"\n",
      "\t\t\t\tOffset=2\n",
      "\t\t\t\tIncrement=5\n",
      "\t\t\tEND_OBJECT=DimensionMap_1\n",
      "\t\t\tOBJECT=DimensionMap_2\n",
      "\t\t\t\tGeoDimension=\"Cell_Along_Swath_5km\"\n",
      "\t\t\t\tDataDimension=\"Cell_Along_Swath_1km\"\n",
      "\t\t\t\tOffset=2\n",
      "\t\t\t\tIncrement=5\n",
      "\t\t\tEND_OBJECT=DimensionMap_2\n",
      "\t\tEND_GROUP=DimensionMap\n",
      "\t\tGROUP=IndexDimensionMap\n",
      "\t\tEND_GROUP=IndexDimensionMap\n",
      "\t\tGROUP=GeoField\n",
      "\t\t\tOBJECT=GeoField_1\n",
      "\t\t\t\tGeoFieldName=\"Latitude\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=GeoField_1\n",
      "\t\t\tOBJECT=GeoField_2\n",
      "\t\t\t\tGeoFieldName=\"Longitude\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=GeoField_2\n",
      "\t\tEND_GROUP=GeoField\n",
      "\t\tGROUP=DataField\n",
      "\t\t\tOBJECT=DataField_1\n",
      "\t\t\t\tDataFieldName=\"Band_Number\"\n",
      "\t\t\t\tDataType=DFNT_INT32\n",
      "\t\t\t\tDimList=(\"Band_Number\")\n",
      "\t\t\tEND_OBJECT=DataField_1\n",
      "\t\t\tOBJECT=DataField_2\n",
      "\t\t\t\tDataFieldName=\"Statistics_1km\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"Statistic_Parameter_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_2\n",
      "\t\t\tOBJECT=DataField_3\n",
      "\t\t\t\tDataFieldName=\"Scan_Start_Time\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT64\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_3\n",
      "\t\t\tOBJECT=DataField_4\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_4\n",
      "\t\t\tOBJECT=DataField_5\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_5\n",
      "\t\t\tOBJECT=DataField_6\n",
      "\t\t\t\tDataFieldName=\"Solar_Zenith_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_6\n",
      "\t\t\tOBJECT=DataField_7\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_7\n",
      "\t\t\tOBJECT=DataField_8\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_8\n",
      "\t\t\tOBJECT=DataField_9\n",
      "\t\t\t\tDataFieldName=\"Solar_Azimuth_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_9\n",
      "\t\t\tOBJECT=DataField_10\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_10\n",
      "\t\t\tOBJECT=DataField_11\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_11\n",
      "\t\t\tOBJECT=DataField_12\n",
      "\t\t\t\tDataFieldName=\"Sensor_Zenith_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_12\n",
      "\t\t\tOBJECT=DataField_13\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_13\n",
      "\t\t\tOBJECT=DataField_14\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_14\n",
      "\t\t\tOBJECT=DataField_15\n",
      "\t\t\t\tDataFieldName=\"Sensor_Azimuth_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_15\n",
      "\t\t\tOBJECT=DataField_16\n",
      "\t\t\t\tDataFieldName=\"Brightness_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Number\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_16\n",
      "\t\t\tOBJECT=DataField_17\n",
      "\t\t\t\tDataFieldName=\"Surface_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_17\n",
      "\t\t\tOBJECT=DataField_18\n",
      "\t\t\t\tDataFieldName=\"Surface_Pressure\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_18\n",
      "\t\t\tOBJECT=DataField_19\n",
      "\t\t\t\tDataFieldName=\"Cloud_Height_Method\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_19\n",
      "\t\t\tOBJECT=DataField_20\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_20\n",
      "\t\t\tOBJECT=DataField_21\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_21\n",
      "\t\t\tOBJECT=DataField_22\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_22\n",
      "\t\t\tOBJECT=DataField_23\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Height_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_23\n",
      "\t\t\tOBJECT=DataField_24\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_24\n",
      "\t\t\tOBJECT=DataField_25\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_25\n",
      "\t\t\tOBJECT=DataField_26\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_26\n",
      "\t\t\tOBJECT=DataField_27\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_27\n",
      "\t\t\tOBJECT=DataField_28\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_28\n",
      "\t\t\tOBJECT=DataField_29\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_29\n",
      "\t\t\tOBJECT=DataField_30\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_30\n",
      "\t\t\tOBJECT=DataField_31\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_31\n",
      "\t\t\tOBJECT=DataField_32\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_32\n",
      "\t\t\tOBJECT=DataField_33\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_33\n",
      "\t\t\tOBJECT=DataField_34\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_34\n",
      "\t\t\tOBJECT=DataField_35\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Temperature_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_35\n",
      "\t\t\tOBJECT=DataField_36\n",
      "\t\t\t\tDataFieldName=\"Tropopause_Height\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_36\n",
      "\t\t\tOBJECT=DataField_37\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_37\n",
      "\t\t\tOBJECT=DataField_38\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_38\n",
      "\t\t\tOBJECT=DataField_39\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_39\n",
      "\t\t\tOBJECT=DataField_40\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_40\n",
      "\t\t\tOBJECT=DataField_41\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_41\n",
      "\t\t\tOBJECT=DataField_42\n",
      "\t\t\t\tDataFieldName=\"Cloud_Fraction_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_42\n",
      "\t\t\tOBJECT=DataField_43\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_43\n",
      "\t\t\tOBJECT=DataField_44\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_44\n",
      "\t\t\tOBJECT=DataField_45\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_45\n",
      "\t\t\tOBJECT=DataField_46\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_46\n",
      "\t\t\tOBJECT=DataField_47\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_47\n",
      "\t\t\tOBJECT=DataField_48\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Emissivity_Nadir_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_48\n",
      "\t\t\tOBJECT=DataField_49\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_Infrared\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_49\n",
      "\t\t\tOBJECT=DataField_50\n",
      "\t\t\t\tDataFieldName=\"Spectral_Cloud_Forcing\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Forcing\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_50\n",
      "\t\t\tOBJECT=DataField_51\n",
      "\t\t\t\tDataFieldName=\"Cloud_Top_Pressure_From_Ratios\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Band_Ratio\",\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_51\n",
      "\t\t\tOBJECT=DataField_52\n",
      "\t\t\t\tDataFieldName=\"Radiance_Variance\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_52\n",
      "\t\t\tOBJECT=DataField_53\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_53\n",
      "\t\t\tOBJECT=DataField_54\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_Night\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_54\n",
      "\t\t\tOBJECT=DataField_55\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_Day\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_55\n",
      "\t\t\tOBJECT=DataField_56\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Infrared_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_56\n",
      "\t\t\tOBJECT=DataField_57\n",
      "\t\t\t\tDataFieldName=\"IRP_CTH_Consistency_Flag_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_57\n",
      "\t\t\tOBJECT=DataField_58\n",
      "\t\t\t\tDataFieldName=\"os_top_flag_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_58\n",
      "\t\t\tOBJECT=DataField_59\n",
      "\t\t\t\tDataFieldName=\"cloud_top_pressure_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_59\n",
      "\t\t\tOBJECT=DataField_60\n",
      "\t\t\t\tDataFieldName=\"cloud_top_height_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_60\n",
      "\t\t\tOBJECT=DataField_61\n",
      "\t\t\t\tDataFieldName=\"cloud_top_temperature_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_61\n",
      "\t\t\tOBJECT=DataField_62\n",
      "\t\t\t\tDataFieldName=\"cloud_emissivity_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_62\n",
      "\t\t\tOBJECT=DataField_63\n",
      "\t\t\t\tDataFieldName=\"cloud_top_method_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_63\n",
      "\t\t\tOBJECT=DataField_64\n",
      "\t\t\t\tDataFieldName=\"surface_temperature_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_64\n",
      "\t\t\tOBJECT=DataField_65\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss11_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_65\n",
      "\t\t\tOBJECT=DataField_66\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss12_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_66\n",
      "\t\t\tOBJECT=DataField_67\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss13_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_67\n",
      "\t\t\tOBJECT=DataField_68\n",
      "\t\t\t\tDataFieldName=\"cloud_emiss85_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_68\n",
      "\t\t\tOBJECT=DataField_69\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_69\n",
      "\t\t\tOBJECT=DataField_70\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_70\n",
      "\t\t\tOBJECT=DataField_71\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_71\n",
      "\t\t\tOBJECT=DataField_72\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_72\n",
      "\t\t\tOBJECT=DataField_73\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_73\n",
      "\t\t\tOBJECT=DataField_74\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_74\n",
      "\t\t\tOBJECT=DataField_75\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_75\n",
      "\t\t\tOBJECT=DataField_76\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_76\n",
      "\t\t\tOBJECT=DataField_77\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_77\n",
      "\t\t\tOBJECT=DataField_78\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_78\n",
      "\t\t\tOBJECT=DataField_79\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_79\n",
      "\t\t\tOBJECT=DataField_80\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_80\n",
      "\t\t\tOBJECT=DataField_81\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_81\n",
      "\t\t\tOBJECT=DataField_82\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_82\n",
      "\t\t\tOBJECT=DataField_83\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_83\n",
      "\t\t\tOBJECT=DataField_84\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_84\n",
      "\t\t\tOBJECT=DataField_85\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_85\n",
      "\t\t\tOBJECT=DataField_86\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_86\n",
      "\t\t\tOBJECT=DataField_87\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_87\n",
      "\t\t\tOBJECT=DataField_88\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_1621_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_88\n",
      "\t\t\tOBJECT=DataField_89\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_89\n",
      "\t\t\tOBJECT=DataField_90\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_16_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_90\n",
      "\t\t\tOBJECT=DataField_91\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_91\n",
      "\t\t\tOBJECT=DataField_92\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_37_PCL\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_92\n",
      "\t\t\tOBJECT=DataField_93\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_93\n",
      "\t\t\tOBJECT=DataField_94\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_94\n",
      "\t\t\tOBJECT=DataField_95\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_95\n",
      "\t\t\tOBJECT=DataField_96\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_96\n",
      "\t\t\tOBJECT=DataField_97\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_97\n",
      "\t\t\tOBJECT=DataField_98\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_98\n",
      "\t\t\tOBJECT=DataField_99\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_99\n",
      "\t\t\tOBJECT=DataField_100\n",
      "\t\t\t\tDataFieldName=\"Cloud_Effective_Radius_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_100\n",
      "\t\t\tOBJECT=DataField_101\n",
      "\t\t\t\tDataFieldName=\"Cloud_Optical_Thickness_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_101\n",
      "\t\t\tOBJECT=DataField_102\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_102\n",
      "\t\t\tOBJECT=DataField_103\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_103\n",
      "\t\t\tOBJECT=DataField_104\n",
      "\t\t\t\tDataFieldName=\"Cloud_Water_Path_Uncertainty_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_104\n",
      "\t\t\tOBJECT=DataField_105\n",
      "\t\t\t\tDataFieldName=\"Above_Cloud_Water_Vapor_094\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_105\n",
      "\t\t\tOBJECT=DataField_106\n",
      "\t\t\t\tDataFieldName=\"IRW_Low_Cloud_Temperature_From_COP\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_106\n",
      "\t\t\tOBJECT=DataField_107\n",
      "\t\t\t\tDataFieldName=\"Cloud_Phase_Optical_Properties\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_107\n",
      "\t\t\tOBJECT=DataField_108\n",
      "\t\t\t\tDataFieldName=\"Cloud_Multi_Layer_Flag\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_108\n",
      "\t\t\tOBJECT=DataField_109\n",
      "\t\t\t\tDataFieldName=\"Cirrus_Reflectance\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_109\n",
      "\t\t\tOBJECT=DataField_110\n",
      "\t\t\t\tDataFieldName=\"Cirrus_Reflectance_Flag\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_110\n",
      "\t\t\tOBJECT=DataField_111\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_5km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\",\"Cloud_Mask_5km_Num_Bytes\")\n",
      "\t\t\tEND_OBJECT=DataField_111\n",
      "\t\t\tOBJECT=DataField_112\n",
      "\t\t\t\tDataFieldName=\"Quality_Assurance_5km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_5km\",\"Cell_Across_Swath_5km\",\"QA_Parameter_5km\")\n",
      "\t\t\tEND_OBJECT=DataField_112\n",
      "\t\t\tOBJECT=DataField_113\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"Cloud_Mask_1km_Num_Bytes\")\n",
      "\t\t\tEND_OBJECT=DataField_113\n",
      "\t\t\tOBJECT=DataField_114\n",
      "\t\t\t\tDataFieldName=\"Extinction_Efficiency_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_114\n",
      "\t\t\tOBJECT=DataField_115\n",
      "\t\t\t\tDataFieldName=\"Asymmetry_Parameter_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_115\n",
      "\t\t\tOBJECT=DataField_116\n",
      "\t\t\t\tDataFieldName=\"Single_Scatter_Albedo_Ice\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Ice\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_116\n",
      "\t\t\tOBJECT=DataField_117\n",
      "\t\t\t\tDataFieldName=\"Extinction_Efficiency_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_117\n",
      "\t\t\tOBJECT=DataField_118\n",
      "\t\t\t\tDataFieldName=\"Asymmetry_Parameter_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_118\n",
      "\t\t\tOBJECT=DataField_119\n",
      "\t\t\t\tDataFieldName=\"Single_Scatter_Albedo_Liq\"\n",
      "\t\t\t\tDataType=DFNT_FLOAT32\n",
      "\t\t\t\tDimList=(\"RadTran_NRE_Liq\",\"RadTran_NWL\")\n",
      "\t\t\tEND_OBJECT=DataField_119\n",
      "\t\t\tOBJECT=DataField_120\n",
      "\t\t\t\tDataFieldName=\"Cloud_Mask_SPI\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"SPI_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_120\n",
      "\t\t\tOBJECT=DataField_121\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_121\n",
      "\t\t\tOBJECT=DataField_122\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_16\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_122\n",
      "\t\t\tOBJECT=DataField_123\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_37\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_123\n",
      "\t\t\tOBJECT=DataField_124\n",
      "\t\t\t\tDataFieldName=\"Retrieval_Failure_Metric_1621\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"RFM_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_124\n",
      "\t\t\tOBJECT=DataField_125\n",
      "\t\t\t\tDataFieldName=\"Atm_Corr_Refl\"\n",
      "\t\t\t\tDataType=DFNT_INT16\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"ACR_nband\")\n",
      "\t\t\tEND_OBJECT=DataField_125\n",
      "\t\t\tOBJECT=DataField_126\n",
      "\t\t\t\tDataFieldName=\"Quality_Assurance_1km\"\n",
      "\t\t\t\tDataType=DFNT_INT8\n",
      "\t\t\t\tDimList=(\"Cell_Along_Swath_1km\",\"Cell_Across_Swath_1km\",\"QA_Parameter_1km\")\n",
      "\t\t\tEND_OBJECT=DataField_126\n",
      "\t\tEND_GROUP=DataField\n",
      "\t\tGROUP=MergedFields\n",
      "\t\tEND_GROUP=MergedFields\n",
      "\tEND_GROUP=SWATH_1\n",
      "END_GROUP=SwathStructure\n",
      "GROUP=GridStructure\n",
      "END_GROUP=GridStructure\n",
      "GROUP=PointStructure\n",
      "END_GROUP=PointStructure\n",
      "END\n",
      "\n",
      "    Number_of_Instrument_Scans: 2040\n",
      "    Maximum_Number_of_1km_Frames: 1354\n",
      "    history: $Id: MOD06_L2.CDL.fs,v 1.13 2013/06/19 15:38:46 wind Exp $                          \n",
      "\n",
      "    title: MODIS Level 2 Cloud Properties                                                      \n",
      "\n",
      "    CoreMetadata.0: \n",
      "GROUP                  = INVENTORYMETADATA\n",
      "  GROUPTYPE            = MASTERGROUP\n",
      "\n",
      "  GROUP                  = ECSDATAGRANULE\n",
      "\n",
      "    OBJECT                 = REPROCESSINGPLANNED\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"further update is anticipated\"\n",
      "    END_OBJECT             = REPROCESSINGPLANNED\n",
      "\n",
      "    OBJECT                 = REPROCESSINGACTUAL\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"processed once\"\n",
      "    END_OBJECT             = REPROCESSINGACTUAL\n",
      "\n",
      "    OBJECT                 = LOCALGRANULEID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MYD06_L2.A2002185.0000.061.2018003215042.hdf\"\n",
      "    END_OBJECT             = LOCALGRANULEID\n",
      "\n",
      "    OBJECT                 = DAYNIGHTFLAG\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"Night\"\n",
      "    END_OBJECT             = DAYNIGHTFLAG\n",
      "\n",
      "    OBJECT                 = PRODUCTIONDATETIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2018-01-03T21:50:42.000Z\"\n",
      "    END_OBJECT             = PRODUCTIONDATETIME\n",
      "\n",
      "    OBJECT                 = LOCALVERSIONID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"061\"\n",
      "    END_OBJECT             = LOCALVERSIONID\n",
      "\n",
      "  END_GROUP              = ECSDATAGRANULE\n",
      "\n",
      "  GROUP                  = MEASUREDPARAMETER\n",
      "\n",
      "    OBJECT                 = MEASUREDPARAMETERCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = PARAMETERNAME\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"Cloud_Top_Pressure\"\n",
      "      END_OBJECT             = PARAMETERNAME\n",
      "\n",
      "      GROUP                  = QAFLAGS\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = AUTOMATICQUALITYFLAG\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"Passed\"\n",
      "        END_OBJECT             = AUTOMATICQUALITYFLAG\n",
      "\n",
      "        OBJECT                 = AUTOMATICQUALITYFLAGEXPLANATION\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"Passed: >10% useable; Failed: <10% useable\"\n",
      "        END_OBJECT             = AUTOMATICQUALITYFLAGEXPLANATION\n",
      "\n",
      "        OBJECT                 = SCIENCEQUALITYFLAG\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = \"Not Investigated\"\n",
      "          CLASS                = \"1\"\n",
      "        END_OBJECT             = SCIENCEQUALITYFLAG\n",
      "\n",
      "        OBJECT                 = SCIENCEQUALITYFLAGEXPLANATION\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = \"See http://modis-atmos.gsfc.nasa.gov/validation.html for more details on MODIS Atmosphere data quality.\"\n",
      "          CLASS                = \"1\"\n",
      "        END_OBJECT             = SCIENCEQUALITYFLAGEXPLANATION\n",
      "\n",
      "      END_GROUP              = QAFLAGS\n",
      "\n",
      "      GROUP                  = QASTATS\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = QAPERCENTMISSINGDATA\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = 63\n",
      "        END_OBJECT             = QAPERCENTMISSINGDATA\n",
      "\n",
      "      END_GROUP              = QASTATS\n",
      "\n",
      "    END_OBJECT             = MEASUREDPARAMETERCONTAINER\n",
      "\n",
      "  END_GROUP              = MEASUREDPARAMETER\n",
      "\n",
      "  GROUP                  = ORBITCALCULATEDSPATIALDOMAIN\n",
      "\n",
      "    OBJECT                 = ORBITCALCULATEDSPATIALDOMAINCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ORBITNUMBER\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = 885\n",
      "      END_OBJECT             = ORBITNUMBER\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGLONGITUDE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = 20.4199118302919\n",
      "      END_OBJECT             = EQUATORCROSSINGLONGITUDE\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGTIME\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"00:12:58.823356\"\n",
      "      END_OBJECT             = EQUATORCROSSINGTIME\n",
      "\n",
      "      OBJECT                 = EQUATORCROSSINGDATE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"2002-07-04\"\n",
      "      END_OBJECT             = EQUATORCROSSINGDATE\n",
      "\n",
      "    END_OBJECT             = ORBITCALCULATEDSPATIALDOMAINCONTAINER\n",
      "\n",
      "  END_GROUP              = ORBITCALCULATEDSPATIALDOMAIN\n",
      "\n",
      "  GROUP                  = COLLECTIONDESCRIPTIONCLASS\n",
      "\n",
      "    OBJECT                 = SHORTNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MYD06_L2\"\n",
      "    END_OBJECT             = SHORTNAME\n",
      "\n",
      "    OBJECT                 = VERSIONID\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = 61\n",
      "    END_OBJECT             = VERSIONID\n",
      "\n",
      "  END_GROUP              = COLLECTIONDESCRIPTIONCLASS\n",
      "\n",
      "  GROUP                  = INPUTGRANULE\n",
      "\n",
      "    OBJECT                 = INPUTPOINTER\n",
      "      NUM_VAL              = 50\n",
      "      VALUE                = (\"MYD021KM.A2002185.0000.061.2017363141721.hdf\", \"MYD03.A2002185.0000.061.2017362174430.hdf\", \"MYD35_L2.A2002185.0000.061.2017363141846.hdf\", \"gdas1.PGrbF00.020704.00z\", \"oisst.20020703\", \"goge1_2_img.v1\", \"modisdet.dry.101.lit_end.v3\", \"\n",
      "          modisdet.ozo.101.lit_end.v3\", \"modisdet.wts.101.lit_end.v3\", \"modisdet.wtl.101.lit_end.v3\", \"modisdet.wco.101.lit_end.v3\", \"MYDCSR_B.A2002185.061.2017363154239.hdf\", \"gdas1.PGrbF00.020704.06z\", \"eng.020704\", \"NISE_SSMIF13_20020704.HDFEOS\", \"MODIS_Ice_library.hdf.v5\", \"\n",
      "          MODIS_Water_library.hdf.v4\", \"MODIS_Ice_library_ws3.hdf.v3\", \"MODIS_Ice_library_ws3sd.hdf.v4\", \"MODIS_Ice_library_ws7.hdf.v3\", \"MODIS_Ice_library_ws7sd.hdf.v4\", \"MODIS_Ice_library_ws15.hdf.v3\", \"MODIS_Ice_library_ws15sd.hdf.v4\", \"MODIS_Water_library_ws3.hdf.v2\", \"\n",
      "          MODIS_Water_library_ws3sd.hdf.v3\", \"MODIS_Water_library_ws7.hdf.v2\", \"MODIS_Water_library_ws7sd.hdf.v3\", \"MODIS_Water_library_ws15.hdf.v2\", \"MODIS_Water_library_ws15sd.hdf.v3\", \"MODIS_Ice_WaterPhaseFunc.hdf.v3\", \"Transmittance.hdf.v2\", \"IGBP.EcoMap.NtoS.2004.149.v004.hdf\", \"\n",
      "          AlbSnwStst.ByNISE.W90.D90.WS.Hemi.2000-2004.YrAvg.hdf\", \"MCD43GF_wsa_Band1_185_2002_V006.hdf\", \"MCD43GF_wsa_Band2_185_2002_V006.hdf\", \"MCD43GF_wsa_Band5_185_2002_V006.hdf\", \"MCD43GF_wsa_Band6_185_2002_V006.hdf\", \"MCD43GF_wsa_Band7_185_2002_V006.hdf\")\n",
      "    END_OBJECT             = INPUTPOINTER\n",
      "\n",
      "  END_GROUP              = INPUTGRANULE\n",
      "\n",
      "  GROUP                  = SPATIALDOMAINCONTAINER\n",
      "\n",
      "    GROUP                  = HORIZONTALSPATIALDOMAINCONTAINER\n",
      "\n",
      "      GROUP                  = BOUNDINGRECTANGLE\n",
      "\n",
      "        OBJECT                 = WESTBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 15.0125121267976\n",
      "        END_OBJECT             = WESTBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = NORTHBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 47.4712121222453\n",
      "        END_OBJECT             = NORTHBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = EASTBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 46.3027701461145\n",
      "        END_OBJECT             = EASTBOUNDINGCOORDINATE\n",
      "\n",
      "        OBJECT                 = SOUTHBOUNDINGCOORDINATE\n",
      "          NUM_VAL              = 1\n",
      "          VALUE                = 26.6036189855835\n",
      "        END_OBJECT             = SOUTHBOUNDINGCOORDINATE\n",
      "\n",
      "      END_GROUP              = BOUNDINGRECTANGLE\n",
      "\n",
      "    END_GROUP              = HORIZONTALSPATIALDOMAINCONTAINER\n",
      "\n",
      "  END_GROUP              = SPATIALDOMAINCONTAINER\n",
      "\n",
      "  GROUP                  = RANGEDATETIME\n",
      "\n",
      "    OBJECT                 = RANGEBEGINNINGDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2002-07-04\"\n",
      "    END_OBJECT             = RANGEBEGINNINGDATE\n",
      "\n",
      "    OBJECT                 = RANGEBEGINNINGTIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"00:00:00.000000\"\n",
      "    END_OBJECT             = RANGEBEGINNINGTIME\n",
      "\n",
      "    OBJECT                 = RANGEENDINGDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2002-07-04\"\n",
      "    END_OBJECT             = RANGEENDINGDATE\n",
      "\n",
      "    OBJECT                 = RANGEENDINGTIME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"00:05:00.000000\"\n",
      "    END_OBJECT             = RANGEENDINGTIME\n",
      "\n",
      "  END_GROUP              = RANGEDATETIME\n",
      "\n",
      "  GROUP                  = PGEVERSIONCLASS\n",
      "\n",
      "    OBJECT                 = PGEVERSION\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"6.1.4\"\n",
      "    END_OBJECT             = PGEVERSION\n",
      "\n",
      "  END_GROUP              = PGEVERSIONCLASS\n",
      "\n",
      "  GROUP                  = ANCILLARYINPUTGRANULE\n",
      "\n",
      "    OBJECT                 = ANCILLARYINPUTGRANULECONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ANCILLARYINPUTTYPE\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"Geolocation\"\n",
      "      END_OBJECT             = ANCILLARYINPUTTYPE\n",
      "\n",
      "      OBJECT                 = ANCILLARYINPUTPOINTER\n",
      "        NUM_VAL              = 1\n",
      "        CLASS                = \"1\"\n",
      "        VALUE                = \"MYD03.A2002185.0000.061.2017362174430.hdf\"\n",
      "      END_OBJECT             = ANCILLARYINPUTPOINTER\n",
      "\n",
      "    END_OBJECT             = ANCILLARYINPUTGRANULECONTAINER\n",
      "\n",
      "  END_GROUP              = ANCILLARYINPUTGRANULE\n",
      "\n",
      "  GROUP                  = ASSOCIATEDPLATFORMINSTRUMENTSENSOR\n",
      "\n",
      "    OBJECT                 = ASSOCIATEDPLATFORMINSTRUMENTSENSORCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDSENSORSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MODIS\"\n",
      "      END_OBJECT             = ASSOCIATEDSENSORSHORTNAME\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDPLATFORMSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"Aqua\"\n",
      "      END_OBJECT             = ASSOCIATEDPLATFORMSHORTNAME\n",
      "\n",
      "      OBJECT                 = ASSOCIATEDINSTRUMENTSHORTNAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MODIS\"\n",
      "      END_OBJECT             = ASSOCIATEDINSTRUMENTSHORTNAME\n",
      "\n",
      "    END_OBJECT             = ASSOCIATEDPLATFORMINSTRUMENTSENSORCONTAINER\n",
      "\n",
      "  END_GROUP              = ASSOCIATEDPLATFORMINSTRUMENTSENSOR\n",
      "\n",
      "  GROUP                  = ADDITIONALATTRIBUTES\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"1\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SuccessCloudTopPropRtrPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"   36.98\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"2\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"2\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SuccessCloudPhaseRtrPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"2\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"2\"\n",
      "          VALUE                = \"   36.66\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"4\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"4\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"LowCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"4\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"4\"\n",
      "          VALUE                = \"   32.12\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"5\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"5\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MidCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"5\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"5\"\n",
      "          VALUE                = \"    2.40\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"6\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"6\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"HighCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"6\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"6\"\n",
      "          VALUE                = \"    2.45\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"7\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"7\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"ThinCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"7\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"7\"\n",
      "          VALUE                = \"    9.74\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"8\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"8\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"ThickCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"8\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"8\"\n",
      "          VALUE                = \"   10.37\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"9\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"9\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"OpaqueCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"9\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"9\"\n",
      "          VALUE                = \"   16.86\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"10\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"10\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"CirrusCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"10\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"10\"\n",
      "          VALUE                = \"   20.12\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"11\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"11\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"IceCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"11\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"11\"\n",
      "          VALUE                = \"    1.27\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"12\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"12\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"WaterCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"12\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"12\"\n",
      "          VALUE                = \"   32.99\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"13\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"13\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"MixedCloudDetectedPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"13\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"13\"\n",
      "          VALUE                = \"    0.00\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"14\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"14\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"CloudPhaseUncertainPct_IR\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"14\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"14\"\n",
      "          VALUE                = \"    2.40\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"15\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"15\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"OceanCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"15\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"15\"\n",
      "          VALUE                = \"   45.11\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"16\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"16\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"LandCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"16\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"16\"\n",
      "          VALUE                = \"   54.89\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"17\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"17\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"SnowCoverFractionPct\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"17\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"17\"\n",
      "          VALUE                = \"    0.06\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"18\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"18\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"identifier_product_doi\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"18\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"18\"\n",
      "          VALUE                = \"10.5067/MODIS/MYD06_L2.061\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "    OBJECT                 = ADDITIONALATTRIBUTESCONTAINER\n",
      "      CLASS                = \"19\"\n",
      "\n",
      "      OBJECT                 = ADDITIONALATTRIBUTENAME\n",
      "        CLASS                = \"19\"\n",
      "        NUM_VAL              = 1\n",
      "        VALUE                = \"identifier_product_doi_authority\"\n",
      "      END_OBJECT             = ADDITIONALATTRIBUTENAME\n",
      "\n",
      "      GROUP                  = INFORMATIONCONTENT\n",
      "        CLASS                = \"19\"\n",
      "\n",
      "        OBJECT                 = PARAMETERVALUE\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"19\"\n",
      "          VALUE                = \"http://dx.doi.org\"\n",
      "        END_OBJECT             = PARAMETERVALUE\n",
      "\n",
      "      END_GROUP              = INFORMATIONCONTENT\n",
      "\n",
      "    END_OBJECT             = ADDITIONALATTRIBUTESCONTAINER\n",
      "\n",
      "  END_GROUP              = ADDITIONALATTRIBUTES\n",
      "\n",
      "END_GROUP              = INVENTORYMETADATA\n",
      "\n",
      "END\n",
      "\n",
      "    ArchiveMetadata.0: \n",
      "GROUP                  = ARCHIVEDMETADATA\n",
      "  GROUPTYPE            = MASTERGROUP\n",
      "\n",
      "  OBJECT                 = PROCESSINGENVIRONMENT\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"Linux minion7260 3.10.0-693.11.1.el7.x86_64 #1 SMP Mon Dec 4 23:52:40 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\"\n",
      "  END_OBJECT             = PROCESSINGENVIRONMENT\n",
      "\n",
      "  GROUP                  = ALGORITHMPACKAGE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEACCEPTANCEDATE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"June 1997\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEACCEPTANCEDATE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEMATURITYCODE\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"at-launch\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEMATURITYCODE\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGENAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"ATBD-MOD-04 and ATBD-MOD-05\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGENAME\n",
      "\n",
      "    OBJECT                 = ALGORITHMPACKAGEVERSION\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"2\"\n",
      "    END_OBJECT             = ALGORITHMPACKAGEVERSION\n",
      "\n",
      "    OBJECT                 = INSTRUMENTNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"Moderate Resolution Imaging Spectroradiometer\"\n",
      "    END_OBJECT             = INSTRUMENTNAME\n",
      "\n",
      "    OBJECT                 = LONGNAME\n",
      "      NUM_VAL              = 1\n",
      "      VALUE                = \"MODIS/Aqua Clouds 5-Min L2 Swath 1km and 5km\"\n",
      "    END_OBJECT             = LONGNAME\n",
      "\n",
      "    OBJECT                 = LOCALINPUTGRANULEID\n",
      "      NUM_VAL              = 20\n",
      "      VALUE                = (\"MYD021KM.A2002185.0000.061.2017363141721.hdf\", \"MYD03.A2002185.0000.061.2017362174430.hdf\", \"MYD35_L2.A2002185.0000.061.2017363141846.hdf\", \"MYDCSR_B.A2002185.061.2017363154239.hdf\")\n",
      "    END_OBJECT             = LOCALINPUTGRANULEID\n",
      "\n",
      "  END_GROUP              = ALGORITHMPACKAGE\n",
      "\n",
      "  GROUP                  = GPOLYGON\n",
      "\n",
      "    OBJECT                 = GPOLYGONCONTAINER\n",
      "      CLASS                = \"1\"\n",
      "\n",
      "      GROUP                  = GRING\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = EXCLUSIONGRINGFLAG\n",
      "          NUM_VAL              = 1\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = \"N\"\n",
      "        END_OBJECT             = EXCLUSIONGRINGFLAG\n",
      "\n",
      "      END_GROUP              = GRING\n",
      "\n",
      "      GROUP                  = GRINGPOINT\n",
      "        CLASS                = \"1\"\n",
      "\n",
      "        OBJECT                 = GRINGPOINTLONGITUDE\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (16.6802142122724, 46.3037511907536, 38.7590005022001, 14.9898525858698)\n",
      "        END_OBJECT             = GRINGPOINTLONGITUDE\n",
      "\n",
      "        OBJECT                 = GRINGPOINTLATITUDE\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (47.4897565211684, 43.2358532665263, 26.5236230538033, 29.7440219071349)\n",
      "        END_OBJECT             = GRINGPOINTLATITUDE\n",
      "\n",
      "        OBJECT                 = GRINGPOINTSEQUENCENO\n",
      "          NUM_VAL              = 4\n",
      "          CLASS                = \"1\"\n",
      "          VALUE                = (1, 2, 3, 4)\n",
      "        END_OBJECT             = GRINGPOINTSEQUENCENO\n",
      "\n",
      "      END_GROUP              = GRINGPOINT\n",
      "\n",
      "    END_OBJECT             = GPOLYGONCONTAINER\n",
      "\n",
      "  END_GROUP              = GPOLYGON\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_TOP_PROPERTY_IR\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_TOP_PROPERTY_IR\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_PHASE_IR\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_PHASE_IR\n",
      "\n",
      "  OBJECT                 = ALGORITHM_VERSION_CLOUD_PROPERTY_VIS\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"1\"\n",
      "  END_OBJECT             = ALGORITHM_VERSION_CLOUD_PROPERTY_VIS\n",
      "\n",
      "  OBJECT                 = DESCRREVISION\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"6.1\"\n",
      "  END_OBJECT             = DESCRREVISION\n",
      "\n",
      "  OBJECT                 = PRODUCTIONHISTORY\n",
      "    NUM_VAL              = 1\n",
      "    VALUE                = \"PGE06:6.1.4\"\n",
      "  END_OBJECT             = PRODUCTIONHISTORY\n",
      "\n",
      "END_GROUP              = ARCHIVEDMETADATA\n",
      "\n",
      "END\n",
      "\n",
      "    Clear_Sky_Restoral_Status: y\n",
      "    Collection_4_Phase_Used: n\n",
      "    Ice_Phase_Forced: n\n",
      "    Water_Phase_Forced: n\n",
      "    identifier_product_doi: 10.5067/MODIS/MYD06_L2.061\n",
      "    identifier_product_doi_authority: http://dx.doi.org\n",
      "    dimensions(sizes): Cell_Along_Swath_5km:mod06(408), Cell_Across_Swath_5km:mod06(270), Band_Number:mod06(7), Band_Forcing:mod06(5), Band_Ratio:mod06(5), Cell_Along_Swath_1km:mod06(2040), Cell_Across_Swath_1km:mod06(1354), Cloud_Mask_5km_Num_Bytes:mod06(2), QA_Parameter_5km:mod06(10), Cloud_Mask_1km_Num_Bytes:mod06(2), RadTran_NRE_Ice:mod06(12), RadTran_NWL:mod06(7), RadTran_NRE_Liq:mod06(18), SPI_nband:mod06(2), RFM_nband:mod06(3), ACR_nband:mod06(6), QA_Parameter_1km:mod06(9), fakeDim17(17)\n",
      "    variables(dimensions): >f4 \u001b[4mLatitude\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >f4 \u001b[4mLongitude\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >f8 \u001b[4mScan_Start_Time\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Zenith_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSolar_Azimuth_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Zenith_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSensor_Azimuth_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mBrightness_Temperature\u001b[0m(Band_Number:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSurface_Temperature\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSurface_Pressure\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Height_Method\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Height_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Temperature_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mTropopause_Height\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Fraction_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Effective_Emissivity_Nadir_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_Infrared\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mSpectral_Cloud_Forcing\u001b[0m(Band_Forcing:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mCloud_Top_Pressure_From_Ratios\u001b[0m(Band_Ratio:mod06,Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), >i2 \u001b[4mRadiance_Variance\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_Night\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_Day\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06), int8 \u001b[4mCloud_Phase_Infrared_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mIRP_CTH_Consistency_Flag_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mos_top_flag_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_pressure_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_height_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_top_temperature_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mcloud_emissivity_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mcloud_top_method_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4msurface_temperature_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss11_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss12_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss13_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mcloud_emiss85_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_1621_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_16_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_37_PCL\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Effective_Radius_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Optical_Thickness_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCloud_Water_Path_Uncertainty_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mAbove_Cloud_Water_Vapor_094\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mIRW_Low_Cloud_Temperature_From_COP\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Phase_Optical_Properties\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Multi_Layer_Flag\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), >i2 \u001b[4mCirrus_Reflectance\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCirrus_Reflectance_Flag\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06), int8 \u001b[4mCloud_Mask_5km\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06,Cloud_Mask_5km_Num_Bytes:mod06), int8 \u001b[4mQuality_Assurance_5km\u001b[0m(Cell_Along_Swath_5km:mod06,Cell_Across_Swath_5km:mod06,QA_Parameter_5km:mod06), int8 \u001b[4mCloud_Mask_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,Cloud_Mask_1km_Num_Bytes:mod06), >f4 \u001b[4mExtinction_Efficiency_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mAsymmetry_Parameter_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mSingle_Scatter_Albedo_Ice\u001b[0m(RadTran_NRE_Ice:mod06,RadTran_NWL:mod06), >f4 \u001b[4mExtinction_Efficiency_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >f4 \u001b[4mAsymmetry_Parameter_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >f4 \u001b[4mSingle_Scatter_Albedo_Liq\u001b[0m(RadTran_NRE_Liq:mod06,RadTran_NWL:mod06), >i2 \u001b[4mCloud_Mask_SPI\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,SPI_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_16\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_37\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mRetrieval_Failure_Metric_1621\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,RFM_nband:mod06), >i2 \u001b[4mAtm_Corr_Refl\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,ACR_nband:mod06), int8 \u001b[4mQuality_Assurance_1km\u001b[0m(Cell_Along_Swath_1km:mod06,Cell_Across_Swath_1km:mod06,QA_Parameter_1km:mod06), >f4 \u001b[4mStatistics_1km_sds\u001b[0m(fakeDim17)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(MOD06)\n",
    "data = MOD06\n",
    "print(\"---------------\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5qc7b2uNDt8",
    "outputId": "3d465c5b-ab69-4124-e666-6056448d0f42"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-af3aea3ae2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, coords, dims, name, attrs, encoding, fastpath)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dims'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dims'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attrs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__getattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "foo = xr.DataArray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyi6FKnwNDt_"
   },
   "outputs": [],
   "source": [
    "cm = data.variables['Cloud_Mask_1km'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwWipViZNDuD"
   },
   "outputs": [],
   "source": [
    "MOD03 = Dataset('/Users/saviosebastian/Desktop/XArraysTest/MYD03.A2002185.0000.061.2017362174430.hdf', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 264,
     "status": "error",
     "timestamp": 1537890483814,
     "user": {
      "displayName": "Savio Sebastian",
      "photoUrl": "",
      "userId": "11901389089153341952"
     },
     "user_tz": 240
    },
    "id": "BFP5YmuTNDuE",
    "outputId": "5c0cb36b-c9e5-48c4-86cb-f8cb0ba60620"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-146d27312179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMOD03\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MOD03' is not defined"
     ]
    }
   ],
   "source": [
    "lat = MOD03.variables['Latitude'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUHFJMEsNDuI",
    "outputId": "5e3fb39d-a8c3-4da8-9716-7c3d87c134d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        ...,\n",
       "        [30.023902893066406, 30.0213565826416, 30.018844604492188, ...,\n",
       "         26.660354614257812, 26.649608612060547, 26.63867950439453],\n",
       "        [30.005863189697266, 30.003368377685547, 30.000919342041016, ...,\n",
       "         26.643115997314453, 26.632293701171875, 26.621292114257812],\n",
       "        [29.987855911254883, 29.985397338867188, 29.982986450195312, ...,\n",
       "         26.62582778930664, 26.614887237548828, 26.603618621826172]],\n",
       "  mask=[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "  fill_value=-999.0,\n",
       "  dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgUyTSbdNDuO"
   },
   "outputs": [],
   "source": [
    "long = MOD03.variables['Longitude'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jD4j9ZUhNDuQ",
    "outputId": "36445072-e292-43eb-deed-05aaf654ce55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        ...,\n",
       "        [15.015028953552246, 15.064740180969238, 15.11345100402832, ...,\n",
       "         38.32621383666992, 38.37139129638672, 38.41727066040039],\n",
       "        [15.014087677001953, 15.063920021057129, 15.112410545349121, ...,\n",
       "         38.32081985473633, 38.366085052490234, 38.41202926635742],\n",
       "        [15.01251220703125, 15.062713623046875, 15.111570358276367, ...,\n",
       "         38.31563949584961, 38.36115646362305, 38.4079704284668]],\n",
       "  mask=[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "  fill_value=-999.0,\n",
       "  dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 263,
     "status": "error",
     "timestamp": 1537890477544,
     "user": {
      "displayName": "Savio Sebastian",
      "photoUrl": "",
      "userId": "11901389089153341952"
     },
     "user_tz": 240
    },
    "id": "y33zPiCkNDuV",
    "outputId": "9735839d-1107-4833-f699-b0a377116931"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e45eb6b6b1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lat' is not defined"
     ]
    }
   ],
   "source": [
    "Lat=np.ravel(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPSomJ6ZamRs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy' from '/Users/saviosebastian/anaconda3/lib/python3.6/site-packages/numpy/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
       "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
       "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
       "       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
       "       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
       "       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
       "       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
       "       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
       "       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
       "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
       "       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n",
       "       832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844,\n",
       "       845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857,\n",
       "       858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870,\n",
       "       871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883,\n",
       "       884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n",
       "       897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n",
       "       910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922,\n",
       "       923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935,\n",
       "       936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948,\n",
       "       949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961,\n",
       "       962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n",
       "       975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987,\n",
       "       988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function arange in module numpy.core.multiarray:\n",
      "\n",
      "arange(...)\n",
      "    arange([start,] stop[, step,], dtype=None)\n",
      "    \n",
      "    Return evenly spaced values within a given interval.\n",
      "    \n",
      "    Values are generated within the half-open interval ``[start, stop)``\n",
      "    (in other words, the interval including `start` but excluding `stop`).\n",
      "    For integer arguments the function is equivalent to the Python built-in\n",
      "    `range <http://docs.python.org/lib/built-in-funcs.html>`_ function,\n",
      "    but returns an ndarray rather than a list.\n",
      "    \n",
      "    When using a non-integer step, such as 0.1, the results will often not\n",
      "    be consistent.  It is better to use ``linspace`` for these cases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : number, optional\n",
      "        Start of interval.  The interval includes this value.  The default\n",
      "        start value is 0.\n",
      "    stop : number\n",
      "        End of interval.  The interval does not include this value, except\n",
      "        in some cases where `step` is not an integer and floating point\n",
      "        round-off affects the length of `out`.\n",
      "    step : number, optional\n",
      "        Spacing between values.  For any output `out`, this is the distance\n",
      "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
      "        step size is 1.  If `step` is specified as a position argument,\n",
      "        `start` must also be given.\n",
      "    dtype : dtype\n",
      "        The type of the output array.  If `dtype` is not given, infer the data\n",
      "        type from the other input arguments.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    arange : ndarray\n",
      "        Array of evenly spaced values.\n",
      "    \n",
      "        For floating point arguments, the length of the result is\n",
      "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
      "        this rule may result in the last element of `out` being greater\n",
      "        than `stop`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    linspace : Evenly spaced numbers with careful handling of endpoints.\n",
      "    ogrid: Arrays of evenly spaced numbers in N-dimensions.\n",
      "    mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.arange(3)\n",
      "    array([0, 1, 2])\n",
      "    >>> np.arange(3.0)\n",
      "    array([ 0.,  1.,  2.])\n",
      "    >>> np.arange(3,7)\n",
      "    array([3, 4, 5, 6])\n",
      "    >>> np.arange(3,7,2)\n",
      "    array([3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.arange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module numpy.core.fromnumeric:\n",
      "\n",
      "reshape(a, newshape, order='C')\n",
      "    Gives a new shape to an array without changing its data.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array to be reshaped.\n",
      "    newshape : int or tuple of ints\n",
      "        The new shape should be compatible with the original shape. If\n",
      "        an integer, then the result will be a 1-D array of that length.\n",
      "        One shape dimension can be -1. In this case, the value is\n",
      "        inferred from the length of the array and remaining dimensions.\n",
      "    order : {'C', 'F', 'A'}, optional\n",
      "        Read the elements of `a` using this index order, and place the\n",
      "        elements into the reshaped array using this index order.  'C'\n",
      "        means to read / write the elements using C-like index order,\n",
      "        with the last axis index changing fastest, back to the first\n",
      "        axis index changing slowest. 'F' means to read / write the\n",
      "        elements using Fortran-like index order, with the first index\n",
      "        changing fastest, and the last index changing slowest. Note that\n",
      "        the 'C' and 'F' options take no account of the memory layout of\n",
      "        the underlying array, and only refer to the order of indexing.\n",
      "        'A' means to read / write the elements in Fortran-like index\n",
      "        order if `a` is Fortran *contiguous* in memory, C-like order\n",
      "        otherwise.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    reshaped_array : ndarray\n",
      "        This will be a new view object if possible; otherwise, it will\n",
      "        be a copy.  Note there is no guarantee of the *memory layout* (C- or\n",
      "        Fortran- contiguous) of the returned array.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.reshape : Equivalent method.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    It is not always possible to change the shape of an array without\n",
      "    copying the data. If you want an error to be raised when the data is copied,\n",
      "    you should assign the new shape to the shape attribute of the array::\n",
      "    \n",
      "     >>> a = np.zeros((10, 2))\n",
      "     # A transpose makes the array non-contiguous\n",
      "     >>> b = a.T\n",
      "     # Taking a view makes it possible to modify the shape without modifying\n",
      "     # the initial object.\n",
      "     >>> c = b.view()\n",
      "     >>> c.shape = (20)\n",
      "     AttributeError: incompatible shape for a non-contiguous array\n",
      "    \n",
      "    The `order` keyword gives the index ordering both for *fetching* the values\n",
      "    from `a`, and then *placing* the values into the output array.\n",
      "    For example, let's say you have an array:\n",
      "    \n",
      "    >>> a = np.arange(6).reshape((3, 2))\n",
      "    >>> a\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5]])\n",
      "    \n",
      "    You can think of reshaping as first raveling the array (using the given\n",
      "    index order), then inserting the elements from the raveled array into the\n",
      "    new array using the same kind of index ordering as was used for the\n",
      "    raveling.\n",
      "    \n",
      "    >>> np.reshape(a, (2, 3)) # C-like index ordering\n",
      "    array([[0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "    >>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape\n",
      "    array([[0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "    >>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering\n",
      "    array([[0, 4, 3],\n",
      "           [2, 1, 5]])\n",
      "    >>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')\n",
      "    array([[0, 4, 3],\n",
      "           [2, 1, 5]])\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1,2,3], [4,5,6]])\n",
      "    >>> np.reshape(a, 6)\n",
      "    array([1, 2, 3, 4, 5, 6])\n",
      "    >>> np.reshape(a, 6, order='F')\n",
      "    array([1, 4, 2, 5, 3, 6])\n",
      "    \n",
      "    >>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\n",
      "    array([[1, 2],\n",
      "           [3, 4],\n",
      "           [5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape((25,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.arange(4*3*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'xarray' has no attribute 'to_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c9220c519182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'xarray' has no attribute 'to_dataframe'"
     ]
    }
   ],
   "source": [
    "d = xr.to_dataframe(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataArray in module xarray.core.dataarray:\n",
      "\n",
      "class DataArray(xarray.core.common.AbstractArray, xarray.core.common.DataWithCoords)\n",
      " |  N-dimensional array with labeled coordinates and dimensions.\n",
      " |  \n",
      " |  DataArray provides a wrapper around numpy ndarrays that uses labeled\n",
      " |  dimensions and coordinates to support metadata aware operations. The API is\n",
      " |  similar to that for the pandas Series or DataFrame, but DataArray objects\n",
      " |  can have any number of dimensions, and their contents have fixed data\n",
      " |  types.\n",
      " |  \n",
      " |  Additional features over raw numpy arrays:\n",
      " |  \n",
      " |  - Apply operations over dimensions by name: ``x.sum('time')``.\n",
      " |  - Select or assign values by integer location (like numpy): ``x[:10]``\n",
      " |    or by label (like pandas): ``x.loc['2014-01-01']`` or\n",
      " |    ``x.sel(time='2014-01-01')``.\n",
      " |  - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n",
      " |    dimensions (known in numpy as \"broadcasting\") based on dimension names,\n",
      " |    regardless of their original order.\n",
      " |  - Keep track of arbitrary metadata in the form of a Python dictionary:\n",
      " |    ``x.attrs``\n",
      " |  - Convert to a pandas Series: ``x.to_series()``.\n",
      " |  \n",
      " |  Getting items from or doing mathematical operations with a DataArray\n",
      " |  always returns another DataArray.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  dims : tuple\n",
      " |      Dimension names associated with this array.\n",
      " |  values : np.ndarray\n",
      " |      Access or modify DataArray values as a numpy array.\n",
      " |  coords : dict-like\n",
      " |      Dictionary of DataArray objects that label values along each dimension.\n",
      " |  name : str or None\n",
      " |      Name of this array.\n",
      " |  attrs : OrderedDict\n",
      " |      Dictionary for holding arbitrary metadata.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataArray\n",
      " |      xarray.core.common.AbstractArray\n",
      " |      xarray.core.common.ImplementsArrayReduce\n",
      " |      xarray.core.formatting.ReprMixin\n",
      " |      xarray.core.common.DataWithCoords\n",
      " |      xarray.core.arithmetic.SupportsArithmetic\n",
      " |      xarray.core.common.AttrAccessMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(...)\n",
      " |      abs(a) -- Same as abs(a).\n",
      " |  \n",
      " |  __add__ = add(...)\n",
      " |      add(a, b) -- Same as a + b.\n",
      " |  \n",
      " |  __and__ = and_(...)\n",
      " |      and_(a, b) -- Same as a & b.\n",
      " |  \n",
      " |  __array_wrap__(self, obj, context=None)\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __dask_graph__(self)\n",
      " |  \n",
      " |  __dask_keys__(self)\n",
      " |  \n",
      " |  __dask_postcompute__(self)\n",
      " |  \n",
      " |  __dask_postpersist__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, memo=None)\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |  \n",
      " |  __eq__ = array_eq(self, other)\n",
      " |  \n",
      " |  __floordiv__ = floordiv(...)\n",
      " |      floordiv(a, b) -- Same as a // b.\n",
      " |  \n",
      " |  __ge__ = ge(...)\n",
      " |      ge(a, b) -- Same as a>=b.\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __gt__ = gt(...)\n",
      " |      gt(a, b) -- Same as a>b.\n",
      " |  \n",
      " |  __iadd__ = iadd(...)\n",
      " |      a = iadd(a, b) -- Same as a += b.\n",
      " |  \n",
      " |  __iand__ = iand(...)\n",
      " |      a = iand(a, b) -- Same as a &= b.\n",
      " |  \n",
      " |  __ifloordiv__ = ifloordiv(...)\n",
      " |      a = ifloordiv(a, b) -- Same as a //= b.\n",
      " |  \n",
      " |  __imod__ = imod(...)\n",
      " |      a = imod(a, b) -- Same as a %= b.\n",
      " |  \n",
      " |  __imul__ = imul(...)\n",
      " |      a = imul(a, b) -- Same as a *= b.\n",
      " |  \n",
      " |  __init__(self, data, coords=None, dims=None, name=None, attrs=None, encoding=None, fastpath=False)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : array_like\n",
      " |          Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n",
      " |          or castable to an ``ndarray``. If a self-described xarray or pandas\n",
      " |          object, attempts are made to use this array's metadata to fill in\n",
      " |          other unspecified arguments. A view of the array's data is used\n",
      " |          instead of a copy if possible.\n",
      " |      coords : sequence or dict of array_like objects, optional\n",
      " |          Coordinates (tick labels) to use for indexing along each dimension.\n",
      " |          If dict-like, should be a mapping from dimension names to the\n",
      " |          corresponding coordinates. If sequence-like, should be a sequence\n",
      " |          of tuples where the first element is the dimension name and the\n",
      " |          second element is the corresponding coordinate array_like object.\n",
      " |      dims : str or sequence of str, optional\n",
      " |          Name(s) of the data dimension(s). Must be either a string (only\n",
      " |          for 1D data) or a sequence of strings with length equal to the\n",
      " |          number of dimensions. If this argument is omitted, dimension names\n",
      " |          are taken from ``coords`` (if possible) and otherwise default to\n",
      " |          ``['dim_0', ... 'dim_n']``.\n",
      " |      name : str or None, optional\n",
      " |          Name of this array.\n",
      " |      attrs : dict_like or None, optional\n",
      " |          Attributes to assign to the new instance. By default, an empty\n",
      " |          attribute dictionary is initialized.\n",
      " |      encoding : dict_like or None, optional\n",
      " |          Dictionary specifying how to encode this array's data into a\n",
      " |          serialized format like netCDF4. Currently used keys (for netCDF)\n",
      " |          include '_FillValue', 'scale_factor', 'add_offset', 'dtype',\n",
      " |          'units' and 'calendar' (the later two only for datetime arrays).\n",
      " |          Unrecognized keys are ignored.\n",
      " |  \n",
      " |  __invert__ = invert(...)\n",
      " |      invert(a) -- Same as ~a.\n",
      " |  \n",
      " |  __ior__ = ior(...)\n",
      " |      a = ior(a, b) -- Same as a |= b.\n",
      " |  \n",
      " |  __ipow__ = ipow(...)\n",
      " |      a = ipow(a, b) -- Same as a **= b.\n",
      " |  \n",
      " |  __isub__ = isub(...)\n",
      " |      a = isub(a, b) -- Same as a -= b.\n",
      " |  \n",
      " |  __itruediv__ = itruediv(...)\n",
      " |      a = itruediv(a, b) -- Same as a /= b\n",
      " |  \n",
      " |  __ixor__ = ixor(...)\n",
      " |      a = ixor(a, b) -- Same as a ^= b.\n",
      " |  \n",
      " |  __le__ = le(...)\n",
      " |      le(a, b) -- Same as a<=b.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__ = lt(...)\n",
      " |      lt(a, b) -- Same as a<b.\n",
      " |  \n",
      " |  __mod__ = mod(...)\n",
      " |      mod(a, b) -- Same as a % b.\n",
      " |  \n",
      " |  __mul__ = mul(...)\n",
      " |      mul(a, b) -- Same as a * b.\n",
      " |  \n",
      " |  __ne__ = array_ne(self, other)\n",
      " |  \n",
      " |  __neg__ = neg(...)\n",
      " |      neg(a) -- Same as -a.\n",
      " |  \n",
      " |  __or__ = or_(...)\n",
      " |      or_(a, b) -- Same as a | b.\n",
      " |  \n",
      " |  __pos__ = pos(...)\n",
      " |      pos(a) -- Same as +a.\n",
      " |  \n",
      " |  __pow__ = pow(...)\n",
      " |      pow(a, b) -- Same as a ** b.\n",
      " |  \n",
      " |  __radd__ = add(...)\n",
      " |      add(a, b) -- Same as a + b.\n",
      " |  \n",
      " |  __rand__ = and_(...)\n",
      " |      and_(a, b) -- Same as a & b.\n",
      " |  \n",
      " |  __rfloordiv__ = floordiv(...)\n",
      " |      floordiv(a, b) -- Same as a // b.\n",
      " |  \n",
      " |  __rmod__ = mod(...)\n",
      " |      mod(a, b) -- Same as a % b.\n",
      " |  \n",
      " |  __rmul__ = mul(...)\n",
      " |      mul(a, b) -- Same as a * b.\n",
      " |  \n",
      " |  __ror__ = or_(...)\n",
      " |      or_(a, b) -- Same as a | b.\n",
      " |  \n",
      " |  __rpow__ = pow(...)\n",
      " |      pow(a, b) -- Same as a ** b.\n",
      " |  \n",
      " |  __rsub__ = sub(...)\n",
      " |      sub(a, b) -- Same as a - b.\n",
      " |  \n",
      " |  __rtruediv__ = truediv(...)\n",
      " |      truediv(a, b) -- Same as a / b.\n",
      " |  \n",
      " |  __rxor__ = xor(...)\n",
      " |      xor(a, b) -- Same as a ^ b.\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  __sub__ = sub(...)\n",
      " |      sub(a, b) -- Same as a - b.\n",
      " |  \n",
      " |  __truediv__ = truediv(...)\n",
      " |      truediv(a, b) -- Same as a / b.\n",
      " |  \n",
      " |  __xor__ = xor(...)\n",
      " |      xor(a, b) -- Same as a ^ b.\n",
      " |  \n",
      " |  all(self, dim=None, axis=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `all` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `all`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `all`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `all` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `all` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `all` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  any(self, dim=None, axis=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `any` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `any`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `any`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `any` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `any` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `any` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  argmax(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `argmax` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `argmax`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `argmax`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `argmax` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `argmax` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `argmax` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  argmin(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `argmin` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `argmin`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `argmin`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `argmin` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `argmin` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `argmin` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  argsort(self, *args, **kwargs)\n",
      " |      a.argsort(axis=-1, kind='quicksort', order=None)\n",
      " |      \n",
      " |      Returns the indices that would sort this array.\n",
      " |      \n",
      " |      Refer to `numpy.argsort` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argsort : equivalent function\n",
      " |  \n",
      " |  astype(self, *args, **kwargs)\n",
      " |      a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n",
      " |      \n",
      " |      Copy of the array, cast to a specified type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or dtype\n",
      " |          Typecode or data-type to which the array is cast.\n",
      " |      order : {'C', 'F', 'A', 'K'}, optional\n",
      " |          Controls the memory layout order of the result.\n",
      " |          'C' means C order, 'F' means Fortran order, 'A'\n",
      " |          means 'F' order if all the arrays are Fortran contiguous,\n",
      " |          'C' order otherwise, and 'K' means as close to the\n",
      " |          order the array elements appear in memory as possible.\n",
      " |          Default is 'K'.\n",
      " |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      " |          Controls what kind of data casting may occur. Defaults to 'unsafe'\n",
      " |          for backwards compatibility.\n",
      " |      \n",
      " |            * 'no' means the data types should not be cast at all.\n",
      " |            * 'equiv' means only byte-order changes are allowed.\n",
      " |            * 'safe' means only casts which can preserve values are allowed.\n",
      " |            * 'same_kind' means only safe casts or casts within a kind,\n",
      " |              like float64 to float32, are allowed.\n",
      " |            * 'unsafe' means any data conversions may be done.\n",
      " |      subok : bool, optional\n",
      " |          If True, then sub-classes will be passed-through (default), otherwise\n",
      " |          the returned array will be forced to be a base-class array.\n",
      " |      copy : bool, optional\n",
      " |          By default, astype always returns a newly allocated array. If this\n",
      " |          is set to false, and the `dtype`, `order`, and `subok`\n",
      " |          requirements are satisfied, the input array is returned instead\n",
      " |          of a copy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      arr_t : ndarray\n",
      " |          Unless `copy` is False and the other conditions for returning the input\n",
      " |          array are satisfied (see description for `copy` input parameter), `arr_t`\n",
      " |          is a new array of the same shape as the input array, with dtype, order\n",
      " |          given by `dtype`, `order`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Starting in NumPy 1.9, astype method now returns an error if the string\n",
      " |      dtype to cast to is not long enough in 'safe' casting mode to hold the max\n",
      " |      value of integer/float array that is being casted. Previously the casting\n",
      " |      was allowed even if the result was truncated.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ComplexWarning\n",
      " |          When casting from complex to float or int. To avoid this,\n",
      " |          one should use ``a.real.astype(t)``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.array([1, 2, 2.5])\n",
      " |      >>> x\n",
      " |      array([ 1. ,  2. ,  2.5])\n",
      " |      \n",
      " |      >>> x.astype(int)\n",
      " |      array([1, 2, 2])\n",
      " |  \n",
      " |  bfill(self, dim, limit=None)\n",
      " |      Fill NaN values by propogating values backward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int, default None\n",
      " |          The maximum number of consecutive NaN values to backward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  broadcast_equals(self, other)\n",
      " |      Two DataArrays are broadcast equal if they are equal after\n",
      " |      broadcasting them against each other such that they have the same\n",
      " |      dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.equals\n",
      " |      DataArray.identical\n",
      " |  \n",
      " |  chunk(self, chunks=None, name_prefix='xarray-', token=None, lock=False)\n",
      " |      Coerce this array's data into a dask arrays with the given chunks.\n",
      " |      \n",
      " |      If this variable is a non-dask array, it will be converted to dask\n",
      " |      array. If it's a dask array, it will be rechunked to the given chunk\n",
      " |      sizes.\n",
      " |      \n",
      " |      If neither chunks is not provided for one or more dimensions, chunk\n",
      " |      sizes along that dimension will not be updated; non-dask arrays will be\n",
      " |      converted into dask arrays with a single block.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunks : int, tuple or dict, optional\n",
      " |          Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n",
      " |          ``{'x': 5, 'y': 5}``.\n",
      " |      name_prefix : str, optional\n",
      " |          Prefix for the name of the new dask array.\n",
      " |      token : str, optional\n",
      " |          Token uniquely identifying this array.\n",
      " |      lock : optional\n",
      " |          Passed on to :py:func:`dask.array.from_array`, if the array is not\n",
      " |          already as dask array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      chunked : xarray.DataArray\n",
      " |  \n",
      " |  clip(self, *args, **kwargs)\n",
      " |      a.clip(min=None, max=None, out=None)\n",
      " |      \n",
      " |      Return an array whose values are limited to ``[min, max]``.\n",
      " |      One of max or min must be given.\n",
      " |      \n",
      " |      Refer to `numpy.clip` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.clip : equivalent function\n",
      " |  \n",
      " |  combine_first(self, other)\n",
      " |      Combine two DataArray objects, with union of coordinates.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules of\n",
      " |      ``join='outer'``.  Default to non-null values of array calling the\n",
      " |      method.  Use np.nan to fill in vacant cells after alignment.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataArray\n",
      " |          Used to fill all matching missing values in this array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  compute(self, **kwargs)\n",
      " |      Manually trigger loading of this array's data from disk or a\n",
      " |      remote source into memory and return a new array. The original is\n",
      " |      left unaltered.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.array.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.compute\n",
      " |  \n",
      " |  conj(self, *args, **kwargs)\n",
      " |      a.conj()\n",
      " |      \n",
      " |      Complex-conjugate all elements.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  conjugate(self, *args, **kwargs)\n",
      " |      a.conjugate()\n",
      " |      \n",
      " |      Return the complex conjugate, element-wise.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  copy(self, deep=True, data=None)\n",
      " |      Returns a copy of this array.\n",
      " |      \n",
      " |      If `deep=True`, a deep copy is made of the data array.\n",
      " |      Otherwise, a shallow copy is made, so each variable in the new\n",
      " |      array's dataset is also a variable in this array's dataset.\n",
      " |      \n",
      " |      Use `data` to create a new object with the same structure as\n",
      " |      original but entirely new data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional\n",
      " |          Whether the data array and its coordinates are loaded into memory\n",
      " |          and copied onto the new object. Default is True.\n",
      " |      data : array_like, optional\n",
      " |          Data to use in the new object. Must have same shape as original.\n",
      " |          When `data` is used, `deep` is ignored for all data variables,\n",
      " |          and only used for coords.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : DataArray\n",
      " |          New object with dimensions, attributes, coordinates, name,\n",
      " |          encoding, and optionally data copied from original.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Shallow versus deep copy\n",
      " |      \n",
      " |      >>> array = xr.DataArray([1, 2, 3], dims='x',\n",
      " |      ...                      coords={'x': ['a', 'b', 'c']})\n",
      " |      >>> array.copy()\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([1, 2, 3])\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U1 'a' 'b' 'c'\n",
      " |      >>> array_0 = array.copy(deep=False)\n",
      " |      >>> array_0[0] = 7\n",
      " |      >>> array_0\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([7, 2, 3])\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U1 'a' 'b' 'c'\n",
      " |      >>> array\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([7, 2, 3])\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U1 'a' 'b' 'c'\n",
      " |      \n",
      " |      Changing the data using the ``data`` argument maintains the\n",
      " |      structure of the original object, but with the new data. Original\n",
      " |      object is unaffected.\n",
      " |      \n",
      " |      >>> array.copy(data=[0.1, 0.2, 0.3])\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ 0.1,  0.2,  0.3])\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U1 'a' 'b' 'c'\n",
      " |      >>> array\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([1, 2, 3])\n",
      " |      Coordinates:\n",
      " |      * x        (x) <U1 'a' 'b' 'c'\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      pandas.DataFrame.copy\n",
      " |  \n",
      " |  count(self, dim=None, axis=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `count` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `count`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `count`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `count` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `count` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `count` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  cumprod(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Apply `cumprod` along some dimension of DataArray.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension over which to apply `cumprod`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis over which to apply `cumprod`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumprod`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : DataArray\n",
      " |          New DataArray object with `cumprod` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  cumsum(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Apply `cumsum` along some dimension of DataArray.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension over which to apply `cumsum`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis over which to apply `cumsum`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumsum`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : DataArray\n",
      " |          New DataArray object with `cumsum` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  diff(self, dim, n=1, label='upper')\n",
      " |      Calculate the n-th order discrete difference along given axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          Dimension over which to calculate the finite difference.\n",
      " |      n : int, optional\n",
      " |          The number of times values are differenced.\n",
      " |      label : str, optional\n",
      " |          The new coordinate in dimension ``dim`` will have the\n",
      " |          values of either the minuend's or subtrahend's coordinate\n",
      " |          for values 'upper' and 'lower', respectively.  Other\n",
      " |          values are not supported.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      difference : same type as caller\n",
      " |          The n-th order finite difference of this object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ['x'])\n",
      " |      >>> arr.diff('x')\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([0, 1, 0])\n",
      " |      Coordinates:\n",
      " |      * x        (x) int64 2 3 4\n",
      " |      >>> arr.diff('x', 2)\n",
      " |      <xarray.DataArray (x: 2)>\n",
      " |      array([ 1, -1])\n",
      " |      Coordinates:\n",
      " |      * x        (x) int64 3 4\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.differentiate\n",
      " |  \n",
      " |  differentiate(self, coord, edge_order=1, datetime_unit=None)\n",
      " |      Differentiate the array with the second order accurate central\n",
      " |      differences.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This feature is limited to simple cartesian geometry, i.e. coord\n",
      " |          must be one dimensional.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coord: str\n",
      " |          The coordinate to be used to compute the gradient.\n",
      " |      edge_order: 1 or 2. Default 1\n",
      " |          N-th order accurate differences at the boundaries.\n",
      " |      datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n",
      " |          'us', 'ns', 'ps', 'fs', 'as'}\n",
      " |          Unit to compute gradient. Only valid for datetime coordinate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      differentiated: DataArray\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.gradient: corresponding numpy function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.arange(12).reshape(4, 3), dims=['x', 'y'],\n",
      " |      ...                   coords={'x': [0, 0.1, 1.1, 1.2]})\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (x: 4, y: 3)>\n",
      " |      array([[ 0,  1,  2],\n",
      " |             [ 3,  4,  5],\n",
      " |             [ 6,  7,  8],\n",
      " |             [ 9, 10, 11]])\n",
      " |      Coordinates:\n",
      " |        * x        (x) float64 0.0 0.1 1.1 1.2\n",
      " |      Dimensions without coordinates: y\n",
      " |      >>>\n",
      " |      >>> da.differentiate('x')\n",
      " |      <xarray.DataArray (x: 4, y: 3)>\n",
      " |      array([[30.      , 30.      , 30.      ],\n",
      " |             [27.545455, 27.545455, 27.545455],\n",
      " |             [27.545455, 27.545455, 27.545455],\n",
      " |             [30.      , 30.      , 30.      ]])\n",
      " |      Coordinates:\n",
      " |        * x        (x) float64 0.0 0.1 1.1 1.2\n",
      " |      Dimensions without coordinates: y\n",
      " |  \n",
      " |  dot(self, other, dims=None)\n",
      " |      Perform dot product of two DataArrays along their shared dims.\n",
      " |      \n",
      " |      Equivalent to taking taking tensordot over all shared dims.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : DataArray\n",
      " |          The other array with which the dot product is performed.\n",
      " |      dims: list of strings, optional\n",
      " |          Along which dimensions to be summed over. Default all the common\n",
      " |          dimensions are summed over.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : DataArray\n",
      " |          Array resulting from the dot product over all shared dimensions.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      dot\n",
      " |      numpy.tensordot\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n",
      " |      >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])\n",
      " |      >>> dm_vals = np.arange(4)\n",
      " |      >>> dm = DataArray(dm_vals, dims=['z'])\n",
      " |      \n",
      " |      >>> dm.dims\n",
      " |      ('z')\n",
      " |      >>> da.dims\n",
      " |      ('x', 'y', 'z')\n",
      " |      \n",
      " |      >>> dot_result = da.dot(dm)\n",
      " |      >>> dot_result.dims\n",
      " |      ('x', 'y')\n",
      " |  \n",
      " |  drop(self, labels, dim=None)\n",
      " |      Drop coordinates or index labels from this DataArray.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : scalar or list of scalars\n",
      " |          Name(s) of coordinate variables or index labels to drop.\n",
      " |      dim : str, optional\n",
      " |          Dimension along which to drop index labels. By default (if\n",
      " |          ``dim is None``), drops coordinates rather than index labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dropped : DataArray\n",
      " |  \n",
      " |  dropna(self, dim, how='any', thresh=None)\n",
      " |      Returns a new array with dropped labels for missing values along\n",
      " |      the provided dimension.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Dimension along which to drop missing values. Dropping along\n",
      " |          multiple dimensions simultaneously is not yet supported.\n",
      " |      how : {'any', 'all'}, optional\n",
      " |          * any : if any NA values are present, drop that label\n",
      " |          * all : if all values are NA, drop that label\n",
      " |      thresh : int, default None\n",
      " |          If supplied, require this many non-NA values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  equals(self, other)\n",
      " |      True if two DataArrays have the same dimensions, coordinates and\n",
      " |      values; otherwise False.\n",
      " |      \n",
      " |      DataArrays can still be equal (like pandas objects) if they have NaN\n",
      " |      values in the same locations.\n",
      " |      \n",
      " |      This method is necessary because `v1 == v2` for ``DataArray``\n",
      " |      does element-wise comparisons (like numpy.ndarrays).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.broadcast_equals\n",
      " |      DataArray.identical\n",
      " |  \n",
      " |  expand_dims(self, dim, axis=None)\n",
      " |      Return a new object with an additional axis (or axes) inserted at\n",
      " |      the corresponding position in the array shape.\n",
      " |      \n",
      " |      If dim is already a scalar coordinate, it will be promoted to a 1D\n",
      " |      coordinate consisting of a single value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str.\n",
      " |          Dimensions to include on the new variable.\n",
      " |          dimensions are inserted with length 1.\n",
      " |      axis : integer, list (or tuple) of integers, or None\n",
      " |          Axis position(s) where new axis is to be inserted (position(s) on\n",
      " |          the result array). If a list (or tuple) of integers is passed,\n",
      " |          multiple axes are inserted. In this case, dim arguments should be\n",
      " |          same length list. If axis=None is passed, all the axes will be\n",
      " |          inserted to the start of the result array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      expanded : same type as caller\n",
      " |          This object, but with an additional dimension(s).\n",
      " |  \n",
      " |  ffill(self, dim, limit=None)\n",
      " |      Fill NaN values by propogating values forward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int, default None\n",
      " |          The maximum number of consecutive NaN values to forward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  fillna(self, value)\n",
      " |      Fill missing values in this object.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic, except the result is aligned to this\n",
      " |      object (``join='left'``) instead of aligned to the intersection of\n",
      " |      index coordinates (``join='inner'``).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar, ndarray or DataArray\n",
      " |          Used to fill all matching missing values in this array. If the\n",
      " |          argument is a DataArray, it is first aligned with (reindexed to)\n",
      " |          this array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |  \n",
      " |  identical(self, other)\n",
      " |      Like equals, but also checks the array name and attributes, and\n",
      " |      attributes on all coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.broadcast_equals\n",
      " |      DataArray.equal\n",
      " |  \n",
      " |  interp(self, coords=None, method='linear', assume_sorted=False, kwargs={}, **coords_kwargs)\n",
      " |      Multidimensional interpolation of variables.\n",
      " |      \n",
      " |      coords : dict, optional\n",
      " |          Mapping from dimension names to the new coordinates.\n",
      " |          new coordinate can be an scalar, array-like or DataArray.\n",
      " |          If DataArrays are passed as new coordates, their dimensions are\n",
      " |          used for the broadcasting.\n",
      " |      method: {'linear', 'nearest'} for multidimensional array,\n",
      " |          {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n",
      " |          for 1-dimensional array.\n",
      " |      assume_sorted: boolean, optional\n",
      " |          If False, values of x can be in any order and they are sorted\n",
      " |          first. If True, x has to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs: dictionary\n",
      " |          Additional keyword passed to scipy's interpolator.\n",
      " |      **coords_kwarg : {dim: coordinate, ...}, optional\n",
      " |          The keyword arguments form of ``coords``.\n",
      " |          One of coords or coords_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated: xr.DataArray\n",
      " |          New dataarray on the new coordinates.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      scipy is required.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      scipy.interpolate.interp1d\n",
      " |      scipy.interpolate.interpn\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray([1, 3], [('x', np.arange(2))])\n",
      " |      >>> da.interp(x=0.5)\n",
      " |      <xarray.DataArray ()>\n",
      " |      array(2.0)\n",
      " |      Coordinates:\n",
      " |          x        float64 0.5\n",
      " |  \n",
      " |  interp_like(self, other, method='linear', assume_sorted=False, kwargs={})\n",
      " |      Interpolate this object onto the coordinates of another object,\n",
      " |      filling out of range values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to an 1d array-like, which provides coordinates upon\n",
      " |          which to index the variables in this dataset.\n",
      " |      method: string, optional.\n",
      " |          {'linear', 'nearest'} for multidimensional array,\n",
      " |          {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n",
      " |          for 1-dimensional array. 'linear' is used by default.\n",
      " |      assume_sorted: boolean, optional\n",
      " |          If False, values of coordinates that are interpolated over can be\n",
      " |          in any order and they are sorted first. If True, interpolated\n",
      " |          coordinates are assumed to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs: dictionary, optional\n",
      " |          Additional keyword passed to scipy's interpolator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated: xr.DataArray\n",
      " |          Another dataarray by interpolating this dataarray's data along the\n",
      " |          coordinates of the other object.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      scipy is required.\n",
      " |      If the dataarray has object-type coordinates, reindex is used for these\n",
      " |      coordinates instead of the interpolation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.interp\n",
      " |      DataArray.reindex_like\n",
      " |  \n",
      " |  interpolate_na(self, dim=None, method='linear', limit=None, use_coordinate=True, **kwargs)\n",
      " |      Interpolate values according to different methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Specifies the dimension along which to interpolate.\n",
      " |      method : {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
      " |                'polynomial', 'barycentric', 'krog', 'pchip',\n",
      " |                'spline', 'akima'}, optional\n",
      " |          String indicating which method to use for interpolation:\n",
      " |      \n",
      " |          - 'linear': linear interpolation (Default). Additional keyword\n",
      " |            arguments are passed to ``numpy.interp``\n",
      " |          - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
      " |            'polynomial': are passed to ``scipy.interpolate.interp1d``. If\n",
      " |            method=='polynomial', the ``order`` keyword argument must also be\n",
      " |            provided.\n",
      " |          - 'barycentric', 'krog', 'pchip', 'spline', and `akima`: use their\n",
      " |            respective``scipy.interpolate`` classes.\n",
      " |      use_coordinate : boolean or str, default True\n",
      " |          Specifies which index to use as the x values in the interpolation\n",
      " |          formulated as `y = f(x)`. If False, values are treated as if\n",
      " |          eqaully-spaced along `dim`. If True, the IndexVariable `dim` is\n",
      " |          used. If use_coordinate is a string, it specifies the name of a\n",
      " |          coordinate variariable to use as the index.\n",
      " |      limit : int, default None\n",
      " |          Maximum number of consecutive NaNs to fill. Must be greater than 0\n",
      " |          or None for no limit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.interp\n",
      " |      scipy.interpolate\n",
      " |  \n",
      " |  isel(self, indexers=None, drop=False, **indexers_kwargs)\n",
      " |      Return a new DataArray whose dataset is given by integer indexing\n",
      " |      along the specified dimension(s).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.isel\n",
      " |      DataArray.sel\n",
      " |  \n",
      " |  isel_points(self, dim='points', **indexers)\n",
      " |      Return a new DataArray whose dataset is given by pointwise integer\n",
      " |      indexing along the specified dimension(s).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.isel_points\n",
      " |  \n",
      " |  isnull(self, *args, **kwargs)\n",
      " |  \n",
      " |  item(self, *args, **kwargs)\n",
      " |      a.item(*args)\n",
      " |      \n",
      " |      Copy an element of an array to a standard Python scalar and return it.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \\*args : Arguments (variable number and type)\n",
      " |      \n",
      " |          * none: in this case, the method only works for arrays\n",
      " |            with one element (`a.size == 1`), which element is\n",
      " |            copied into a standard Python scalar object and returned.\n",
      " |      \n",
      " |          * int_type: this argument is interpreted as a flat index into\n",
      " |            the array, specifying which element to copy and return.\n",
      " |      \n",
      " |          * tuple of int_types: functions as does a single int_type argument,\n",
      " |            except that the argument is interpreted as an nd-index into the\n",
      " |            array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      z : Standard Python scalar object\n",
      " |          A copy of the specified element of the array as a suitable\n",
      " |          Python scalar\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When the data type of `a` is longdouble or clongdouble, item() returns\n",
      " |      a scalar array object because there is no available Python scalar that\n",
      " |      would not lose information. Void arrays return a buffer object for item(),\n",
      " |      unless fields are defined, in which case a tuple is returned.\n",
      " |      \n",
      " |      `item` is very similar to a[args], except, instead of an array scalar,\n",
      " |      a standard Python scalar is returned. This can be useful for speeding up\n",
      " |      access to elements of the array and doing arithmetic on elements of the\n",
      " |      array using Python's optimized math.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = np.random.randint(9, size=(3, 3))\n",
      " |      >>> x\n",
      " |      array([[3, 1, 7],\n",
      " |             [2, 8, 3],\n",
      " |             [8, 5, 3]])\n",
      " |      >>> x.item(3)\n",
      " |      2\n",
      " |      >>> x.item(7)\n",
      " |      5\n",
      " |      >>> x.item((0, 1))\n",
      " |      1\n",
      " |      >>> x.item((2, 2))\n",
      " |      3\n",
      " |  \n",
      " |  load(self, **kwargs)\n",
      " |      Manually trigger loading of this array's data from disk or a\n",
      " |      remote source into memory and return this array.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.array.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.compute\n",
      " |  \n",
      " |  max(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `max` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `max`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `max`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `max` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `max` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `max` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  mean(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `mean` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `mean`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `mean`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `mean` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `mean` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `mean` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  median(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `median` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `median`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `median`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `median` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `median` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `median` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  min(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `min` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `min`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `min`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `min` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `min` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `min` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  notnull(self, *args, **kwargs)\n",
      " |  \n",
      " |  persist(self, **kwargs)\n",
      " |      Trigger computation in constituent dask arrays\n",
      " |      \n",
      " |      This keeps them as dask arrays but encourages them to keep data in\n",
      " |      memory.  This is particularly useful when on a distributed machine.\n",
      " |      When on a single machine consider using ``.compute()`` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.persist``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.persist\n",
      " |  \n",
      " |  prod(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `prod` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `prod`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `prod`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `prod` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `prod` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `prod` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  quantile(self, q, dim=None, interpolation='linear', keep_attrs=False)\n",
      " |      Compute the qth quantile of the data along the specified dimension.\n",
      " |      \n",
      " |      Returns the qth quantiles(s) of the array elements.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : float in range of [0,1] (or sequence of floats)\n",
      " |          Quantile to compute, which must be between 0 and 1 inclusive.\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply quantile.\n",
      " |      interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n",
      " |          This optional parameter specifies the interpolation method to\n",
      " |          use when the desired quantile lies between two data points\n",
      " |          ``i < j``:\n",
      " |      \n",
      " |              * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n",
      " |                the fractional part of the index surrounded by ``i`` and\n",
      " |                ``j``.\n",
      " |              * lower: ``i``.\n",
      " |              * higher: ``j``.\n",
      " |              * nearest: ``i`` or ``j``, whichever is nearest.\n",
      " |              * midpoint: ``(i + j) / 2``.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      quantiles : DataArray\n",
      " |          If `q` is a single quantile, then the result\n",
      " |          is a scalar. If multiple percentiles are given, first axis of\n",
      " |          the result corresponds to the quantile and a quantile dimension\n",
      " |          is added to the return array. The other dimensions are the\n",
      " |           dimensions that remain after the reduction of the array.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile\n",
      " |  \n",
      " |  rank(self, dim, pct=False, keep_attrs=False)\n",
      " |      Ranks the data.\n",
      " |      \n",
      " |      Equal values are assigned a rank that is the average of the ranks that\n",
      " |      would have been otherwise assigned to all of the values within that\n",
      " |      set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n",
      " |      \n",
      " |      NaNs in the input array are returned as NaNs.\n",
      " |      \n",
      " |      The `bottleneck` library is required.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str\n",
      " |          Dimension over which to compute rank.\n",
      " |      pct : bool, optional\n",
      " |          If True, compute percentage ranks, otherwise compute integer ranks.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ranked : DataArray\n",
      " |          DataArray with the same coordinates and dtype 'float64'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> arr = xr.DataArray([5, 6, 7], dims='x')\n",
      " |      >>> arr.rank('x')\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ 1.,   2.,   3.])\n",
      " |      Dimensions without coordinates: x\n",
      " |  \n",
      " |  reduce(self, func, dim=None, axis=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this array by applying `func` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          Function which can be called in the form\n",
      " |          `f(x, axis=axis, **kwargs)` to return the result of reducing an\n",
      " |          np.ndarray over an integer valued axis.\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `func`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to repeatedly apply `func`. Only one of the\n",
      " |          'dim' and 'axis' arguments can be supplied. If neither are\n",
      " |          supplied, then the reduction is calculated over the flattened array\n",
      " |          (by calling `f(x)` without an axis argument).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the variable's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `func`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          DataArray with this object's array replaced with an array with\n",
      " |          summarized data and the indicated dimension(s) removed.\n",
      " |  \n",
      " |  reindex(self, indexers=None, method=None, tolerance=None, copy=True, **indexers_kwargs)\n",
      " |      Conform this object onto a new set of indexes, filling in\n",
      " |      missing values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          Dictionary with keys given by dimension names and values given by\n",
      " |          arrays of coordinates tick labels. Any mis-matched coordinate\n",
      " |          values will be filled in with NaN, and any mis-matched dimension\n",
      " |          names will simply be ignored.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      copy : bool, optional\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for filling index values in ``indexers`` not found on\n",
      " |          this data array:\n",
      " |      \n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value (requires pandas>=0.16)\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |      **indexers_kwarg : {dim: indexer, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : DataArray\n",
      " |          Another dataset array, with this array's data but replaced\n",
      " |          coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.reindex_like\n",
      " |      align\n",
      " |  \n",
      " |  reindex_like(self, other, method=None, tolerance=None, copy=True)\n",
      " |      Conform this object onto the indexes of another object, filling\n",
      " |      in missing values with NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to pandas.Index objects, which provides coordinates upon\n",
      " |          which to index the variables in this dataset. The indexes on this\n",
      " |          other object need not be the same as the indexes on this\n",
      " |          dataset. Any mis-matched index values will be filled in with\n",
      " |          NaN, and any mis-matched dimension names will simply be ignored.\n",
      " |      method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n",
      " |          Method to use for filling index values from other not found on this\n",
      " |          data array:\n",
      " |      \n",
      " |          * None (default): don't fill gaps\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value (requires pandas>=0.16)\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations most\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Requires pandas>=0.17.\n",
      " |      copy : bool, optional\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : DataArray\n",
      " |          Another dataset array, with this array's data but coordinates from\n",
      " |          the other object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.reindex\n",
      " |      align\n",
      " |  \n",
      " |  rename(self, new_name_or_name_dict=None, **names)\n",
      " |      Returns a new DataArray with renamed coordinates or a new name.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_name_or_name_dict : str or dict-like, optional\n",
      " |          If the argument is dict-like, it it used as a mapping from old\n",
      " |          names to new names for coordinates. Otherwise, use the argument\n",
      " |          as the new name for this array.\n",
      " |      **names, optional\n",
      " |          The keyword arguments form of a mapping from old names to\n",
      " |          new names for coordinates.\n",
      " |          One of new_name_or_name_dict or names must be provided.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : DataArray\n",
      " |          Renamed array or array with renamed coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.rename\n",
      " |      DataArray.swap_dims\n",
      " |  \n",
      " |  reorder_levels(self, dim_order=None, inplace=False, **dim_order_kwargs)\n",
      " |      Rearrange index levels using input order.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order : optional\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by lists representing new level orders. Every given dimension\n",
      " |          must have a multi-index.\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify the dataarray in-place. Otherwise, return a new\n",
      " |          DataArray object.\n",
      " |      **dim_order_kwargs: optional\n",
      " |          The keyword arguments form of ``dim_order``.\n",
      " |          One of dim_order or dim_order_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : DataArray\n",
      " |          Another dataarray, with this dataarray's data but replaced\n",
      " |          coordinates.\n",
      " |  \n",
      " |  reset_coords(self, names=None, drop=False, inplace=False)\n",
      " |      Given names of coordinates, reset them to become variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : str or list of str, optional\n",
      " |          Name(s) of non-index coordinates in this dataset to reset into\n",
      " |          variables. By default, all non-index coordinates are reset.\n",
      " |      drop : bool, optional\n",
      " |          If True, remove coordinates instead of converting them into\n",
      " |          variables.\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify this dataset inplace. Otherwise, create a new\n",
      " |          object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset, or DataArray if ``drop == True``\n",
      " |  \n",
      " |  reset_index(self, dims_or_levels, drop=False, inplace=False)\n",
      " |      Reset the specified index(es) or multi-index level(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_or_levels : str or list\n",
      " |          Name(s) of the dimension(s) and/or multi-index level(s) that will\n",
      " |          be reset.\n",
      " |      drop : bool, optional\n",
      " |          If True, remove the specified indexes and/or multi-index levels\n",
      " |          instead of extracting them as new coordinates (default: False).\n",
      " |      inplace : bool, optional\n",
      " |          If True, modify the dataarray in-place. Otherwise, return a new\n",
      " |          DataArray object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : DataArray\n",
      " |          Another dataarray, with this dataarray's data but replaced\n",
      " |          coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.set_index\n",
      " |  \n",
      " |  roll(self, shifts=None, roll_coords=None, **shifts_kwargs)\n",
      " |      Roll this array by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Unlike shift, roll may rotate all variables, including coordinates\n",
      " |      if specified. The direction of rotation is consistent with\n",
      " |      :py:func:`numpy.roll`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      roll_coords : bool\n",
      " |          Indicates whether to  roll the coordinates by the offset\n",
      " |          The current default of roll_coords (None, equivalent to True) is\n",
      " |          deprecated and will change to False in a future version.\n",
      " |          Explicitly pass roll_coords to silence the warning.\n",
      " |      **shifts : keyword arguments of the form {dim: offset}\n",
      " |          Integer offset to rotate each of the given dimensions. Positive\n",
      " |          offsets roll to the right; negative offsets roll to the left.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rolled : DataArray\n",
      " |          DataArray with the same attributes but rolled data and coordinates.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      shift\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> arr = xr.DataArray([5, 6, 7], dims='x')\n",
      " |      >>> arr.roll(x=1)\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([7, 5, 6])\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 2 0 1\n",
      " |  \n",
      " |  round(self, *args, **kwargs)\n",
      " |  \n",
      " |  searchsorted(self, *args, **kwargs)\n",
      " |      a.searchsorted(v, side='left', sorter=None)\n",
      " |      \n",
      " |      Find indices where elements of v should be inserted in a to maintain order.\n",
      " |      \n",
      " |      For full documentation, see `numpy.searchsorted`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.searchsorted : equivalent function\n",
      " |  \n",
      " |  sel(self, indexers=None, method=None, tolerance=None, drop=False, **indexers_kwargs)\n",
      " |      Return a new DataArray whose dataset is given by selecting\n",
      " |      index labels along the specified dimension(s).\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |        Do not try to assign values when using any of the indexing methods\n",
      " |        ``isel`` or ``sel``::\n",
      " |      \n",
      " |          da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n",
      " |          # DO NOT do this\n",
      " |          da.isel(x=[0, 1, 2])[1] = -1\n",
      " |      \n",
      " |        Assigning values with the chained indexing using ``.sel`` or\n",
      " |        ``.isel`` fails silently.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel\n",
      " |      DataArray.isel\n",
      " |  \n",
      " |  sel_points(self, dim='points', method=None, tolerance=None, **indexers)\n",
      " |      Return a new DataArray whose dataset is given by pointwise selection\n",
      " |      of index labels along the specified dimension(s).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel_points\n",
      " |  \n",
      " |  set_index(self, indexes=None, append=False, inplace=False, **indexes_kwargs)\n",
      " |      Set DataArray (multi-)indexes using one or more existing\n",
      " |      coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexes : {dim: index, ...}\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by (lists of) the names of existing coordinates or variables to set\n",
      " |          as new (multi-)index.\n",
      " |      append : bool, optional\n",
      " |          If True, append the supplied index(es) to the existing index(es).\n",
      " |          Otherwise replace the existing index(es) (default).\n",
      " |      inplace : bool, optional\n",
      " |          If True, set new index(es) in-place. Otherwise, return a new\n",
      " |          DataArray object.\n",
      " |      **indexes_kwargs: optional\n",
      " |          The keyword arguments form of ``indexes``.\n",
      " |          One of indexes or indexes_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : DataArray\n",
      " |          Another dataarray, with this data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.reset_index\n",
      " |  \n",
      " |  shift(self, shifts=None, **shifts_kwargs)\n",
      " |      Shift this array by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Only the data is moved; coordinates stay in place. Values shifted from\n",
      " |      beyond array bounds are replaced by NaN. This is consistent with the\n",
      " |      behavior of ``shift`` in pandas.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shifts : Mapping with the form of {dim: offset}\n",
      " |          Integer offset to shift along each of the given dimensions.\n",
      " |          Positive offsets shift to the right; negative offsets shift to the\n",
      " |          left.\n",
      " |      **shifts_kwargs:\n",
      " |          The keyword arguments form of ``shifts``.\n",
      " |          One of shifts or shifts_kwarg must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      shifted : DataArray\n",
      " |          DataArray with the same coordinates and attributes but shifted\n",
      " |          data.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      roll\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> arr = xr.DataArray([5, 6, 7], dims='x')\n",
      " |      >>> arr.shift(x=1)\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ nan,   5.,   6.])\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2\n",
      " |  \n",
      " |  sortby(self, variables, ascending=True)\n",
      " |      Sort object by labels or values (along an axis).\n",
      " |      \n",
      " |      Sorts the dataarray, either along specified dimensions,\n",
      " |      or according to values of 1-D dataarrays that share dimension\n",
      " |      with calling object.\n",
      " |      \n",
      " |      If the input variables are dataarrays, then the dataarrays are aligned\n",
      " |      (via left-join) to the calling object prior to sorting by cell values.\n",
      " |      NaNs are sorted to the end, following Numpy convention.\n",
      " |      \n",
      " |      If multiple sorts along the same dimension is\n",
      " |      given, numpy's lexsort is performed along that dimension:\n",
      " |      https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n",
      " |      and the FIRST key in the sequence is used as the primary sort key,\n",
      " |      followed by the 2nd key, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      variables: str, DataArray, or list of either\n",
      " |          1D DataArray objects or name(s) of 1D variable(s) in\n",
      " |          coords whose values are used to sort this array.\n",
      " |      ascending: boolean, optional\n",
      " |          Whether to sort by ascending or descending order.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sorted: DataArray\n",
      " |          A new dataarray where all the specified dims are sorted by dim\n",
      " |          labels.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.random.rand(5),\n",
      " |      ...                   coords=[pd.date_range('1/1/2000', periods=5)],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 5)>\n",
      " |      array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |      \n",
      " |      >>> da.sortby(da)\n",
      " |      <xarray.DataArray (time: 5)>\n",
      " |      array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n",
      " |  \n",
      " |  stack(self, dimensions=None, **dimensions_kwargs)\n",
      " |      Stack any number of existing dimensions into a single new dimension.\n",
      " |      \n",
      " |      New dimensions will be added at the end, and the corresponding\n",
      " |      coordinate variables will be combined into a MultiIndex.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n",
      " |          Names of new dimensions, and the existing dimensions that they\n",
      " |          replace.\n",
      " |      **dimensions_kwargs:\n",
      " |          The keyword arguments form of ``dimensions``.\n",
      " |          One of dimensions or dimensions_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stacked : DataArray\n",
      " |          DataArray with stacked data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> arr = DataArray(np.arange(6).reshape(2, 3),\n",
      " |      ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n",
      " |      >>> arr\n",
      " |      <xarray.DataArray (x: 2, y: 3)>\n",
      " |      array([[0, 1, 2],\n",
      " |             [3, 4, 5]])\n",
      " |      Coordinates:\n",
      " |        * x        (x) |S1 'a' 'b'\n",
      " |        * y        (y) int64 0 1 2\n",
      " |      >>> stacked = arr.stack(z=('x', 'y'))\n",
      " |      >>> stacked.indexes['z']\n",
      " |      MultiIndex(levels=[[u'a', u'b'], [0, 1, 2]],\n",
      " |                 labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n",
      " |                 names=[u'x', u'y'])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.unstack\n",
      " |  \n",
      " |  std(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `std` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `std`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `std`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `std` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `std` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `std` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  sum(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `sum` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `sum`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `sum`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `sum` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      min_count : int, default None\n",
      " |          The required number of valid values to perform the operation.\n",
      " |          If fewer than min_count non-NA values are present the result will\n",
      " |          be NA. New in version 0.10.8: Added with the default being None.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `sum` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `sum` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  swap_dims(self, dims_dict)\n",
      " |      Returns a new DataArray with swapped dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_dict : dict-like\n",
      " |          Dictionary whose keys are current dimension names and whose values\n",
      " |          are new names. Each value must already be a coordinate on this\n",
      " |          array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          DataArray with swapped dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      \n",
      " |      DataArray.rename\n",
      " |      Dataset.swap_dims\n",
      " |  \n",
      " |  to_cdms2(self)\n",
      " |      Convert this array into a cdms2.Variable\n",
      " |  \n",
      " |  to_dataframe(self, name=None)\n",
      " |      Convert this array and its coordinates into a tidy pandas.DataFrame.\n",
      " |      \n",
      " |      The DataFrame is indexed by the Cartesian product of index coordinates\n",
      " |      (in the form of a :py:class:`pandas.MultiIndex`).\n",
      " |      \n",
      " |      Other coordinates are included as columns in the DataFrame.\n",
      " |  \n",
      " |  to_dataset(self, dim=None, name=None)\n",
      " |      Convert a DataArray to a Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          Name of the dimension on this array along which to split this array\n",
      " |          into separate variables. If not provided, this array is converted\n",
      " |          into a Dataset of one variable.\n",
      " |      name : str, optional\n",
      " |          Name to substitute for this array's name. Only valid if ``dim`` is\n",
      " |          not provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dataset : Dataset\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |      Convert this xarray.DataArray into a dictionary following xarray\n",
      " |      naming conventions.\n",
      " |      \n",
      " |      Converts all variables and attributes to native Python objects.\n",
      " |      Useful for coverting to json. To avoid datetime incompatibility\n",
      " |      use decode_times=False kwarg in xarrray.open_dataset.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.from_dict\n",
      " |  \n",
      " |  to_index(self)\n",
      " |      Convert this variable to a pandas.Index. Only possible for 1D\n",
      " |      arrays.\n",
      " |  \n",
      " |  to_iris(self)\n",
      " |      Convert this array into a iris.cube.Cube\n",
      " |  \n",
      " |  to_masked_array(self, copy=True)\n",
      " |      Convert this array into a numpy.ma.MaskedArray\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : bool\n",
      " |          If True (default) make a copy of the array in the result. If False,\n",
      " |          a MaskedArray view of DataArray.values is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : MaskedArray\n",
      " |          Masked where invalid values (nan or inf) occur.\n",
      " |  \n",
      " |  to_netcdf(self, *args, **kwargs)\n",
      " |      Write DataArray contents to a netCDF file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or Path, optional\n",
      " |          Path to which to save this dataset. If no path is provided, this\n",
      " |          function returns the resulting netCDF file as a bytes object; in\n",
      " |          this case, we need to use scipy.io.netcdf, which does not support\n",
      " |          netCDF version 4 (the default format becomes NETCDF3_64BIT).\n",
      " |      mode : {'w', 'a'}, optional\n",
      " |          Write ('w') or append ('a') mode. If mode='w', any existing file at\n",
      " |          this location will be overwritten.\n",
      " |      format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',\n",
      " |                'NETCDF3_CLASSIC'}, optional\n",
      " |          File format for the resulting netCDF file:\n",
      " |      \n",
      " |          * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n",
      " |            features.\n",
      " |          * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n",
      " |            netCDF 3 compatible API features.\n",
      " |          * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n",
      " |            which fully supports 2+ GB files, but is only compatible with\n",
      " |            clients linked against netCDF version 3.6.0 or later.\n",
      " |          * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n",
      " |            handle 2+ GB files very well.\n",
      " |      \n",
      " |          All formats are supported by the netCDF4-python library.\n",
      " |          scipy.io.netcdf only supports the last two formats.\n",
      " |      \n",
      " |          The default format is NETCDF4 if you are saving a file to disk and\n",
      " |          have the netCDF4-python library available. Otherwise, xarray falls\n",
      " |          back to using scipy to write netCDF files and defaults to the\n",
      " |          NETCDF3_64BIT format (scipy does not support netCDF4).\n",
      " |      group : str, optional\n",
      " |          Path to the netCDF4 group in the given file to open (only works for\n",
      " |          format='NETCDF4'). The group(s) will be created if necessary.\n",
      " |      engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional\n",
      " |          Engine to use when writing netCDF files. If not provided, the\n",
      " |          default engine is chosen based on available dependencies, with a\n",
      " |          preference for 'netcdf4' if writing to a file on disk.\n",
      " |      encoding : dict, optional\n",
      " |          Nested dictionary with variable names as keys and dictionaries of\n",
      " |          variable specific encodings as values, e.g.,\n",
      " |          ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,\n",
      " |             'zlib': True}, ...}``\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Only xarray.Dataset objects can be written to netCDF files, so\n",
      " |      the xarray.DataArray is converted to a xarray.Dataset object\n",
      " |      containing a single variable. If the DataArray has no name, or if the\n",
      " |      name is the same as a co-ordinate name, then it is given the name\n",
      " |      '__xarray_dataarray_variable__'.\n",
      " |      \n",
      " |      All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n",
      " |  \n",
      " |  to_pandas(self)\n",
      " |      Convert this array into a pandas object with the same shape.\n",
      " |      \n",
      " |      The type of the returned object depends on the number of DataArray\n",
      " |      dimensions:\n",
      " |      \n",
      " |      * 1D -> `pandas.Series`\n",
      " |      * 2D -> `pandas.DataFrame`\n",
      " |      * 3D -> `pandas.Panel`\n",
      " |      \n",
      " |      Only works for arrays with 3 or fewer dimensions.\n",
      " |      \n",
      " |      The DataArray constructor performs the inverse transformation.\n",
      " |  \n",
      " |  to_series(self)\n",
      " |      Convert this array into a pandas.Series.\n",
      " |      \n",
      " |      The Series is indexed by the Cartesian product of index coordinates\n",
      " |      (in the form of a :py:class:`pandas.MultiIndex`).\n",
      " |  \n",
      " |  transpose(self, *dims)\n",
      " |      Return a new DataArray object with transposed dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *dims : str, optional\n",
      " |          By default, reverse the dimensions. Otherwise, reorder the\n",
      " |          dimensions to this order.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      transposed : DataArray\n",
      " |          The returned DataArray's array is transposed.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Although this operation returns a view of this array's data, it is\n",
      " |      not lazy -- the data will be fully loaded.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.transpose\n",
      " |      Dataset.transpose\n",
      " |  \n",
      " |  unstack(self, dim=None)\n",
      " |      Unstack existing dimensions corresponding to MultiIndexes into\n",
      " |      multiple new dimensions.\n",
      " |      \n",
      " |      New dimensions will be added at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to unstack. By default unstacks all\n",
      " |          MultiIndexes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      unstacked : DataArray\n",
      " |          Array with unstacked data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> arr = DataArray(np.arange(6).reshape(2, 3),\n",
      " |      ...                 coords=[('x', ['a', 'b']), ('y', [0, 1, 2])])\n",
      " |      >>> arr\n",
      " |      <xarray.DataArray (x: 2, y: 3)>\n",
      " |      array([[0, 1, 2],\n",
      " |             [3, 4, 5]])\n",
      " |      Coordinates:\n",
      " |        * x        (x) |S1 'a' 'b'\n",
      " |        * y        (y) int64 0 1 2\n",
      " |      >>> stacked = arr.stack(z=('x', 'y'))\n",
      " |      >>> stacked.indexes['z']\n",
      " |      MultiIndex(levels=[[u'a', u'b'], [0, 1, 2]],\n",
      " |                 labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n",
      " |                 names=[u'x', u'y'])\n",
      " |      >>> roundtripped = stacked.unstack()\n",
      " |      >>> arr.identical(roundtripped)\n",
      " |      True\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.stack\n",
      " |  \n",
      " |  var(self, dim=None, axis=None, skipna=None, keep_attrs=False, **kwargs)\n",
      " |      Reduce this DataArray's data by applying `var` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `var`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis(es) over which to apply `var`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied. If neither are supplied, then\n",
      " |          `var` is calculated over axes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating `var` on this object's data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : DataArray\n",
      " |          New DataArray object with `var` applied to its data and the\n",
      " |          indicated dimension(s) removed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_cdms2(variable) from builtins.type\n",
      " |      Convert a cdms2.Variable into an xarray.DataArray\n",
      " |  \n",
      " |  from_dict(d) from builtins.type\n",
      " |      Convert a dictionary into an xarray.DataArray\n",
      " |      \n",
      " |      Input dict can take several forms::\n",
      " |      \n",
      " |          d = {'dims': ('t'), 'data': x}\n",
      " |      \n",
      " |          d = {'coords': {'t': {'dims': 't', 'data': t,\n",
      " |                                'attrs': {'units':'s'}}},\n",
      " |               'attrs': {'title': 'air temperature'},\n",
      " |               'dims': 't',\n",
      " |               'data': x,\n",
      " |               'name': 'a'}\n",
      " |      \n",
      " |      where 't' is the name of the dimesion, 'a' is the name of the array,\n",
      " |      and  x and t are lists, numpy.arrays, or pandas objects.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : xarray.DataArray\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.to_dict\n",
      " |      Dataset.from_dict\n",
      " |  \n",
      " |  from_iris(cube) from builtins.type\n",
      " |      Convert a iris.cube.Cube into an xarray.DataArray\n",
      " |  \n",
      " |  from_series(series) from builtins.type\n",
      " |      Convert a pandas.Series into an xarray.DataArray.\n",
      " |      \n",
      " |      If the series's index is a MultiIndex, it will be expanded into a\n",
      " |      tensor product of one-dimensional coordinates (filling in missing\n",
      " |      values with NaN). Thus this operation should be the inverse of the\n",
      " |      `to_series` method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dask_optimize__\n",
      " |  \n",
      " |  __dask_scheduler__\n",
      " |  \n",
      " |  attrs\n",
      " |      Dictionary storing arbitrary metadata with this array.\n",
      " |  \n",
      " |  chunks\n",
      " |      Block dimensions for this array's data or None if it's not a dask\n",
      " |      array.\n",
      " |  \n",
      " |  coords\n",
      " |      Dictionary-like container of coordinate arrays.\n",
      " |  \n",
      " |  data\n",
      " |      The array's data as a dask or numpy array\n",
      " |  \n",
      " |  dims\n",
      " |      Tuple of dimension names associated with this array.\n",
      " |      \n",
      " |      Note that the type of this property is inconsistent with\n",
      " |      `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n",
      " |      consistently named properties.\n",
      " |  \n",
      " |  dt\n",
      " |      Access datetime fields for DataArrays with datetime-like dtypes.\n",
      " |      \n",
      " |      Similar to pandas, fields can be accessed through the `.dt` attribute\n",
      " |      for applicable DataArrays:\n",
      " |      \n",
      " |         >>> ds = xarray.Dataset({'time': pd.date_range(start='2000/01/01',\n",
      " |         ...                                            freq='D', periods=100)})\n",
      " |         >>> ds.time.dt\n",
      " |         <xarray.core.accessors.DatetimeAccessor at 0x10c369f60>\n",
      " |         >>> ds.time.dt.dayofyear[:5]\n",
      " |         <xarray.DataArray 'dayofyear' (time: 5)>\n",
      " |         array([1, 2, 3, 4, 5], dtype=int32)\n",
      " |         Coordinates:\n",
      " |           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |      \n",
      " |      All of the pandas fields are accessible here. Note that these fields are\n",
      " |      not calendar-aware; if your datetimes are encoded with a non-Gregorian\n",
      " |      calendar (e.g. a 360-day calendar) using cftime, then some fields like\n",
      " |      `dayofyear` may not be accurate.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  encoding\n",
      " |      Dictionary of format-specific settings for how this array should be\n",
      " |      serialized.\n",
      " |  \n",
      " |  imag\n",
      " |  \n",
      " |  indexes\n",
      " |      OrderedDict of pandas.Index objects used for label based indexing\n",
      " |  \n",
      " |  loc\n",
      " |      Attribute for location based indexing like pandas.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this array.\n",
      " |  \n",
      " |  nbytes\n",
      " |  \n",
      " |  ndim\n",
      " |  \n",
      " |  plot\n",
      " |      Access plotting functions\n",
      " |      \n",
      " |      >>> d = DataArray([[1, 2], [3, 4]])\n",
      " |      \n",
      " |      For convenience just call this directly\n",
      " |      >>> d.plot()\n",
      " |      \n",
      " |      Or use it as a namespace to use xarray.plot functions as\n",
      " |      DataArray methods\n",
      " |      >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)\n",
      " |  \n",
      " |  real\n",
      " |  \n",
      " |  shape\n",
      " |  \n",
      " |  size\n",
      " |  \n",
      " |  values\n",
      " |      The array's data as a numpy.ndarray\n",
      " |  \n",
      " |  variable\n",
      " |      Low level interface to the Variable object for this DataArray.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __array_priority__ = 60\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.AbstractArray:\n",
      " |  \n",
      " |  __array__(self, dtype=None)\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __complex__(self)\n",
      " |  \n",
      " |  __float__(self)\n",
      " |  \n",
      " |  __int__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __long__(self)\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  get_axis_num(self, dim)\n",
      " |      Return axis number(s) corresponding to dimension(s) in this array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or iterable of str\n",
      " |          Dimension name(s) for which to lookup axes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int or tuple of int\n",
      " |          Axis number or numbers corresponding to the given dimensions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from xarray.core.common.AbstractArray:\n",
      " |  \n",
      " |  T\n",
      " |  \n",
      " |  sizes\n",
      " |      Ordered mapping from dimension names to lengths.\n",
      " |      \n",
      " |      Immutable.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.sizes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from xarray.core.common.ImplementsArrayReduce:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.DataWithCoords:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  assign_attrs(self, *args, **kwargs)\n",
      " |      Assign new attrs to this object.\n",
      " |      \n",
      " |      Returns a new object equivalent to self.attrs.update(*args, **kwargs).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : positional arguments passed into ``attrs.update``.\n",
      " |      kwargs : keyword arguments passed into ``attrs.update``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new attrs in addition to the existing data.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |  \n",
      " |  assign_coords(self, **kwargs)\n",
      " |      Assign new coordinates to this object.\n",
      " |      \n",
      " |      Returns a new object with all the original data in addition to the new\n",
      " |      coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kwargs : keyword, value pairs\n",
      " |          keywords are the variables names. If the values are callable, they\n",
      " |          are computed on this object and assigned to new coordinate\n",
      " |          variables. If the values are not callable, (e.g. a DataArray,\n",
      " |          scalar, or array), they are simply assigned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new coordinates in addition to the existing\n",
      " |          data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Convert longitude coordinates from 0-359 to -180-179:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.random.rand(4),\n",
      " |      ...                   coords=[np.array([358, 359, 0, 1])],\n",
      " |      ...                   dims='lon')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.28298 , 0.667347, 0.657938, 0.177683])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 358 359 0 1\n",
      " |      >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.28298 , 0.667347, 0.657938, 0.177683])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 -2 -1 0 1\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since ``kwargs`` is a dictionary, the order of your arguments may not\n",
      " |      be preserved, and so the order of the new variables is not well\n",
      " |      defined. Assigning multiple variables within the same ``assign_coords``\n",
      " |      is possible, but you cannot reference other variables created within\n",
      " |      the same ``assign_coords`` call.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close any files linked to this object\n",
      " |  \n",
      " |  get_index(self, key)\n",
      " |      Get an index for a dimension, with fall-back to a default RangeIndex\n",
      " |  \n",
      " |  groupby(self, group, squeeze=True)\n",
      " |      Returns a GroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : str, DataArray or IndexVariable\n",
      " |          Array whose unique values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      squeeze : boolean, optional\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : GroupBy\n",
      " |          A `GroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Calculate daily anomalies for daily data:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n",
      " |      ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n",
      " |      ...                           freq='D')],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 1827)>\n",
      " |      array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |      >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n",
      " |      <xarray.DataArray (time: 1827)>\n",
      " |      array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n",
      " |      Coordinates:\n",
      " |        * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n",
      " |          dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.groupby.DataArrayGroupBy\n",
      " |      core.groupby.DatasetGroupBy\n",
      " |  \n",
      " |  groupby_bins(self, group, bins, right=True, labels=None, precision=3, include_lowest=False, squeeze=True)\n",
      " |      Returns a GroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Rather than using all unique values of `group`, the values are discretized\n",
      " |      first by applying `pandas.cut` [1]_ to `group`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : str, DataArray or IndexVariable\n",
      " |          Array whose binned values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      bins : int or array of scalars\n",
      " |          If bins is an int, it defines the number of equal-width bins in the\n",
      " |          range of x. However, in this case, the range of x is extended by .1%\n",
      " |          on each side to include the min or max values of x. If bins is a\n",
      " |          sequence it defines the bin edges allowing for non-uniform bin\n",
      " |          width. No extension of the range of x is done in this case.\n",
      " |      right : boolean, optional\n",
      " |          Indicates whether the bins include the rightmost edge or not. If\n",
      " |          right == True (the default), then the bins [1,2,3,4] indicate\n",
      " |          (1,2], (2,3], (3,4].\n",
      " |      labels : array or boolean, default None\n",
      " |          Used as labels for the resulting bins. Must be of the same length as\n",
      " |          the resulting bins. If False, string bin labels are assigned by\n",
      " |          `pandas.cut`.\n",
      " |      precision : int\n",
      " |          The precision at which to store and display the bins labels.\n",
      " |      include_lowest : bool\n",
      " |          Whether the first interval should be left-inclusive or not.\n",
      " |      squeeze : boolean, optional\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : GroupBy\n",
      " |          A `GroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |          The name of the group has the added suffix `_bins` in order to\n",
      " |          distinguish it from the original variable.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
      " |  \n",
      " |  isin(self, test_elements)\n",
      " |      Tests each value in the array for whether it is in the supplied list.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      test_elements : array_like\n",
      " |          The values against which to test each value of `element`.\n",
      " |          This argument is flattened if an array or array_like.\n",
      " |          See numpy notes for behavior with non-array-like parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      isin : same as object, bool\n",
      " |          Has the same shape as this object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> array = xr.DataArray([1, 2, 3], dims='x')\n",
      " |      >>> array.isin([1, 3])\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ True, False,  True])\n",
      " |      Dimensions without coordinates: x\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.isin\n",
      " |  \n",
      " |  pipe(self, func, *args, **kwargs)\n",
      " |      Apply func(self, *args, **kwargs)\n",
      " |      \n",
      " |      This method replicates the pandas method of the same name.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          function to apply to this xarray object (Dataset/DataArray).\n",
      " |          ``args``, and ``kwargs`` are passed into ``func``.\n",
      " |          Alternatively a ``(callable, data_keyword)`` tuple where\n",
      " |          ``data_keyword`` is a string indicating the keyword of\n",
      " |          ``callable`` that expects the xarray object.\n",
      " |      args : positional arguments passed into ``func``.\n",
      " |      kwargs : a dictionary of keyword arguments passed into ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : the return type of ``func``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      Use ``.pipe`` when chaining together functions that expect\n",
      " |      xarray or pandas objects, e.g., instead of writing\n",
      " |      \n",
      " |      >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n",
      " |      \n",
      " |      You can write\n",
      " |      \n",
      " |      >>> (ds.pipe(h)\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe(f, arg2=b, arg3=c)\n",
      " |      ... )\n",
      " |      \n",
      " |      If you have a function that takes the data as (say) the second\n",
      " |      argument, pass a tuple indicating which keyword expects the\n",
      " |      data. For example, suppose ``f`` takes its data as ``arg2``:\n",
      " |      \n",
      " |      >>> (ds.pipe(h)\n",
      " |      ...    .pipe(g, arg1=a)\n",
      " |      ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n",
      " |      ...  )\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.pipe\n",
      " |  \n",
      " |  resample(self, freq=None, dim=None, how=None, skipna=None, closed=None, label=None, base=0, keep_attrs=False, **indexer)\n",
      " |      Returns a Resample object for performing resampling operations.\n",
      " |      \n",
      " |      Handles both downsampling and upsampling. If any intervals contain no\n",
      " |      values from the original object, they will be given the value ``NaN``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      skipna : bool, optional\n",
      " |          Whether to skip missing values when aggregating in downsampling.\n",
      " |      closed : 'left' or 'right', optional\n",
      " |          Side of each interval to treat as closed.\n",
      " |      label : 'left or 'right', optional\n",
      " |          Side of each interval to use for labeling.\n",
      " |      base : int, optional\n",
      " |          For frequencies that evenly subdivide 1 day, the \"origin\" of the\n",
      " |          aggregated intervals. For example, for '24H' frequency, base could\n",
      " |          range from 0 through 23.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the object's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      **indexer : {dim: freq}\n",
      " |          Dictionary with a key indicating the dimension name to resample\n",
      " |          over and a value corresponding to the resampling frequency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      resampled : same type as caller\n",
      " |          This object resampled.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Downsample monthly time-series data to seasonal data:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n",
      " |      ...                   coords=[pd.date_range('15/12/1999',\n",
      " |      ...                           periods=12, freq=pd.DateOffset(months=1))],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      >>> da.resample(time=\"QS-DEC\").mean()\n",
      " |      <xarray.DataArray (time: 4)>\n",
      " |      array([ 1.,  4.,  7., 10.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n",
      " |      \n",
      " |      Upsample monthly time-series data to daily data:\n",
      " |      \n",
      " |      >>> da.resample(time='1D').interpolate('linear')\n",
      " |      <xarray.DataArray (time: 337)>\n",
      " |      array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      \n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
      " |  \n",
      " |  rolling(self, dim=None, min_periods=None, center=False, **dim_kwargs)\n",
      " |      Rolling window object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim: dict, optional\n",
      " |          Mapping from the dimension name to create the rolling iterator\n",
      " |          along (e.g. `time`) to its moving window size.\n",
      " |      min_periods : int, default None\n",
      " |          Minimum number of observations in window required to have a value\n",
      " |          (otherwise result is NA). The default, None, is equivalent to\n",
      " |          setting min_periods equal to the size of the window.\n",
      " |      center : boolean, default False\n",
      " |          Set the labels at the center of the window.\n",
      " |      **dim_kwargs : optional\n",
      " |          The keyword arguments form of ``dim``.\n",
      " |          One of dim or dim_kwarg must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Rolling object (core.rolling.DataArrayRolling for DataArray,\n",
      " |      core.rolling.DatasetRolling for Dataset.)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n",
      " |      ...                   coords=[pd.date_range('15/12/1999',\n",
      " |      ...                           periods=12, freq=pd.DateOffset(months=1))],\n",
      " |      ...                   dims='time')\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      >>> da.rolling(time=3, center=True).mean()\n",
      " |      <xarray.DataArray (time: 12)>\n",
      " |      array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n",
      " |      \n",
      " |      Remove the NaNs using ``dropna()``:\n",
      " |      \n",
      " |      >>> da.rolling(time=3, center=True).mean().dropna('time')\n",
      " |      <xarray.DataArray (time: 10)>\n",
      " |      array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.rolling.DataArrayRolling\n",
      " |      core.rolling.DatasetRolling\n",
      " |  \n",
      " |  squeeze(self, dim=None, drop=False, axis=None)\n",
      " |      Return a new object with squeezed data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : None or str or tuple of str, optional\n",
      " |          Selects a subset of the length one dimensions. If a dimension is\n",
      " |          selected with length greater than one, an error is raised. If\n",
      " |          None, all length one dimensions are squeezed.\n",
      " |      drop : bool, optional\n",
      " |          If ``drop=True``, drop squeezed coordinates instead of making them\n",
      " |          scalar.\n",
      " |      axis : int, optional\n",
      " |          Select the dimension to squeeze. Added for compatibility reasons.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      squeezed : same type as caller\n",
      " |          This object, but with with all or a subset of the dimensions of\n",
      " |          length 1 removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.squeeze\n",
      " |  \n",
      " |  where(self, cond, other=<NA>, drop=False)\n",
      " |      Filter elements from this object according to a condition.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : DataArray or Dataset with boolean dtype\n",
      " |          Locations at which to preserve this object's values.\n",
      " |      other : scalar, DataArray or Dataset, optional\n",
      " |          Value to use for locations in this object where ``cond`` is False.\n",
      " |          By default, these locations filled with NA.\n",
      " |      drop : boolean, optional\n",
      " |          If True, coordinate labels that only correspond to False values of\n",
      " |          the condition are dropped from the result. Mutually exclusive with\n",
      " |          ``other``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Same type as caller.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=('x', 'y'))\n",
      " |      >>> a.where(a.x + a.y < 4)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[  0.,   1.,   2.,   3.,  nan],\n",
      " |             [  5.,   6.,   7.,  nan,  nan],\n",
      " |             [ 10.,  11.,  nan,  nan,  nan],\n",
      " |             [ 15.,  nan,  nan,  nan,  nan],\n",
      " |             [ nan,  nan,  nan,  nan,  nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      >>> a.where(a.x + a.y < 5, -1)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[ 0,  1,  2,  3,  4],\n",
      " |             [ 5,  6,  7,  8, -1],\n",
      " |             [10, 11, 12, -1, -1],\n",
      " |             [15, 16, -1, -1, -1],\n",
      " |             [20, -1, -1, -1, -1]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      >>> a.where(a.x + a.y < 4, drop=True)\n",
      " |      <xarray.DataArray (x: 4, y: 4)>\n",
      " |      array([[  0.,   1.,   2.,   3.],\n",
      " |             [  5.,   6.,   7.,  nan],\n",
      " |             [ 10.,  11.,  nan,  nan],\n",
      " |             [ 15.,  nan,  nan,  nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.where : corresponding numpy function\n",
      " |      where : equivalent function\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.arithmetic.SupportsArithmetic:\n",
      " |  \n",
      " |  __array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n",
      " |  \n",
      " |  __div__ = not_implemented(*args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.AttrAccessMixin:\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Provide method name lookup and completion. Only provide 'public'\n",
      " |      methods.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xr.DataArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=xr.DataArray(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (dim_0: 50, dim_1: 50)>\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])\n",
       "Dimensions without coordinates: dim_0, dim_1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray '2DTest' (dim_0: 50, dim_1: 50)>\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])\n",
       "Dimensions without coordinates: dim_0, dim_1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rename('2DTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "XArray-Test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
